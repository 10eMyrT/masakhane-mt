{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "English_To_Dendi_BPE_notebook_custom_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x4fXCKCf36IK"
      },
      "source": [
        "## Note before beginning:\n",
        "### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
        "\n",
        "### - The tl;dr: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
        "\n",
        "### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
        "\n",
        "### - With 100 epochs, it should take around 7 hours to run in Google Colab\n",
        "\n",
        "### - Once you've gotten a result for your language, please attach and email your notebook that generated it to masakhanetranslation@gmail.com\n",
        "\n",
        "### - If you care enough and get a chance, doing a brief background on your language would be amazing. See examples in  [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "### - This notebook is intended to be used with custom parallel data. That means that you need two files, where one is in your language, the other English, and the lines in the files are corresponding translations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Pre-process your data\n",
        "\n",
        "We assume here that you already have a data set. The format in which we will process it here requires that \n",
        "1. you have two files, one for each language\n",
        "2. the files are sentence-aligned, which means that each line should correspond to the same line in the other file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGRmDELn7Az0",
        "outputId": "0e0c41aa-8c1d-442c-ae0b-ca98b368f935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cn3tgQLzUxwn",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"ddn\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBSgJHEw7Nvx",
        "outputId": "27531066-dd37-48c8-d9c2-b1658cfbc3b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masakhane/en-ddn-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gA75Fs9ys8Y9",
        "outputId": "8093b427-a18f-4175-9bff-52334866d88a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opustools-pkg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/9f/e829a0cceccc603450cd18e1ff80807b6237a88d9a8df2c0bb320796e900/opustools_pkg-0.0.52-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xq-tDZVks7ZD",
        "outputId": "4bee1143-1a6f-42fa-bf48-c79e097febca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# TODO: specify the file paths here\n",
        "source_file = \"/test.en\"\n",
        "target_file = \"/test.ddn\"\n",
        "\n",
        "# They should both have the same length.\n",
        "! wc -l $source_file\n",
        "! wc -l $target_file"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7943 /my_file.en\n",
            "7943 /my_file.ddn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNmkusFfGorx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Pre-processing! (OPTIONAL)\n",
        "\n",
        "# If your data contains weird symbols or the like, you might want to do some cleaning and normalization.\n",
        "# We don't have the code in the notebook for that, but you can use sacremoses \"normalize\" for example for normalization punctuation: https://github.com/alvations/sacremoses.\n",
        "\n",
        "# We apply tokenization to separate punctuation marks from the actual words, split words at hyphens etc.\n",
        "# If your data is already tokenized, that's great! Skip this cell.\n",
        "# Otherwise we can use sacremoses to do the tokenization for us. \n",
        "# We need the data to be tokenized such that it matches the global test set.\n",
        "\n",
        "#! pip install sacremoses\n",
        "\n",
        "#tok_source_file = source_file+\".tok\"\n",
        "#tok_target_file = target_file+\".tok\"\n",
        "\n",
        "# Tokenize the source\n",
        "#! sacremoses tokenize -l $source_language < $source_file > $tok_source_file\n",
        "# Tokenize the target\n",
        "#! sacremoses tokenize -l $target_language < $target_file > $tok_target_file\n",
        "\n",
        "# Let's take a look what tokenization did to the text.\n",
        "#! head $source_file*\n",
        "#! head $target_file*\n",
        "\n",
        "# Change the pointers to our files such that we continue to work with the tokenized data.\n",
        "#source_file = tok_source_file\n",
        "#target_file = tok_target_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n48GDRnP8y2G",
        "colab": {}
      },
      "source": [
        "# Download the global test set.\n",
        "#! wget https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-any.en\n",
        "  \n",
        "# And the specific test set for this language pair.\n",
        "#os.environ[\"trg\"] = target_language \n",
        "#os.environ[\"src\"] = target_language \n",
        "\n",
        "#! wget https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
        "#! mv test.en-$trg.en test.en\n",
        "#! wget https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
        "#! mv test.en-$trg.$trg test.$trg\n",
        "\n",
        "# TODO: if this fails it means that there is NO test set for your language yet. It's on you to create one.\n",
        "# A good idea would be to take a random subset of your data, and add it to https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-any.en.\n",
        "# Make a Pull Request and get it approved and merged.\n",
        "# Then repeat this cell to retrieve the new test set.\n",
        "# Then proceed to the next cell that will filter out all duplicates from the training set, so that there is no overlap between training and test set."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NqDG-CI28y2L",
        "colab": {}
      },
      "source": [
        "# Read the test data to filter from train and dev splits.\n",
        "# Store english portion in set for quick filtering checks.\n",
        "#en_test_sents = set()\n",
        "#filter_test_sents = \"test.en-any.en\"\n",
        "#j = 0\n",
        "#with open(filter_test_sents) as f:\n",
        "#  for line in f:\n",
        "#    en_test_sents.add(line.strip())\n",
        "#    j += 1\n",
        "#print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CNdwLBCfSIl",
        "outputId": "b226473c-2ca5-40eb-f636-00a3caaf8f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        #if line.strip() not in en_test_sents:\n",
        "        source.append(line.strip())\n",
        "        #else:\n",
        "        #    skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        #if j not in skip_lines:\n",
        "        target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "df.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 0/7942 lines since contained in test set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The book of the generation of Jesus Christ, th...</td>\n",
              "      <td>Yesu Mɛsiyɑ ko ɑ̀ ci Dɑfidi ize, Abulɛmɑ ize, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Abraham begat Isaac; and Isaac begat Jacob; an...</td>\n",
              "      <td>Abulɛmɑ nɑ Isɑɑkɑ hɛi. Isɑɑkɑ nɑ Yɑkɔfu hɛi. Y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>and Judah begat Perez and Zerah of Tamar; and ...</td>\n",
              "      <td>Yudɑ mo nɑ Fɑresi ndɑ Zerɑ hɛi Tɑmɑrɑ gɑɑ. Fɑr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     source_sentence                                    target_sentence\n",
              "0  The book of the generation of Jesus Christ, th...  Yesu Mɛsiyɑ ko ɑ̀ ci Dɑfidi ize, Abulɛmɑ ize, ...\n",
              "1  Abraham begat Isaac; and Isaac begat Jacob; an...  Abulɛmɑ nɑ Isɑɑkɑ hɛi. Isɑɑkɑ nɑ Yɑkɔfu hɛi. Y...\n",
              "2  and Judah begat Perez and Zerah of Tamar; and ...  Yudɑ mo nɑ Fɑresi ndɑ Zerɑ hɛi Tɑmɑrɑ gɑɑ. Fɑr..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Pre-processing and export\n",
        "\n",
        "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
        "\n",
        "In addition we will split our data into dev/test/train and export to the filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_2ouEOH1_1q",
        "colab": {}
      },
      "source": [
        "# drop duplicate translations\n",
        "df_pp = df.drop_duplicates()\n",
        "\n",
        "# drop conflicting translations\n",
        "#df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "#df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niVvXvLV0bkE",
        "colab_type": "code",
        "outputId": "287acd89-77d9-4315-b22b-519285b8e6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_pp.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7937, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxxBOCA-xXhy",
        "outputId": "6687d582-acfd-4a44-e79b-0eb8bddad1d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# TODO: if your corpus is smaller than 1000, reduce this number. With a corpus that small you might not obtain good results with NMT though :/\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
        "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
        "\n",
        "\n",
        "#Divide df_pp into train and test sets\n",
        "\n",
        "msk = np.random.rand(len(df_pp)) < 0.98\n",
        "train = df_pp[msk]\n",
        "test_df = df_pp[~msk]\n",
        "\n",
        "# Julia: test sets are already generated\n",
        "dev_df = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped_df = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped_df.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev_df.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in test_df.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "\n",
        "# TODO: Doublecheck the format below. There should be no extra quotation marks or weird characters. It should also not be empty.\n",
        "! head train.*\n",
        "! head dev.*\n",
        "! head test.*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.ddn <==\n",
            "À kɑɑ hunu kɑ hɑndunyɑ dimi yom kɑ ǹ goono hɑndunyɑ cɛnjɛ tɑɑci gɑɑ dɛrɑndi, wɑtom Gɔgu ndɑ Mɑgɔgu. À gɑ ǹ meigu wɑngu tɛyom sɛ. Ǹ bɑyom bisɑ tɛku tɑɑsi.\n",
            "Zɑngɑ Ikpɛ Sendi hɑntumɑntɛ cii: Ikpɛ nɑ ǹ nɑ birikɑyom biyɑ. À nɑ ǹ nɑ moo yom kɑ ǹ si di, ndɑ hɑngɑ yom kɑ ǹ si mɑɑ hɑli kɑ kɑɑ hɔ.\n",
            "A mɑɑ jinde beeri fɔ kɑ ɑ̀ go hunu zɑ Ikpɛ goonoyom dɔ. À go cii Ikpɛ dɔntɔneize iye di sɛ: Wɑ kpei kɑ Ikpɛ binefute gɑɑsiɑ iye munu hɑndunyɑ bɔm.\n",
            "Kɑnkɑmi kunɑ ɑ nɑ gbei futu yom tɛ. A zɑm jiibi ce boobo. A mɑɑ hɛrɛɛ. A mɑɑ jo. A nɑ meehɔ yom zɑɑ cɛrɛ bɔm. A mɑɑ yeeni. A gɔrɔ bumbum.\n",
            "Ammɑ Piɛɛ nɑ Yesu ze kɑ cii: A si ɑ̀ bei. A si fɑhɑm mo ndɑ hɛ kɑ n gɑ cii. Piɛɛ hunu hundi bɑtumɑ kunɑ kɑ kpei kɑtɑ dɔ. À gɑɑ no, gɔrɔngɔ cɑ jinde fɔ.\n",
            "Ammɑ sɑɑ kɑ i nɑ jiribi iye tɔnɑndi, i tunu kɑ dirɑ. Ǹ kulu ndɑ ǹ wɛndɛ yom ndɑ ǹ koo yom nɑ i dum kɑlɑ wɑngɑrɑ bɑndɑ. I sɔmbu tɛkuɑ mee gɑɑ kɑ ɑduwɑ tɛ.\n",
            "Yesu cii ɑ̀ sɛ: A koo hundiyo, n lɑɑkɑli mɑ si tunu. N nɑɑne nɑ n no bɑɑni. Kpei ndɑ lɑɑkɑli kɑne.\n",
            "Ǹ kɔmɑ hɑndunyɑ ziibi yom gɑɑ Kpe Yesu Mɛsiyɑ kɑ ɑ̀ ci Fɑɑbɑkɔ ɑ̀ beiyom dɔ. À bɑndɑ de ǹ kɑɑ yɑɑrɑ kɑ wuluwɑli hɑndunyɑ ziibi yom kunɑ, hɑndunyɑ wɔ gɑɑŋmɑɑ ǹ bɔm. Ǹ kɔkɔrɔ bɑndɑ goonoyom, ɑ̀ jɑɑsɑ sintine gɔrɛ.\n",
            "Zɑngɑ koo yom go kɑ ngei bɑɑbɑ gɑnɑ, wɑ si wom bɔm no wom dom bineibɑɑi sɛ kɑ ɑ̀ kunɑ wom gɔrɔ sɑɑ fɔ zɑm kɑ bei kunɑ.\n",
            "Ngɑ di no, Biyɑ Hɑlɑɑlɑ mo go tɛ sɛdɑ i sɛ ngɑ di bɔm. Sintine ɑ̀ cii:\n",
            "\n",
            "==> train.en <==\n",
            "and shall come forth to deceive the nations which are in the four corners of the earth, Gog and Magog, to gather them together to the war: the number of whom is as the sand of the sea.\n",
            "according as it is written, God gave them a spirit of stupor, eyes that they should not see, and ears that they should not hear, unto this very day.\n",
            "And I heard a great voice out of the temple, saying to the seven angels, Go ye, and pour out the seven bowls of the wrath of God into the earth.\n",
            "in labor and travail, in watchings often, in hunger and thirst, in fastings often, in cold and nakedness.\n",
            "But he denied, saying, I neither know, nor understand what thou sayest: and he went out into the porch; and the cock crew.\n",
            "And when it came to pass that we had accomplished the days, we departed and went on our journey; and they all, with wives and children, brought us on our way till we were out of the city: and kneeling down on the beach, we prayed, and bade each other farewell;\n",
            "And he said unto her, Daughter, thy faith hath made thee whole; go in peace.\n",
            "For if, after they have escaped the defilements of the world through the knowledge of the Lord and Saviour Jesus Christ, they are again entangled therein and overcome, the last state is become worse with them than the first.\n",
            "as children of obedience, not fashioning yourselves according to your former lusts in the time of your ignorance:\n",
            "And the Holy Spirit also beareth witness to us; for after he hath said,\n",
            "==> dev.ddn <==\n",
            "A go sɑɑbu i Kpe Yesu Mɛsiyɑ sɛ, ngɑ kɑ ɑ̀ nɑ ɑ no gɑɑbi. A gɑ kɑ ɑ̀ sɑɑbu domi ɑ̀ nɑ ɑ lɑsɑbu nɑɑnekpɛ kɑ ɑ dɑm ngɑ gbei kunɑ.\n",
            "Mɑɑsɑnkulu Kpe, n mɑ tu n bɑnyɑ mɑ kpei ndɑ bɑɑni zɑngɑ n Sendɑ cii.\n",
            "Ikpɛ fɔlɔnku yɑ gɑ bɑnguize yom cɛɑndi susu nɑɑne gɑɑ. À go zɑm kɑ dɑmbɑnguize yom mo cɛɑndi susu nɑɑne gɑɑ.\n",
            "Wɑngɑrɑ ngɑ di, Sɑmɑriɑncɛ boobo nɑɑne Yesu gɑɑ weibɔrɔ di sendɑ sɑbu sɛ. Weibɔrɔ di tɛ sɛdɑ kɑ cii: Hɛ kulu kɑ ɑ jinɑ kɑ tɛ, ɑ̀ nɑ ɑ̀ cii ɑ sɛ.\n",
            "Cɔfɔ ɑ̀ go cii: A fuu kɑ ɑ̀ kunɑ ɑ hunu no, ɑ kɑɑ ye. Sɑɑ kɑ hɔllɛ di ye fuu di, ɑ̀ gɑru hɛ fɔ kulu si ɑ̀ kunɑ. Ammɑ ǹ nɑ ɑ̀ hɑɑbu kɑ booriɑndi.\n",
            "Kɑ Yesu ye kɑ huro hɑrihi ɑ̀ nɑ buulɑ dem kɑ kpei ngɑ kpɑɑrɑ.\n",
            "Cɔfɔ jinde fɔ hunu zɑ bɛɛnɛ kɑ cii: Ni yɑ cii ɑ Ize Binegɑnji. N gɑɑ nɑ ɑ du ɑ binekɑɑne.\n",
            "Yesu tu ǹ sɛ kɑ cii: Beerem kɑ ǹ gundɑ bɑɑni si bɑɑ lokotoro kɑlɑ beerem kɑ ǹ sindɑ bɑɑni.\n",
            "Sɑɑ kɑ Pɔlu ndɑ Silɑ bisɑ Amfipoli wɑngɑrɑ ndɑ Apoloni wɑngɑrɑ ǹ kɑɑ Tesɑlonikɑ wɑngɑrɑ. Nungu di Yuifu yom mɑrgɑ fuu fɔ goono.\n",
            "Ikpɛ mɑnɑ ɑ̀ nɑ gɑndɑ hɛ fɔ kulu ɑ̀ mɑ tubu, bɑ cee dɛcɛyom dɔ sɑrɑ. Ammɑ Ikpɛ nɑ ɑlikɑwɑli tɛ ɑ̀ sɛ kɑ cii ngɑ gɑ ɑ̀ no lɑɑbu ɑ̀ mɑ ci ɑ̀ ŋmɔne ndɑ ɑ̀ bɑndɑ yom ŋmɔne mo. Sɑɑ ngɑ di Abulɛmɑ sindɑ koo jinɑ.\n",
            "\n",
            "==> dev.en <==\n",
            "I thank him that enabled me, even Christ Jesus our Lord, for that he counted me faithful, appointing me to his service;\n",
            "Now lettest thou thy servant depart, Lord, According to thy word, in peace;\n",
            "if so be that God is one, and he shall justify the circumcision by faith, and the uncircumcision through faith.\n",
            "And from that city many of the Samaritans believed on him because of the word of the woman, who testified, He told me all things that ever I did.\n",
            "Then he saith, I will return into my house whence I came out; and when he is come, he findeth it empty, swept, and garnished.\n",
            "And he entered into a boat, and crossed over, and came into his own city.\n",
            "and a voice came out of the heavens, Thou art my beloved Son, in thee I am well pleased.\n",
            "And Jesus answering said unto them, They that are in health have no need of a physician; but they that are sick.\n",
            "Now when they had passed through Amphipolis and Apollonia, they came to Thessalonica, where was a synagogue of the Jews:\n",
            "and he gave him none inheritance in it, no, not so much as to set his foot on: and he promised that he would give it to him in possession, and to his seed after him, when as yet he had no child.\n",
            "==> test.ddn <==\n",
            "Ŋmɑɑri yɑ gunde ŋmɔne no. Gunde mo ŋmɑɑri ŋmɔne nɑ. Ammɑ ɑfɔ fɔ Ikpɛ gɑ ǹ kulu hɑlɑci. Gɑɑhɑm wɔ mɑnɑ ci zinɑ ŋmɔne, Kpe ŋmɔne no. Kpe mo gɑɑhɑm ŋmɔne.\n",
            "Wom go bei Tifɑnu ndɑ ɑ̀ fuu bɔrɔ yom, ngei yɑ ci Akɑyɑ lɑɑbu gɑnɑkɔ sintine yom. Ǹ nɑ ngei bɔm no mo kɑ cini tɑmtɛrɛ hɑlɑɑlɑntɛ yom sɛ. Mɑɑsɑ nyɑizei, hɛ fɔ himmɑ nɑ ɑ gɑ wom no.\n",
            "Ammɑ bɔrɔ kulu kɑ ɑ̀ koo cɛnɑ yom wɔ kɑ ǹ go nɑɑne ɑ gɑɑ ǹ ŋmɔne ɑfɔ dɛrɑndi ǹ mɑ fufuyom tondi hɔ ɑkpɛ jinde gɑɑ. Ǹ mɑ ɑ̀ cetu buulɑ nungu guusu kunɑ, ɑ̀ bisɑ.\n",
            "Wɑngu bɑrikɑri yom goono zɑngɑ ǹ bɔrɔ dubu zɑngu hinkɑ hɑli ce dubu wei. Yɑ nɑ ɑ mɑɑ.\n",
            "Coobɑɑbɑize yom di cii: Bɔrɔ fɔ yom go cii Mirɑndikɔ Yohɑnnɑ no. Afɔ yom mo go cii Eli no. Afɔ yom mo go cii Yeremi no wɑlɑ dom ɑndebi yom ŋmɔne ɑfɔ no.\n",
            "Ǹ cii Aɑrɔ sɛ: N mɑ tooru yom tɛ i sɛ kɑ ǹ mo dirɑ i jine. N mɑ ngɑ di tɛ zɑmɑ Moisi wɔ kɑ ɑ̀ nɑ i kɑɑ Eziputu lɑɑbu, i mɑnɑ bei hɛ kɑ du ɑ̀.\n",
            "Ǹ sɑwɑrɑ kɑ cii i mɑ huro hɑrihi fɔ kunɑ kɑ kpei Itɑli lɑɑbu. Ǹ nɑ Pɔlu ndɑ kɑsuize fɔ yom dɑm wɑnguize yom jinehune fɔ kɑmbɛ kɑ ɑ̀ mɑɑ Yuli. Ngɑ yɑ ci Agusitu wɑnguize yom ŋmɔne kuru fɔ jinehune.\n",
            "Bɔrɔ kɑ ɑ̀ go fɑri wi du tukunsi. À go liibɑ meigu hundi kɑ ɑ̀ si beeni sɛ, hɑli bɔrɔ kɑ ɑ̀ go dumɑ ndɑ bɔrɔ kɑ ɑ̀ go wi, ǹ mɑ fɑihã mefɔ.\n",
            "Wɑ gbei tɛ, ɑ̀ mɑ si ci ŋmɑɑri kɑ ɑ̀ hɑlɑci sɛ ɑmmɑ ŋmɑɑri kɑ ɑ̀ bo goono kɑlɑ hundi kɑ ɑ̀ si beeni sɛ. Ŋmɑɑrɑ wɔ no hɛɛ Adɑmize, ɑ kɑɑ wom no zɑmɑ ɑ gɑɑ no Bɑɑbɑ Ikpɛ nɑ ngɑ sɛdɑ dɑm.\n",
            "Ǹ nɑ jɑmɑ di ndɑ mɑrgɑ hɑikusu yom ndɑ Moisi fɔndɑ cooɑndikɔ yom zukum hɑli ǹ kɑm Tifɑnu bɔm. Ǹ nɑ ɑ̀ dii kɑ kpeindɑɑ̀ mɑrgɑ ciiti dumbukɔ yom dɔ.\n",
            "\n",
            "==> test.en <==\n",
            "Meats for the belly, and the belly for meats: but God shall bring to nought both it and them. But the body is not for fornication, but for the Lord; and the Lord for the body:\n",
            "Now I beseech you, brethren (ye know the house of Stephanas, that it is the firstfruits of Achaia, and that they have set themselves to minister unto the saints),\n",
            "but whoso shall cause one of these little ones that believe on me to stumble, it is profitable for him that a great millstone should be hanged about his neck, and that he should be sunk in the depth of the sea.\n",
            "And the number of the armies of the horsemen was twice ten thousand times ten thousand: I heard the number of them.\n",
            "And they said, Some say John the Baptist; some, Elijah; and others, Jeremiah, or one of the prophets.\n",
            "saying unto Aaron, Make us gods that shall go before us: for as for this Moses, who led us forth out of the land of Egypt, we know not what is become of him.\n",
            "And when it was determined that we should sail for Italy, they delivered Paul and certain other prisoners to a centurion named Julius, of the Augustan band.\n",
            "He that reapeth receiveth wages, and gathereth fruit unto life eternal; that he that soweth and he that reapeth may rejoice together.\n",
            "Work not for the food which perisheth, but for the food which abideth unto eternal life, which the Son of man shall give unto you: for him the Father, even God, hath sealed.\n",
            "And they stirred up the people, and the elders, and the scribes, and came upon him, and seized him, and brought him into the council,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEDAsKXPw28d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stripped_df.to_csv('train.csv')\n",
        "!cp train.csv \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yGbVVjlw61I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.to_csv('test.csv')\n",
        "!cp test.csv \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f--enEIOw-2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_df.to_csv('dev.csv')\n",
        "!cp dev.csv \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBRMm4kMxZ8L",
        "outputId": "a632e5d1-34cd-4542-ec53-e76cae5f7829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 2281 (delta 57), reused 46 (delta 25), pack-reused 2184\u001b[K\n",
            "Receiving objects: 100% (2281/2281), 2.63 MiB | 2.83 MiB/s, done.\n",
            "Resolving deltas: 100% (1578/1578), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (6.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.17.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (45.1.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.4.0)\n",
            "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n",
            "Collecting sacrebleu>=1.3.6\n",
            "  Downloading https://files.pythonhosted.org/packages/45/31/1a135b964c169984b27fb2f7a50280fa7f8e6d9d404d8a9e596180487fd1/sacrebleu-1.4.3-py3-none-any.whl\n",
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.1.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.10.0)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d9/ea9816aea31beeadccd03f1f8b625ecf8f645bd66744484d162d84803ce5/PyYAML-5.3.tar.gz (268kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 6.1MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/59/43fc36c5ee316bb9aeb7cf5329cdbdca89e5749c34d5602753827c0aa2dc/pylint-2.4.4-py3-none-any.whl (302kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.12 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.1.8)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.21.0)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/91/db/7bc703c0760df726839e0699b7f78a4d8217fdc9c7fcb1b51b39c5a22a4e/portalocker-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.3.6->joeynmt==0.0.1) (3.6.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (0.25.3)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.4.1)\n",
            "Collecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting isort<5,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n",
            "\u001b[?25hCollecting astroid<2.4,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/ae/86734823047962e7b8c8529186a1ac4a7ca19aaf1aa0c7713c022ef593fd/astroid-2.3.3-py3-none-any.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 21.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.14->joeynmt==0.0.1) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (0.16.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.8)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn->joeynmt==0.0.1) (2018.9)\n",
            "Collecting lazy-object-proxy==1.4.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 22.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: joeynmt, pyyaml\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=73054 sha256=c1198ffb12b088aaf489f5c473a170f06b899c645f2bc6228e4c8c4042b1b047\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uytgupvy/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3-cp36-cp36m-linux_x86_64.whl size=44229 sha256=a1eb3cbedc0cdd52555eeab70cf65ac675b51a99bc08e65c16b0d76b3a7d504a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/76/4d/a95b8dd7b452b69e8ed4f68b69e1b55e12c9c9624dd962b191\n",
            "Successfully built joeynmt pyyaml\n",
            "Installing collected packages: portalocker, sacrebleu, subword-nmt, pyyaml, mccabe, isort, lazy-object-proxy, typed-ast, astroid, pylint, joeynmt\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed astroid-2.3.3 isort-4.3.21 joeynmt-0.0.1 lazy-object-proxy-1.4.3 mccabe-0.6.1 portalocker-1.5.2 pylint-2.4.4 pyyaml-5.3 sacrebleu-1.4.3 subword-nmt-0.3.7 typed-ast-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-TyjtmXB1mL",
        "outputId": "9dbb3fa0-2eb9-4589-d48d-1378dff72494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Dendi Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.csv  test.bpe.ddn  test.ddn       train.bpe.en  train.en\n",
            "dev.bpe.ddn\tdev.ddn  test.bpe.en   test.en\t      train.csv\n",
            "dev.bpe.en\tdev.en\t test.csv      train.bpe.ddn  train.ddn\n",
            "bpe.codes.4000\tdev.csv  test.bpe.ddn  test.ddn       train.bpe.en  train.en\n",
            "dev.bpe.ddn\tdev.ddn  test.bpe.en   test.en\t      train.csv\n",
            "dev.bpe.en\tdev.en\t test.csv      train.bpe.ddn  train.ddn\n",
            "BPE Dendi Sentences\n",
            "Wom go hɑnu yom ndɑ hɑndu yom ndɑ I@@ ɔkɑci yom ndɑ jiiri yom lɑsɑbu kɑ bɛɛrɛ tɛ ǹ sɛ.\n",
            "Bɑ ngɑ di bɑ si, i go fɑihã mo bɑ kɑnkɑmi yom kunɑ ko bei kɑ kɑnkɑmi wɔ ko@@ kɑri no ɑ̀ cini hɛi.\n",
            "À nɑ ɑ dɑm gbei kunɑ bɑ kɑ ɑ ci Ikpɛ cɛn@@ ɑndikɔ ndɑ guruguz@@ ɑndikɔ ndɑ bɔrɔ fu@@ tu. Ammɑ ɑ du suuji domi zɑm kɑ bei kunɑ nɑ ɑ nɑ ɑ̀ tɛ zɑm kɑ nɑɑne kunɑ.\n",
            "Jiribi nge yom di no coobɑɑbɑize yom go bɑ kɑ tɔntɔnu. Ammɑ Yuifu yom kɑ ǹ go Gɑrɛku cinɛ sendi go gun@@ u wɔ kɑ yom go E@@ bulu cinɛ sendi gɑɑ. Ǹ go gun@@ u no zɑmɑ E@@ bulu cinɛ send@@ ikɔ yom nɑ sɑɑle kɑɑ ǹ sokoni yom gɑɑ fɑrisime fɔ kunɑ kɑ ǹ cini tɛ hɑnu kulu.\n",
            "Fɑrisi fɔndɔ bɔrɔ yom ndɑ Moisi fɔndɑ cooɑndikɔ yom go gun@@ u@@ gun@@ u kɑ cii: Bɔrɔ di go dulumkoni yom tɑ. À go ŋmɑɑ ndei mo.\n",
            "Combined BPE Vocab\n",
            "clai@@\n",
            "ɑ̀@@\n",
            "secu@@\n",
            "doni\n",
            "uda@@\n",
            "Hol@@\n",
            "Dɑfi@@\n",
            "Æ@@\n",
            "pos@@\n",
            "ɔkɑci\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlMitUHR8Qy-",
        "outputId": "14aad555-9122-4e1e-e7a0-98005cb68d3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.csv  test.bpe.ddn  test.ddn       train.bpe.en  train.en\n",
            "dev.bpe.ddn\tdev.ddn  test.bpe.en   test.en\t      train.csv\n",
            "dev.bpe.en\tdev.en\t test.csv      train.bpe.ddn  train.ddn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PIs1lY2hxMsl",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 100                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZBPFwT94WpI",
        "outputId": "9d4fba0a-5cb2-4608-84d8-9a9f0480769e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-02-11 21:42:00,612 Hello! This is Joey-NMT.\n",
            "2020-02-11 21:42:01,854 Total params: 12102656\n",
            "2020-02-11 21:42:01,855 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
            "2020-02-11 21:42:10,957 cfg.name                           : enddn_transformer\n",
            "2020-02-11 21:42:10,957 cfg.data.src                       : en\n",
            "2020-02-11 21:42:10,957 cfg.data.trg                       : ddn\n",
            "2020-02-11 21:42:10,957 cfg.data.train                     : data/enddn/train.bpe\n",
            "2020-02-11 21:42:10,957 cfg.data.dev                       : data/enddn/dev.bpe\n",
            "2020-02-11 21:42:10,957 cfg.data.test                      : data/enddn/test.bpe\n",
            "2020-02-11 21:42:10,957 cfg.data.level                     : bpe\n",
            "2020-02-11 21:42:10,957 cfg.data.lowercase                 : False\n",
            "2020-02-11 21:42:10,957 cfg.data.max_sent_length           : 100\n",
            "2020-02-11 21:42:10,957 cfg.data.src_vocab                 : data/enddn/vocab.txt\n",
            "2020-02-11 21:42:10,957 cfg.data.trg_vocab                 : data/enddn/vocab.txt\n",
            "2020-02-11 21:42:10,957 cfg.testing.beam_size              : 5\n",
            "2020-02-11 21:42:10,958 cfg.testing.alpha                  : 1.0\n",
            "2020-02-11 21:42:10,958 cfg.training.random_seed           : 42\n",
            "2020-02-11 21:42:10,958 cfg.training.optimizer             : adam\n",
            "2020-02-11 21:42:10,958 cfg.training.normalization         : tokens\n",
            "2020-02-11 21:42:10,958 cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2020-02-11 21:42:10,958 cfg.training.scheduling            : plateau\n",
            "2020-02-11 21:42:10,958 cfg.training.patience              : 5\n",
            "2020-02-11 21:42:10,958 cfg.training.learning_rate_factor  : 0.5\n",
            "2020-02-11 21:42:10,958 cfg.training.learning_rate_warmup  : 1000\n",
            "2020-02-11 21:42:10,958 cfg.training.decrease_factor       : 0.7\n",
            "2020-02-11 21:42:10,958 cfg.training.loss                  : crossentropy\n",
            "2020-02-11 21:42:10,958 cfg.training.learning_rate         : 0.0003\n",
            "2020-02-11 21:42:10,958 cfg.training.learning_rate_min     : 1e-08\n",
            "2020-02-11 21:42:10,958 cfg.training.weight_decay          : 0.0\n",
            "2020-02-11 21:42:10,958 cfg.training.label_smoothing       : 0.1\n",
            "2020-02-11 21:42:10,958 cfg.training.batch_size            : 4096\n",
            "2020-02-11 21:42:10,958 cfg.training.batch_type            : token\n",
            "2020-02-11 21:42:10,958 cfg.training.eval_batch_size       : 3600\n",
            "2020-02-11 21:42:10,958 cfg.training.eval_batch_type       : token\n",
            "2020-02-11 21:42:10,959 cfg.training.batch_multiplier      : 1\n",
            "2020-02-11 21:42:10,959 cfg.training.early_stopping_metric : ppl\n",
            "2020-02-11 21:42:10,959 cfg.training.epochs                : 100\n",
            "2020-02-11 21:42:10,959 cfg.training.validation_freq       : 1000\n",
            "2020-02-11 21:42:10,959 cfg.training.logging_freq          : 100\n",
            "2020-02-11 21:42:10,959 cfg.training.eval_metric           : bleu\n",
            "2020-02-11 21:42:10,959 cfg.training.model_dir             : models/enddn_transformer\n",
            "2020-02-11 21:42:10,959 cfg.training.overwrite             : False\n",
            "2020-02-11 21:42:10,959 cfg.training.shuffle               : True\n",
            "2020-02-11 21:42:10,959 cfg.training.use_cuda              : True\n",
            "2020-02-11 21:42:10,959 cfg.training.max_output_length     : 100\n",
            "2020-02-11 21:42:10,959 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2020-02-11 21:42:10,959 cfg.training.keep_last_ckpts       : 3\n",
            "2020-02-11 21:42:10,959 cfg.model.initializer              : xavier\n",
            "2020-02-11 21:42:10,959 cfg.model.bias_initializer         : zeros\n",
            "2020-02-11 21:42:10,959 cfg.model.init_gain                : 1.0\n",
            "2020-02-11 21:42:10,959 cfg.model.embed_initializer        : xavier\n",
            "2020-02-11 21:42:10,959 cfg.model.embed_init_gain          : 1.0\n",
            "2020-02-11 21:42:10,959 cfg.model.tied_embeddings          : True\n",
            "2020-02-11 21:42:10,960 cfg.model.tied_softmax             : True\n",
            "2020-02-11 21:42:10,960 cfg.model.encoder.type             : transformer\n",
            "2020-02-11 21:42:10,960 cfg.model.encoder.num_layers       : 6\n",
            "2020-02-11 21:42:10,960 cfg.model.encoder.num_heads        : 4\n",
            "2020-02-11 21:42:10,960 cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2020-02-11 21:42:10,960 cfg.model.encoder.embeddings.scale : True\n",
            "2020-02-11 21:42:10,960 cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2020-02-11 21:42:10,960 cfg.model.encoder.hidden_size      : 256\n",
            "2020-02-11 21:42:10,960 cfg.model.encoder.ff_size          : 1024\n",
            "2020-02-11 21:42:10,960 cfg.model.encoder.dropout          : 0.3\n",
            "2020-02-11 21:42:10,960 cfg.model.decoder.type             : transformer\n",
            "2020-02-11 21:42:10,960 cfg.model.decoder.num_layers       : 6\n",
            "2020-02-11 21:42:10,960 cfg.model.decoder.num_heads        : 4\n",
            "2020-02-11 21:42:10,960 cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2020-02-11 21:42:10,960 cfg.model.decoder.embeddings.scale : True\n",
            "2020-02-11 21:42:10,960 cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2020-02-11 21:42:10,960 cfg.model.decoder.hidden_size      : 256\n",
            "2020-02-11 21:42:10,960 cfg.model.decoder.ff_size          : 1024\n",
            "2020-02-11 21:42:10,960 cfg.model.decoder.dropout          : 0.3\n",
            "2020-02-11 21:42:10,961 Data set sizes: \n",
            "\ttrain 6937,\n",
            "\tvalid 1000,\n",
            "\ttest 179\n",
            "2020-02-11 21:42:10,961 First training example:\n",
            "\t[SRC] and shall come forth to decei@@ ve the nations which are in the four cor@@ n@@ ers of the earth, Go@@ g and Ma@@ go@@ g@@ , to gather them together to the war@@ : the number of whom is as the sand of the sea@@ .\n",
            "\t[TRG] À kɑɑ hunu kɑ hɑndunyɑ dimi yom kɑ ǹ goono hɑndunyɑ cɛn@@ jɛ tɑɑci gɑɑ dɛr@@ ɑndi, wɑtom G@@ ɔ@@ gu ndɑ Mɑ@@ g@@ ɔ@@ gu. À gɑ ǹ meigu wɑngu tɛyom sɛ. Ǹ bɑyom bisɑ tɛku tɑɑ@@ si.\n",
            "2020-02-11 21:42:10,961 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) kɑ (5) the (6) ɑ̀ (7) and (8) yom (9) of\n",
            "2020-02-11 21:42:10,961 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) kɑ (5) the (6) ɑ̀ (7) and (8) yom (9) of\n",
            "2020-02-11 21:42:10,961 Number of Src words (types): 4072\n",
            "2020-02-11 21:42:10,962 Number of Trg words (types): 4072\n",
            "2020-02-11 21:42:10,962 Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4072),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4072))\n",
            "2020-02-11 21:42:10,965 EPOCH 1\n",
            "2020-02-11 21:42:20,710 Epoch   1: total training loss 466.82\n",
            "2020-02-11 21:42:20,710 EPOCH 2\n",
            "2020-02-11 21:42:22,356 Epoch   2 Step:      100 Batch Loss:     5.057078 Tokens per Sec:    21770, Lr: 0.000300\n",
            "2020-02-11 21:42:30,128 Epoch   2: total training loss 427.84\n",
            "2020-02-11 21:42:30,128 EPOCH 3\n",
            "2020-02-11 21:42:33,472 Epoch   3 Step:      200 Batch Loss:     4.733548 Tokens per Sec:    21919, Lr: 0.000300\n",
            "2020-02-11 21:42:39,488 Epoch   3: total training loss 402.21\n",
            "2020-02-11 21:42:39,488 EPOCH 4\n",
            "2020-02-11 21:42:44,382 Epoch   4 Step:      300 Batch Loss:     4.514215 Tokens per Sec:    22033, Lr: 0.000300\n",
            "2020-02-11 21:42:48,816 Epoch   4: total training loss 377.79\n",
            "2020-02-11 21:42:48,816 EPOCH 5\n",
            "2020-02-11 21:42:55,551 Epoch   5 Step:      400 Batch Loss:     4.377610 Tokens per Sec:    22574, Lr: 0.000300\n",
            "2020-02-11 21:42:58,200 Epoch   5: total training loss 358.39\n",
            "2020-02-11 21:42:58,200 EPOCH 6\n",
            "2020-02-11 21:43:06,582 Epoch   6 Step:      500 Batch Loss:     4.053829 Tokens per Sec:    22371, Lr: 0.000300\n",
            "2020-02-11 21:43:07,530 Epoch   6: total training loss 351.36\n",
            "2020-02-11 21:43:07,530 EPOCH 7\n",
            "2020-02-11 21:43:16,953 Epoch   7: total training loss 344.26\n",
            "2020-02-11 21:43:16,953 EPOCH 8\n",
            "2020-02-11 21:43:17,522 Epoch   8 Step:      600 Batch Loss:     3.883054 Tokens per Sec:    22277, Lr: 0.000300\n",
            "2020-02-11 21:43:26,343 Epoch   8: total training loss 334.57\n",
            "2020-02-11 21:43:26,343 EPOCH 9\n",
            "2020-02-11 21:43:28,430 Epoch   9 Step:      700 Batch Loss:     3.813072 Tokens per Sec:    22721, Lr: 0.000300\n",
            "2020-02-11 21:43:35,712 Epoch   9: total training loss 325.15\n",
            "2020-02-11 21:43:35,713 EPOCH 10\n",
            "2020-02-11 21:43:39,359 Epoch  10 Step:      800 Batch Loss:     3.651042 Tokens per Sec:    22559, Lr: 0.000300\n",
            "2020-02-11 21:43:45,051 Epoch  10: total training loss 312.56\n",
            "2020-02-11 21:43:45,051 EPOCH 11\n",
            "2020-02-11 21:43:50,310 Epoch  11 Step:      900 Batch Loss:     3.536031 Tokens per Sec:    22303, Lr: 0.000300\n",
            "2020-02-11 21:43:54,423 Epoch  11: total training loss 304.78\n",
            "2020-02-11 21:43:54,423 EPOCH 12\n",
            "2020-02-11 21:44:01,331 Epoch  12 Step:     1000 Batch Loss:     3.727648 Tokens per Sec:    22200, Lr: 0.000300\n",
            "2020-02-11 21:44:27,997 Hooray! New best validation result [ppl]!\n",
            "2020-02-11 21:44:27,998 Saving new checkpoint.\n",
            "2020-02-11 21:44:28,277 Example #0\n",
            "2020-02-11 21:44:28,278 \tSource:     I thank him that enabled me, even Christ Jesus our Lord, for that he counted me faithful, appointing me to his service;\n",
            "2020-02-11 21:44:28,278 \tReference:  A go sɑɑbu i Kpe Yesu Mɛsiyɑ sɛ, ngɑ kɑ ɑ̀ nɑ ɑ no gɑɑbi. A gɑ kɑ ɑ̀ sɑɑbu domi ɑ̀ nɑ ɑ lɑsɑbu nɑɑnekpɛ kɑ ɑ dɑm ngɑ gbei kunɑ.\n",
            "2020-02-11 21:44:28,278 \tHypothesis: A go kɑ ɑ dɑm ɑ sɛ kɑ ɑ go kɑ ɑ dɑm ɑ sɛ kɑ ɑ go kɑ ɑ dɑm ɑ sɛ. À nɑ ɑ dɑm ɑ sɛ kɑ ɑ go kɑ ɑ go kɑ ɑ go kɑ ɑ go kɑ ɑ dɑm ɑ sɛ.\n",
            "2020-02-11 21:44:28,278 Example #1\n",
            "2020-02-11 21:44:28,278 \tSource:     Now lettest thou thy servant depart, Lord, According to thy word, in peace;\n",
            "2020-02-11 21:44:28,278 \tReference:  Mɑɑsɑnkulu Kpe, n mɑ tu n bɑnyɑ mɑ kpei ndɑ bɑɑni zɑngɑ n Sendɑ cii.\n",
            "2020-02-11 21:44:28,278 \tHypothesis: N mɑ n mɑ n bɔm n mɑ n bɔm n mɑ n bɔm n mɑ n bɔm n mɑ n bɔm n mɑ n bɔm n mɑ n mɑ n mɑ n sɛ.\n",
            "2020-02-11 21:44:28,278 Example #2\n",
            "2020-02-11 21:44:28,279 \tSource:     if so be that God is one, and he shall justify the circumcision by faith, and the uncircumcision through faith.\n",
            "2020-02-11 21:44:28,279 \tReference:  Ikpɛ fɔlɔnku yɑ gɑ bɑnguize yom cɛɑndi susu nɑɑne gɑɑ. À go zɑm kɑ dɑmbɑnguize yom mo cɛɑndi susu nɑɑne gɑɑ.\n",
            "2020-02-11 21:44:28,279 \tHypothesis: Ikpɛ nɑ ɑ̀ tɛ ndɑ Ikpɛ nɑ ɑ̀ tɛ ndɑ ɑ̀ mɑ ci Ikpɛ dɔntɔneize yom kɑ ɑ̀ ci Ikpɛ nɑ ɑ̀ tɛ ndɑ ɑ̀ sɛ ndɑ ɑ̀ mɑ ci Mɛsiyɑ Mɛsiyɑ kunɑ.\n",
            "2020-02-11 21:44:28,279 Example #3\n",
            "2020-02-11 21:44:28,279 \tSource:     And from that city many of the Samaritans believed on him because of the word of the woman, who testified, He told me all things that ever I did.\n",
            "2020-02-11 21:44:28,279 \tReference:  Wɑngɑrɑ ngɑ di, Sɑmɑriɑncɛ boobo nɑɑne Yesu gɑɑ weibɔrɔ di sendɑ sɑbu sɛ. Weibɔrɔ di tɛ sɛdɑ kɑ cii: Hɛ kulu kɑ ɑ jinɑ kɑ tɛ, ɑ̀ nɑ ɑ̀ cii ɑ sɛ.\n",
            "2020-02-11 21:44:28,279 \tHypothesis: Ǹ nɑ ǹ dɑm ǹ mɑ tɛ kɑ cii: A go kɑ Ikpɛ nɑ ɑ dɑm kɑ ɑ̀ sɛ kɑ ɑ go kɑ ɑ dɑm ɑ sɛ.\n",
            "2020-02-11 21:44:28,279 Validation result (greedy) at epoch  12, step     1000: bleu:   2.13, loss: 108471.5078, ppl:  33.6451, duration: 26.9481s\n",
            "2020-02-11 21:44:30,783 Epoch  12: total training loss 301.88\n",
            "2020-02-11 21:44:30,783 EPOCH 13\n",
            "2020-02-11 21:44:39,302 Epoch  13 Step:     1100 Batch Loss:     3.400239 Tokens per Sec:    22253, Lr: 0.000300\n",
            "2020-02-11 21:44:40,182 Epoch  13: total training loss 291.00\n",
            "2020-02-11 21:44:40,182 EPOCH 14\n",
            "2020-02-11 21:44:49,486 Epoch  14: total training loss 284.94\n",
            "2020-02-11 21:44:49,486 EPOCH 15\n",
            "2020-02-11 21:44:50,266 Epoch  15 Step:     1200 Batch Loss:     3.345992 Tokens per Sec:    20308, Lr: 0.000300\n",
            "2020-02-11 21:44:58,919 Epoch  15: total training loss 283.64\n",
            "2020-02-11 21:44:58,919 EPOCH 16\n",
            "2020-02-11 21:45:01,232 Epoch  16 Step:     1300 Batch Loss:     3.314917 Tokens per Sec:    22205, Lr: 0.000300\n",
            "2020-02-11 21:45:08,232 Epoch  16: total training loss 277.21\n",
            "2020-02-11 21:45:08,232 EPOCH 17\n",
            "2020-02-11 21:45:12,056 Epoch  17 Step:     1400 Batch Loss:     3.305284 Tokens per Sec:    22689, Lr: 0.000300\n",
            "2020-02-11 21:45:17,606 Epoch  17: total training loss 271.44\n",
            "2020-02-11 21:45:17,606 EPOCH 18\n",
            "2020-02-11 21:45:22,991 Epoch  18 Step:     1500 Batch Loss:     3.106214 Tokens per Sec:    22490, Lr: 0.000300\n",
            "2020-02-11 21:45:26,948 Epoch  18: total training loss 262.52\n",
            "2020-02-11 21:45:26,948 EPOCH 19\n",
            "2020-02-11 21:45:33,979 Epoch  19 Step:     1600 Batch Loss:     2.940624 Tokens per Sec:    22366, Lr: 0.000300\n",
            "2020-02-11 21:45:36,279 Epoch  19: total training loss 256.90\n",
            "2020-02-11 21:45:36,279 EPOCH 20\n",
            "2020-02-11 21:45:44,852 Epoch  20 Step:     1700 Batch Loss:     3.002654 Tokens per Sec:    22265, Lr: 0.000300\n",
            "2020-02-11 21:45:45,638 Epoch  20: total training loss 255.76\n",
            "2020-02-11 21:45:45,638 EPOCH 21\n",
            "2020-02-11 21:45:54,990 Epoch  21: total training loss 248.13\n",
            "2020-02-11 21:45:54,990 EPOCH 22\n",
            "2020-02-11 21:45:55,905 Epoch  22 Step:     1800 Batch Loss:     2.747471 Tokens per Sec:    20563, Lr: 0.000300\n",
            "2020-02-11 21:46:04,398 Epoch  22: total training loss 246.31\n",
            "2020-02-11 21:46:04,399 EPOCH 23\n",
            "2020-02-11 21:46:06,813 Epoch  23 Step:     1900 Batch Loss:     2.851752 Tokens per Sec:    22051, Lr: 0.000300\n",
            "2020-02-11 21:46:13,726 Epoch  23: total training loss 237.90\n",
            "2020-02-11 21:46:13,727 EPOCH 24\n",
            "2020-02-11 21:46:17,824 Epoch  24 Step:     2000 Batch Loss:     2.219049 Tokens per Sec:    21985, Lr: 0.000300\n",
            "2020-02-11 21:46:34,117 Hooray! New best validation result [ppl]!\n",
            "2020-02-11 21:46:34,117 Saving new checkpoint.\n",
            "2020-02-11 21:46:34,412 Example #0\n",
            "2020-02-11 21:46:34,412 \tSource:     I thank him that enabled me, even Christ Jesus our Lord, for that he counted me faithful, appointing me to his service;\n",
            "2020-02-11 21:46:34,412 \tReference:  A go sɑɑbu i Kpe Yesu Mɛsiyɑ sɛ, ngɑ kɑ ɑ̀ nɑ ɑ no gɑɑbi. A gɑ kɑ ɑ̀ sɑɑbu domi ɑ̀ nɑ ɑ lɑsɑbu nɑɑnekpɛ kɑ ɑ dɑm ngɑ gbei kunɑ.\n",
            "2020-02-11 21:46:34,412 \tHypothesis: A go kɑ ɑ ŋmɑɑrɛ ɑ mɑ ɑ no Mɛsiyɑ Yesu Mɛsiyɑ dɔ. À nɑ ɑ no zɑmɑ i mɑ ɑ no himmɑ kɑ ɑ nɑ ɑ no himmɑ tɛ ɑ sɛ.\n",
            "2020-02-11 21:46:34,412 Example #1\n",
            "2020-02-11 21:46:34,412 \tSource:     Now lettest thou thy servant depart, Lord, According to thy word, in peace;\n",
            "2020-02-11 21:46:34,412 \tReference:  Mɑɑsɑnkulu Kpe, n mɑ tu n bɑnyɑ mɑ kpei ndɑ bɑɑni zɑngɑ n Sendɑ cii.\n",
            "2020-02-11 21:46:34,412 \tHypothesis: N mɑ n ŋmɔne ibɛrɛ yom tɛ n sɛ kɑ n mɑ n ŋmɑɑrɛ n mɑ n cee yom wi.\n",
            "2020-02-11 21:46:34,413 Example #2\n",
            "2020-02-11 21:46:34,413 \tSource:     if so be that God is one, and he shall justify the circumcision by faith, and the uncircumcision through faith.\n",
            "2020-02-11 21:46:34,413 \tReference:  Ikpɛ fɔlɔnku yɑ gɑ bɑnguize yom cɛɑndi susu nɑɑne gɑɑ. À go zɑm kɑ dɑmbɑnguize yom mo cɛɑndi susu nɑɑne gɑɑ.\n",
            "2020-02-11 21:46:34,413 \tHypothesis: De Ikpɛ nɑ ɑ̀ tɛ bɔrɔ kɑ ɑ̀ si nɑɑne nɑɑne gɑɑ ɑmmɑ ɑ̀ si nɑɑne nɑɑne nɑɑne gɑɑ. À nɑ cɛɑndiyom susu nɑɑne nɑɑne nɑɑne gɑɑ ndɑ cɛɑndiyom susu nɑɑne kɑ ɑ̀ go nɑɑne nɑɑne gɑɑ.\n",
            "2020-02-11 21:46:34,413 Example #3\n",
            "2020-02-11 21:46:34,413 \tSource:     And from that city many of the Samaritans believed on him because of the word of the woman, who testified, He told me all things that ever I did.\n",
            "2020-02-11 21:46:34,413 \tReference:  Wɑngɑrɑ ngɑ di, Sɑmɑriɑncɛ boobo nɑɑne Yesu gɑɑ weibɔrɔ di sendɑ sɑbu sɛ. Weibɔrɔ di tɛ sɛdɑ kɑ cii: Hɛ kulu kɑ ɑ jinɑ kɑ tɛ, ɑ̀ nɑ ɑ̀ cii ɑ sɛ.\n",
            "2020-02-11 21:46:34,413 \tHypothesis: Bɔrɔ boobo kɑ ǹ goono Yerusɑlɛm wɑngɑrɑ di kunɑ, ǹ nɑ ɑ̀ bei kɑ ɑ̀ nɑ ɑ̀ cii ɑ̀ sɛ hɛ kulu kɑ ɑ̀ go cii ɑ sɛ.\n",
            "2020-02-11 21:46:34,413 Validation result (greedy) at epoch  24, step     2000: bleu:   6.45, loss: 93253.1953, ppl:  20.5446, duration: 16.5888s\n",
            "2020-02-11 21:46:39,700 Epoch  24: total training loss 234.19\n",
            "2020-02-11 21:46:39,701 EPOCH 25\n",
            "2020-02-11 21:46:45,334 Epoch  25 Step:     2100 Batch Loss:     2.635937 Tokens per Sec:    22419, Lr: 0.000300\n",
            "2020-02-11 21:46:49,023 Epoch  25: total training loss 232.52\n",
            "2020-02-11 21:46:49,023 EPOCH 26\n",
            "2020-02-11 21:46:56,288 Epoch  26 Step:     2200 Batch Loss:     2.719319 Tokens per Sec:    22082, Lr: 0.000300\n",
            "2020-02-11 21:46:58,459 Epoch  26: total training loss 228.62\n",
            "2020-02-11 21:46:58,459 EPOCH 27\n",
            "2020-02-11 21:47:07,200 Epoch  27 Step:     2300 Batch Loss:     2.639296 Tokens per Sec:    22571, Lr: 0.000300\n",
            "2020-02-11 21:47:07,752 Epoch  27: total training loss 221.53\n",
            "2020-02-11 21:47:07,753 EPOCH 28\n",
            "2020-02-11 21:47:17,104 Epoch  28: total training loss 217.86\n",
            "2020-02-11 21:47:17,104 EPOCH 29\n",
            "2020-02-11 21:47:18,232 Epoch  29 Step:     2400 Batch Loss:     2.459951 Tokens per Sec:    22358, Lr: 0.000300\n",
            "2020-02-11 21:47:26,395 Epoch  29: total training loss 213.73\n",
            "2020-02-11 21:47:26,395 EPOCH 30\n",
            "2020-02-11 21:47:29,126 Epoch  30 Step:     2500 Batch Loss:     2.554981 Tokens per Sec:    22796, Lr: 0.000300\n",
            "2020-02-11 21:47:35,678 Epoch  30: total training loss 210.35\n",
            "2020-02-11 21:47:35,678 EPOCH 31\n",
            "2020-02-11 21:47:40,015 Epoch  31 Step:     2600 Batch Loss:     2.485868 Tokens per Sec:    22673, Lr: 0.000300\n",
            "2020-02-11 21:47:45,014 Epoch  31: total training loss 209.64\n",
            "2020-02-11 21:47:45,014 EPOCH 32\n",
            "2020-02-11 21:47:50,844 Epoch  32 Step:     2700 Batch Loss:     2.629287 Tokens per Sec:    22271, Lr: 0.000300\n",
            "2020-02-11 21:47:54,376 Epoch  32: total training loss 208.74\n",
            "2020-02-11 21:47:54,377 EPOCH 33\n",
            "2020-02-11 21:48:01,729 Epoch  33 Step:     2800 Batch Loss:     2.780900 Tokens per Sec:    22161, Lr: 0.000300\n",
            "2020-02-11 21:48:03,711 Epoch  33: total training loss 203.27\n",
            "2020-02-11 21:48:03,711 EPOCH 34\n",
            "2020-02-11 21:48:12,601 Epoch  34 Step:     2900 Batch Loss:     2.262625 Tokens per Sec:    22687, Lr: 0.000300\n",
            "2020-02-11 21:48:12,933 Epoch  34: total training loss 197.59\n",
            "2020-02-11 21:48:12,933 EPOCH 35\n",
            "2020-02-11 21:48:22,288 Epoch  35: total training loss 195.98\n",
            "2020-02-11 21:48:22,289 EPOCH 36\n",
            "2020-02-11 21:48:23,518 Epoch  36 Step:     3000 Batch Loss:     2.221372 Tokens per Sec:    22409, Lr: 0.000300\n",
            "2020-02-11 21:48:36,598 Hooray! New best validation result [ppl]!\n",
            "2020-02-11 21:48:36,599 Saving new checkpoint.\n",
            "2020-02-11 21:48:36,892 Example #0\n",
            "2020-02-11 21:48:36,893 \tSource:     I thank him that enabled me, even Christ Jesus our Lord, for that he counted me faithful, appointing me to his service;\n",
            "2020-02-11 21:48:36,893 \tReference:  A go sɑɑbu i Kpe Yesu Mɛsiyɑ sɛ, ngɑ kɑ ɑ̀ nɑ ɑ no gɑɑbi. A gɑ kɑ ɑ̀ sɑɑbu domi ɑ̀ nɑ ɑ lɑsɑbu nɑɑnekpɛ kɑ ɑ dɑm ngɑ gbei kunɑ.\n",
            "2020-02-11 21:48:36,893 \tHypothesis: A go kɑ ɑ dɑm kɑmbɛ, ɑ̀ go kɑ ɑ no i Kpe Yesu Mɛsiyɑ gbei kɑ ɑ nɑ ɑ no ndɑ. À go kɑ ɑ cɛbɛ ɑ sɛ.\n",
            "2020-02-11 21:48:36,893 Example #1\n",
            "2020-02-11 21:48:36,893 \tSource:     Now lettest thou thy servant depart, Lord, According to thy word, in peace;\n",
            "2020-02-11 21:48:36,893 \tReference:  Mɑɑsɑnkulu Kpe, n mɑ tu n bɑnyɑ mɑ kpei ndɑ bɑɑni zɑngɑ n Sendɑ cii.\n",
            "2020-02-11 21:48:36,894 \tHypothesis: N gundɑ mɑɑ n nyɑize fɔ kɑ ɑ̀ goono n bɑndɑ tɑndɑ dɔ kɑ n gundɑ n nyɑize yom kunɑ.\n",
            "2020-02-11 21:48:36,898 Example #2\n",
            "2020-02-11 21:48:36,899 \tSource:     if so be that God is one, and he shall justify the circumcision by faith, and the uncircumcision through faith.\n",
            "2020-02-11 21:48:36,899 \tReference:  Ikpɛ fɔlɔnku yɑ gɑ bɑnguize yom cɛɑndi susu nɑɑne gɑɑ. À go zɑm kɑ dɑmbɑnguize yom mo cɛɑndi susu nɑɑne gɑɑ.\n",
            "2020-02-11 21:48:36,899 \tHypothesis: De bɔrɔ fɔ go kɑ Ikpɛ cɛɑndi susu gɑɑ, ɑkpɛ gɑ dulum tɛ cɛyom susu nɑɑne gɑɑ. À go nɑɑne nɑɑne nɑɑne gɑɑ kɑ ɑ̀ go nɑɑne nɑɑne nɑɑne gɑɑ.\n",
            "2020-02-11 21:48:36,899 Example #3\n",
            "2020-02-11 21:48:36,900 \tSource:     And from that city many of the Samaritans believed on him because of the word of the woman, who testified, He told me all things that ever I did.\n",
            "2020-02-11 21:48:36,900 \tReference:  Wɑngɑrɑ ngɑ di, Sɑmɑriɑncɛ boobo nɑɑne Yesu gɑɑ weibɔrɔ di sendɑ sɑbu sɛ. Weibɔrɔ di tɛ sɛdɑ kɑ cii: Hɛ kulu kɑ ɑ jinɑ kɑ tɛ, ɑ̀ nɑ ɑ̀ cii ɑ sɛ.\n",
            "2020-02-11 21:48:36,900 \tHypothesis: Sɑɑ kɑ bɔrɔ boobo goono Sɑmɑri lɑɑbu bɔrɔ boobo nɑɑne ɑ̀ gɑɑ. À go sendi ngɑ di zɑmɑ ngɑ yɑ ci bɔrɔ kulu kɑ ɑ̀ go cii ɑ sɛ hɛ kulu kɑ ɑ̀ go tɛ.\n",
            "2020-02-11 21:48:36,900 Validation result (greedy) at epoch  36, step     3000: bleu:   9.49, loss: 86804.9688, ppl:  16.6697, duration: 13.3814s\n",
            "2020-02-11 21:48:45,060 Epoch  36: total training loss 193.10\n",
            "2020-02-11 21:48:45,060 EPOCH 37\n",
            "2020-02-11 21:48:47,823 Epoch  37 Step:     3100 Batch Loss:     2.505269 Tokens per Sec:    22803, Lr: 0.000300\n",
            "2020-02-11 21:48:54,379 Epoch  37: total training loss 186.42\n",
            "2020-02-11 21:48:54,380 EPOCH 38\n",
            "2020-02-11 21:48:58,884 Epoch  38 Step:     3200 Batch Loss:     2.424752 Tokens per Sec:    22536, Lr: 0.000300\n",
            "2020-02-11 21:49:03,727 Epoch  38: total training loss 187.98\n",
            "2020-02-11 21:49:03,727 EPOCH 39\n",
            "2020-02-11 21:49:09,708 Epoch  39 Step:     3300 Batch Loss:     2.139492 Tokens per Sec:    22600, Lr: 0.000300\n",
            "2020-02-11 21:49:12,992 Epoch  39: total training loss 183.20\n",
            "2020-02-11 21:49:12,992 EPOCH 40\n",
            "2020-02-11 21:49:20,698 Epoch  40 Step:     3400 Batch Loss:     2.187208 Tokens per Sec:    22198, Lr: 0.000300\n",
            "2020-02-11 21:49:22,354 Epoch  40: total training loss 180.34\n",
            "2020-02-11 21:49:22,354 EPOCH 41\n",
            "2020-02-11 21:49:31,563 Epoch  41 Step:     3500 Batch Loss:     2.217572 Tokens per Sec:    22404, Lr: 0.000300\n",
            "2020-02-11 21:49:31,676 Epoch  41: total training loss 178.96\n",
            "2020-02-11 21:49:31,676 EPOCH 42\n",
            "2020-02-11 21:49:41,010 Epoch  42: total training loss 176.18\n",
            "2020-02-11 21:49:41,010 EPOCH 43\n",
            "2020-02-11 21:49:42,469 Epoch  43 Step:     3600 Batch Loss:     2.271184 Tokens per Sec:    22466, Lr: 0.000300\n",
            "2020-02-11 21:49:50,368 Epoch  43: total training loss 175.12\n",
            "2020-02-11 21:49:50,368 EPOCH 44\n",
            "2020-02-11 21:49:53,355 Epoch  44 Step:     3700 Batch Loss:     2.227483 Tokens per Sec:    21987, Lr: 0.000300\n",
            "2020-02-11 21:49:59,732 Epoch  44: total training loss 169.17\n",
            "2020-02-11 21:49:59,733 EPOCH 45\n",
            "2020-02-11 21:50:04,342 Epoch  45 Step:     3800 Batch Loss:     1.944784 Tokens per Sec:    22190, Lr: 0.000300\n",
            "2020-02-11 21:50:09,020 Epoch  45: total training loss 167.45\n",
            "2020-02-11 21:50:09,020 EPOCH 46\n",
            "2020-02-11 21:50:15,276 Epoch  46 Step:     3900 Batch Loss:     1.900492 Tokens per Sec:    22473, Lr: 0.000300\n",
            "2020-02-11 21:50:18,330 Epoch  46: total training loss 164.67\n",
            "2020-02-11 21:50:18,331 EPOCH 47\n",
            "2020-02-11 21:50:26,242 Epoch  47 Step:     4000 Batch Loss:     2.042727 Tokens per Sec:    22451, Lr: 0.000300\n",
            "2020-02-11 21:50:38,966 Hooray! New best validation result [ppl]!\n",
            "2020-02-11 21:50:38,966 Saving new checkpoint.\n",
            "2020-02-11 21:50:39,197 Example #0\n",
            "2020-02-11 21:50:39,197 \tSource:     I thank him that enabled me, even Christ Jesus our Lord, for that he counted me faithful, appointing me to his service;\n",
            "2020-02-11 21:50:39,197 \tReference:  A go sɑɑbu i Kpe Yesu Mɛsiyɑ sɛ, ngɑ kɑ ɑ̀ nɑ ɑ no gɑɑbi. A gɑ kɑ ɑ̀ sɑɑbu domi ɑ̀ nɑ ɑ lɑsɑbu nɑɑnekpɛ kɑ ɑ dɑm ngɑ gbei kunɑ.\n",
            "2020-02-11 21:50:39,197 \tHypothesis: A go kɑ ɑ hɑmɑ kɑ ɑ dɑm Mɛsiyɑ Yesu kunɑ kɑ ɑ̀ ci i nɑɑne kɑ ɑ̀ nɑ ɑ cɛbɛ ndɑ zɑngɑ i gbei kɑ ɑ̀ ci ɑ binebɑɑkɔ, i yom sɛ i go kɑ ɑ cɛbɛ wom sɛ.\n",
            "2020-02-11 21:50:39,197 Example #1\n",
            "2020-02-11 21:50:39,198 \tSource:     Now lettest thou thy servant depart, Lord, According to thy word, in peace;\n",
            "2020-02-11 21:50:39,198 \tReference:  Mɑɑsɑnkulu Kpe, n mɑ tu n bɑnyɑ mɑ kpei ndɑ bɑɑni zɑngɑ n Sendɑ cii.\n",
            "2020-02-11 21:50:39,198 \tHypothesis: N gundɑ n kom, n mɑ bɑɑ n mɑ tɛ n bɑɑbɑ ndɑ n gɑɑbi kɑ ɑ̀ goono n bine kunɑ ndɑ n gɑɑbi kunɑ.\n",
            "2020-02-11 21:50:39,198 Example #2\n",
            "2020-02-11 21:50:39,198 \tSource:     if so be that God is one, and he shall justify the circumcision by faith, and the uncircumcision through faith.\n",
            "2020-02-11 21:50:39,198 \tReference:  Ikpɛ fɔlɔnku yɑ gɑ bɑnguize yom cɛɑndi susu nɑɑne gɑɑ. À go zɑm kɑ dɑmbɑnguize yom mo cɛɑndi susu nɑɑne gɑɑ.\n",
            "2020-02-11 21:50:39,198 \tHypothesis: De Ikpɛ yɑ ci bɔrɔ kɑ ɑ̀ go nɑɑne gɑɑ, ɑ̀ gɑ cɛɑndiyom susu cɛɑndi susu nɑɑne gɑɑ. À go nɑɑne kɑ ɑ̀ go nɑɑne nɑɑne nɑɑne nɑɑne gɑɑ nɑɑne gɑɑ.\n",
            "2020-02-11 21:50:39,198 Example #3\n",
            "2020-02-11 21:50:39,198 \tSource:     And from that city many of the Samaritans believed on him because of the word of the woman, who testified, He told me all things that ever I did.\n",
            "2020-02-11 21:50:39,199 \tReference:  Wɑngɑrɑ ngɑ di, Sɑmɑriɑncɛ boobo nɑɑne Yesu gɑɑ weibɔrɔ di sendɑ sɑbu sɛ. Weibɔrɔ di tɛ sɛdɑ kɑ cii: Hɛ kulu kɑ ɑ jinɑ kɑ tɛ, ɑ̀ nɑ ɑ̀ cii ɑ sɛ.\n",
            "2020-02-11 21:50:39,199 \tHypothesis: Nungu di no Sɑmɑri bɔrɔ boobo go nɑɑne ɑ̀ gɑɑ. Ǹ go nɑɑne ɑ̀ gɑɑ zɑmɑ ɑ̀ gɑɑ no bɔrɔ kulu kɑ ɑ̀ go sendi. À go cii ɑ sɛ hɛ kulu kɑ ɑ̀ tɛ ɑ sɛ.\n",
            "2020-02-11 21:50:39,199 Validation result (greedy) at epoch  47, step     4000: bleu:  11.08, loss: 85529.5078, ppl:  15.9946, duration: 12.9569s\n",
            "2020-02-11 21:50:40,716 Epoch  47: total training loss 163.43\n",
            "2020-02-11 21:50:40,716 EPOCH 48\n",
            "2020-02-11 21:50:50,026 Epoch  48 Step:     4100 Batch Loss:     1.734297 Tokens per Sec:    22477, Lr: 0.000300\n",
            "2020-02-11 21:50:50,026 Epoch  48: total training loss 162.28\n",
            "2020-02-11 21:50:50,026 EPOCH 49\n",
            "2020-02-11 21:50:59,480 Epoch  49: total training loss 160.22\n",
            "2020-02-11 21:50:59,480 EPOCH 50\n",
            "2020-02-11 21:51:01,053 Epoch  50 Step:     4200 Batch Loss:     1.837417 Tokens per Sec:    22633, Lr: 0.000300\n",
            "2020-02-11 21:51:08,719 Epoch  50: total training loss 154.90\n",
            "2020-02-11 21:51:08,719 EPOCH 51\n",
            "2020-02-11 21:51:11,853 Epoch  51 Step:     4300 Batch Loss:     2.068486 Tokens per Sec:    21868, Lr: 0.000300\n",
            "2020-02-11 21:51:18,141 Epoch  51: total training loss 155.26\n",
            "2020-02-11 21:51:18,142 EPOCH 52\n",
            "2020-02-11 21:51:22,868 Epoch  52 Step:     4400 Batch Loss:     1.779647 Tokens per Sec:    22504, Lr: 0.000300\n",
            "2020-02-11 21:51:27,484 Epoch  52: total training loss 152.37\n",
            "2020-02-11 21:51:27,485 EPOCH 53\n",
            "2020-02-11 21:51:33,663 Epoch  53 Step:     4500 Batch Loss:     1.731201 Tokens per Sec:    22349, Lr: 0.000300\n",
            "2020-02-11 21:51:36,810 Epoch  53: total training loss 149.85\n",
            "2020-02-11 21:51:36,810 EPOCH 54\n",
            "2020-02-11 21:51:44,625 Epoch  54 Step:     4600 Batch Loss:     1.723928 Tokens per Sec:    22540, Lr: 0.000300\n",
            "2020-02-11 21:51:46,128 Epoch  54: total training loss 146.70\n",
            "2020-02-11 21:51:46,128 EPOCH 55\n",
            "2020-02-11 21:51:55,566 Epoch  55: total training loss 145.03\n",
            "2020-02-11 21:51:55,566 EPOCH 56\n",
            "2020-02-11 21:51:55,698 Epoch  56 Step:     4700 Batch Loss:     1.374477 Tokens per Sec:    17443, Lr: 0.000300\n",
            "2020-02-11 21:52:05,084 Epoch  56: total training loss 143.97\n",
            "2020-02-11 21:52:05,084 EPOCH 57\n",
            "2020-02-11 21:52:06,670 Epoch  57 Step:     4800 Batch Loss:     0.828515 Tokens per Sec:    21727, Lr: 0.000300\n",
            "2020-02-11 21:52:14,449 Epoch  57: total training loss 140.28\n",
            "2020-02-11 21:52:14,449 EPOCH 58\n",
            "2020-02-11 21:52:17,804 Epoch  58 Step:     4900 Batch Loss:     1.678988 Tokens per Sec:    21879, Lr: 0.000300\n",
            "2020-02-11 21:52:23,941 Epoch  58: total training loss 139.05\n",
            "2020-02-11 21:52:23,941 EPOCH 59\n",
            "2020-02-11 21:52:28,831 Epoch  59 Step:     5000 Batch Loss:     1.683139 Tokens per Sec:    22251, Lr: 0.000300\n",
            "2020-02-11 21:52:43,056 Example #0\n",
            "2020-02-11 21:52:43,056 \tSource:     I thank him that enabled me, even Christ Jesus our Lord, for that he counted me faithful, appointing me to his service;\n",
            "2020-02-11 21:52:43,057 \tReference:  A go sɑɑbu i Kpe Yesu Mɛsiyɑ sɛ, ngɑ kɑ ɑ̀ nɑ ɑ no gɑɑbi. A gɑ kɑ ɑ̀ sɑɑbu domi ɑ̀ nɑ ɑ lɑsɑbu nɑɑnekpɛ kɑ ɑ dɑm ngɑ gbei kunɑ.\n",
            "2020-02-11 21:52:43,057 \tHypothesis: A go kɑ ɑ hɑmɑ kɑ cii: A go kɑ ɑ no gɑɑbi Mɛsiyɑ Yesu sɑbu sɛ, ɑ̀ mɑ ɑ cɛbɛ gbei yom kɑ simbɑ ndɑ ngɑ gbei kɑ ɑ̀ ci ɑ sɛ.\n",
            "2020-02-11 21:52:43,057 Example #1\n",
            "2020-02-11 21:52:43,057 \tSource:     Now lettest thou thy servant depart, Lord, According to thy word, in peace;\n",
            "2020-02-11 21:52:43,057 \tReference:  Mɑɑsɑnkulu Kpe, n mɑ tu n bɑnyɑ mɑ kpei ndɑ bɑɑni zɑngɑ n Sendɑ cii.\n",
            "2020-02-11 21:52:43,057 \tHypothesis: N di tirɑ cɛnɑ kɑ n nyɑize go bɑɑ n mɑ ɑ̀ moorɑndi n goonoyom gɑɑ.\n",
            "2020-02-11 21:52:43,057 Example #2\n",
            "2020-02-11 21:52:43,057 \tSource:     if so be that God is one, and he shall justify the circumcision by faith, and the uncircumcision through faith.\n",
            "2020-02-11 21:52:43,057 \tReference:  Ikpɛ fɔlɔnku yɑ gɑ bɑnguize yom cɛɑndi susu nɑɑne gɑɑ. À go zɑm kɑ dɑmbɑnguize yom mo cɛɑndi susu nɑɑne gɑɑ.\n",
            "2020-02-11 21:52:43,057 \tHypothesis: De Ikpɛ yɑ ci bɔrɔ kɑ ɑ̀ gɑ kɑ Ikpɛ cɛɑndi susu nɑɑne gɑɑ, ɑ̀ gɑ nɑɑne kɑ ɑ̀ cɛɑndi susu.\n",
            "2020-02-11 21:52:43,057 Example #3\n",
            "2020-02-11 21:52:43,057 \tSource:     And from that city many of the Samaritans believed on him because of the word of the woman, who testified, He told me all things that ever I did.\n",
            "2020-02-11 21:52:43,057 \tReference:  Wɑngɑrɑ ngɑ di, Sɑmɑriɑncɛ boobo nɑɑne Yesu gɑɑ weibɔrɔ di sendɑ sɑbu sɛ. Weibɔrɔ di tɛ sɛdɑ kɑ cii: Hɛ kulu kɑ ɑ jinɑ kɑ tɛ, ɑ̀ nɑ ɑ̀ cii ɑ sɛ.\n",
            "2020-02-11 21:52:43,057 \tHypothesis: Nungu kulu kɑ Sɑmɑriɑncɛ boobo nɑɑne ɑ̀ gɑɑ. Ǹ nɑɑne ɑ̀ gɑɑ. Ǹ bei kɑ ɑ̀ gɑɑ bɔrɔ kulu kɑ ɑ̀ mɑɑ kɑ ɑ̀ cii: Bɔrɔ kɑ ɑ̀ go kɑ ɑ sendɑ cii yɑ di.\n",
            "2020-02-11 21:52:43,057 Validation result (greedy) at epoch  59, step     5000: bleu:  11.61, loss: 85867.1953, ppl:  16.1706, duration: 14.2262s\n",
            "2020-02-11 21:52:47,580 Epoch  59: total training loss 136.98\n",
            "2020-02-11 21:52:47,580 EPOCH 60\n",
            "2020-02-11 21:52:54,095 Epoch  60 Step:     5100 Batch Loss:     1.476606 Tokens per Sec:    22082, Lr: 0.000300\n",
            "2020-02-11 21:52:57,069 Epoch  60: total training loss 136.55\n",
            "2020-02-11 21:52:57,069 EPOCH 61\n",
            "2020-02-11 21:53:05,021 Epoch  61 Step:     5200 Batch Loss:     1.813408 Tokens per Sec:    22658, Lr: 0.000300\n",
            "2020-02-11 21:53:06,333 Epoch  61: total training loss 133.18\n",
            "2020-02-11 21:53:06,333 EPOCH 62\n",
            "2020-02-11 21:53:15,769 Epoch  62: total training loss 132.81\n",
            "2020-02-11 21:53:15,769 EPOCH 63\n",
            "2020-02-11 21:53:16,012 Epoch  63 Step:     5300 Batch Loss:     1.689115 Tokens per Sec:    22074, Lr: 0.000300\n",
            "2020-02-11 21:53:25,039 Epoch  63: total training loss 129.83\n",
            "2020-02-11 21:53:25,039 EPOCH 64\n",
            "2020-02-11 21:53:26,905 Epoch  64 Step:     5400 Batch Loss:     1.600610 Tokens per Sec:    22143, Lr: 0.000300\n",
            "2020-02-11 21:53:34,429 Epoch  64: total training loss 127.99\n",
            "2020-02-11 21:53:34,429 EPOCH 65\n",
            "2020-02-11 21:53:37,903 Epoch  65 Step:     5500 Batch Loss:     1.375525 Tokens per Sec:    22415, Lr: 0.000300\n",
            "2020-02-11 21:53:43,771 Epoch  65: total training loss 127.34\n",
            "2020-02-11 21:53:43,772 EPOCH 66\n",
            "2020-02-11 21:53:48,775 Epoch  66 Step:     5600 Batch Loss:     1.453218 Tokens per Sec:    22614, Lr: 0.000300\n",
            "2020-02-11 21:53:53,046 Epoch  66: total training loss 124.16\n",
            "2020-02-11 21:53:53,046 EPOCH 67\n",
            "2020-02-11 21:53:59,833 Epoch  67 Step:     5700 Batch Loss:     1.423065 Tokens per Sec:    22353, Lr: 0.000300\n",
            "2020-02-11 21:54:02,380 Epoch  67: total training loss 121.15\n",
            "2020-02-11 21:54:02,381 EPOCH 68\n",
            "2020-02-11 21:54:10,690 Epoch  68 Step:     5800 Batch Loss:     1.635956 Tokens per Sec:    22396, Lr: 0.000300\n",
            "2020-02-11 21:54:11,686 Epoch  68: total training loss 121.98\n",
            "2020-02-11 21:54:11,686 EPOCH 69\n",
            "2020-02-11 21:54:20,988 Epoch  69: total training loss 119.07\n",
            "2020-02-11 21:54:20,988 EPOCH 70\n",
            "2020-02-11 21:54:21,685 Epoch  70 Step:     5900 Batch Loss:     1.408005 Tokens per Sec:    21901, Lr: 0.000300\n",
            "2020-02-11 21:54:30,332 Epoch  70: total training loss 118.99\n",
            "2020-02-11 21:54:30,332 EPOCH 71\n",
            "2020-02-11 21:54:32,576 Epoch  71 Step:     6000 Batch Loss:     1.378949 Tokens per Sec:    22081, Lr: 0.000300\n",
            "2020-02-11 21:54:46,881 Example #0\n",
            "2020-02-11 21:54:46,881 \tSource:     I thank him that enabled me, even Christ Jesus our Lord, for that he counted me faithful, appointing me to his service;\n",
            "2020-02-11 21:54:46,881 \tReference:  A go sɑɑbu i Kpe Yesu Mɛsiyɑ sɛ, ngɑ kɑ ɑ̀ nɑ ɑ no gɑɑbi. A gɑ kɑ ɑ̀ sɑɑbu domi ɑ̀ nɑ ɑ lɑsɑbu nɑɑnekpɛ kɑ ɑ dɑm ngɑ gbei kunɑ.\n",
            "2020-02-11 21:54:46,881 \tHypothesis: A go kɑ ɑ bɛɛrɑndi Mɛsiyɑ Yesu kunɑ, ngɑ kɑ ɑ̀ gɑ kɑ ɑ cɛbɛ i sɛ i yom kɑ i ci nɑɑnekpɛ yom hɛ kɑ ɑ̀ sɑbɑ ɑ sɛ.\n",
            "2020-02-11 21:54:46,881 Example #1\n",
            "2020-02-11 21:54:46,882 \tSource:     Now lettest thou thy servant depart, Lord, According to thy word, in peace;\n",
            "2020-02-11 21:54:46,882 \tReference:  Mɑɑsɑnkulu Kpe, n mɑ tu n bɑnyɑ mɑ kpei ndɑ bɑɑni zɑngɑ n Sendɑ cii.\n",
            "2020-02-11 21:54:46,882 \tHypothesis: E Kpe Ajiripɑ, n mɑ n ŋmɔne tɑm. N mɑ lɑɑkɑli ndɑ n fuu kulu.\n",
            "2020-02-11 21:54:46,882 Example #2\n",
            "2020-02-11 21:54:46,882 \tSource:     if so be that God is one, and he shall justify the circumcision by faith, and the uncircumcision through faith.\n",
            "2020-02-11 21:54:46,882 \tReference:  Ikpɛ fɔlɔnku yɑ gɑ bɑnguize yom cɛɑndi susu nɑɑne gɑɑ. À go zɑm kɑ dɑmbɑnguize yom mo cɛɑndi susu nɑɑne gɑɑ.\n",
            "2020-02-11 21:54:46,882 \tHypothesis: Zɑngɑ Ikpɛ gɑ bɔrɔ fɔ kulu kɑ ɑ̀ gɑ kɑ Ikpɛ cɛɑndi susu nɑɑne gɑɑ, ɑ̀ gɑ nɑɑne nɑɑne gɑɑ nɑɑne kɑ ɑ̀ sindɑ nɑɑne nɑɑne nɑɑne gɑɑ.\n",
            "2020-02-11 21:54:46,882 Example #3\n",
            "2020-02-11 21:54:46,882 \tSource:     And from that city many of the Samaritans believed on him because of the word of the woman, who testified, He told me all things that ever I did.\n",
            "2020-02-11 21:54:46,882 \tReference:  Wɑngɑrɑ ngɑ di, Sɑmɑriɑncɛ boobo nɑɑne Yesu gɑɑ weibɔrɔ di sendɑ sɑbu sɛ. Weibɔrɔ di tɛ sɛdɑ kɑ cii: Hɛ kulu kɑ ɑ jinɑ kɑ tɛ, ɑ̀ nɑ ɑ̀ cii ɑ sɛ.\n",
            "2020-02-11 21:54:46,882 \tHypothesis: Zɑ Sɑmɑri lɑɑbu bɔrɔ boobo nɑɑne ɑ̀ gɑɑ. À go nɑɑne ɑ̀ gɑɑ. À go kɑ ɑ̀ sendi ndɑ zɑngɑ Ikpɛ Sendi hɑntumɑntɛ cii: Bɔrɔ kɑ ɑ̀ go cii ɑ sɛ, ɑ̀ go hɛ kulu tɛ.\n",
            "2020-02-11 21:54:46,883 Validation result (greedy) at epoch  71, step     6000: bleu:  12.36, loss: 87552.1328, ppl:  17.0783, duration: 14.3064s\n",
            "2020-02-11 21:54:53,975 Epoch  71: total training loss 115.99\n",
            "2020-02-11 21:54:53,975 EPOCH 72\n",
            "2020-02-11 21:54:57,831 Epoch  72 Step:     6100 Batch Loss:     1.335934 Tokens per Sec:    22250, Lr: 0.000300\n",
            "2020-02-11 21:55:03,324 Epoch  72: total training loss 116.15\n",
            "2020-02-11 21:55:03,324 EPOCH 73\n",
            "2020-02-11 21:55:08,640 Epoch  73 Step:     6200 Batch Loss:     1.614239 Tokens per Sec:    22524, Lr: 0.000300\n",
            "2020-02-11 21:55:12,553 Epoch  73: total training loss 113.09\n",
            "2020-02-11 21:55:12,553 EPOCH 74\n",
            "2020-02-11 21:55:19,652 Epoch  74 Step:     6300 Batch Loss:     1.306311 Tokens per Sec:    22631, Lr: 0.000300\n",
            "2020-02-11 21:55:21,846 Epoch  74: total training loss 109.68\n",
            "2020-02-11 21:55:21,846 EPOCH 75\n",
            "2020-02-11 21:55:30,554 Epoch  75 Step:     6400 Batch Loss:     1.338363 Tokens per Sec:    22183, Lr: 0.000300\n",
            "2020-02-11 21:55:31,227 Epoch  75: total training loss 111.10\n",
            "2020-02-11 21:55:31,227 EPOCH 76\n",
            "2020-02-11 21:55:40,485 Epoch  76: total training loss 108.55\n",
            "2020-02-11 21:55:40,485 EPOCH 77\n",
            "2020-02-11 21:55:41,496 Epoch  77 Step:     6500 Batch Loss:     1.274989 Tokens per Sec:    23129, Lr: 0.000300\n",
            "2020-02-11 21:55:49,828 Epoch  77: total training loss 108.22\n",
            "2020-02-11 21:55:49,828 EPOCH 78\n",
            "2020-02-11 21:55:52,318 Epoch  78 Step:     6600 Batch Loss:     1.027054 Tokens per Sec:    22396, Lr: 0.000300\n",
            "2020-02-11 21:55:59,056 Epoch  78: total training loss 106.35\n",
            "2020-02-11 21:55:59,056 EPOCH 79\n",
            "2020-02-11 21:56:03,136 Epoch  79 Step:     6700 Batch Loss:     1.258515 Tokens per Sec:    22431, Lr: 0.000300\n",
            "2020-02-11 21:56:08,313 Epoch  79: total training loss 104.57\n",
            "2020-02-11 21:56:08,313 EPOCH 80\n",
            "2020-02-11 21:56:14,120 Epoch  80 Step:     6800 Batch Loss:     1.240082 Tokens per Sec:    22447, Lr: 0.000300\n",
            "2020-02-11 21:56:17,666 Epoch  80: total training loss 102.62\n",
            "2020-02-11 21:56:17,666 EPOCH 81\n",
            "2020-02-11 21:56:25,206 Epoch  81 Step:     6900 Batch Loss:     1.268439 Tokens per Sec:    22566, Lr: 0.000300\n",
            "2020-02-11 21:56:26,995 Epoch  81: total training loss 101.90\n",
            "2020-02-11 21:56:26,996 EPOCH 82\n",
            "2020-02-11 21:56:36,197 Epoch  82 Step:     7000 Batch Loss:     0.895320 Tokens per Sec:    22457, Lr: 0.000300\n",
            "2020-02-11 21:56:48,459 Example #0\n",
            "2020-02-11 21:56:48,459 \tSource:     I thank him that enabled me, even Christ Jesus our Lord, for that he counted me faithful, appointing me to his service;\n",
            "2020-02-11 21:56:48,459 \tReference:  A go sɑɑbu i Kpe Yesu Mɛsiyɑ sɛ, ngɑ kɑ ɑ̀ nɑ ɑ no gɑɑbi. A gɑ kɑ ɑ̀ sɑɑbu domi ɑ̀ nɑ ɑ lɑsɑbu nɑɑnekpɛ kɑ ɑ dɑm ngɑ gbei kunɑ.\n",
            "2020-02-11 21:56:48,459 \tHypothesis: A go kɑ ɑ bɛɛrɑndi Mɛsiyɑ Yesu kunɑ, ngɑ kɑ ɑ̀ nɑ ɑ cɛbɛ i gbei yom sɑbu sɛ. À go kɑ ɑ cɛbɛ tɑɑbi ndɑ gbei kɑ ɑ go tɛ ndɑ ɑ sɛ ɑ sɑbu sɛ.\n",
            "2020-02-11 21:56:48,459 Example #1\n",
            "2020-02-11 21:56:48,460 \tSource:     Now lettest thou thy servant depart, Lord, According to thy word, in peace;\n",
            "2020-02-11 21:56:48,460 \tReference:  Mɑɑsɑnkulu Kpe, n mɑ tu n bɑnyɑ mɑ kpei ndɑ bɑɑni zɑngɑ n Sendɑ cii.\n",
            "2020-02-11 21:56:48,460 \tHypothesis: E Kpe Ajiripɑ, n mɑ n ŋmɔne dɔntɔneize gɑnɑ kɑ tɑmtɛrɛ n lɑɑkɑli mɑ n bɔm cɛbɛ kunɑ.\n",
            "2020-02-11 21:56:48,460 Example #2\n",
            "2020-02-11 21:56:48,460 \tSource:     if so be that God is one, and he shall justify the circumcision by faith, and the uncircumcision through faith.\n",
            "2020-02-11 21:56:48,460 \tReference:  Ikpɛ fɔlɔnku yɑ gɑ bɑnguize yom cɛɑndi susu nɑɑne gɑɑ. À go zɑm kɑ dɑmbɑnguize yom mo cɛɑndi susu nɑɑne gɑɑ.\n",
            "2020-02-11 21:56:48,460 \tHypothesis: De Ikpɛ yɑ nɑ ɑ̀ tɛ nɑ hɑli bɔrɔ kɑ ɑ̀ gɑ kɑ ɑ̀ cɛɑndi susu nɑɑne gɑɑ, ɑ̀ gɑ cɛɑndiyom susu nɑɑne gɑɑ.\n",
            "2020-02-11 21:56:48,460 Example #3\n",
            "2020-02-11 21:56:48,460 \tSource:     And from that city many of the Samaritans believed on him because of the word of the woman, who testified, He told me all things that ever I did.\n",
            "2020-02-11 21:56:48,460 \tReference:  Wɑngɑrɑ ngɑ di, Sɑmɑriɑncɛ boobo nɑɑne Yesu gɑɑ weibɔrɔ di sendɑ sɑbu sɛ. Weibɔrɔ di tɛ sɛdɑ kɑ cii: Hɛ kulu kɑ ɑ jinɑ kɑ tɛ, ɑ̀ nɑ ɑ̀ cii ɑ sɛ.\n",
            "2020-02-11 21:56:48,460 \tHypothesis: Zɑ Sɑmɑri lɑɑbu bɔrɔ boobo nɑɑne ɑ̀ gɑɑ. À nɑɑne ɑ̀ gɑɑ. À go tɛ sɛdɑ kɑ cii ngɑ yɑ ci bɔrɔ kulu kɑ ɑ̀ cii ɑ sɛ ndɑ hɛ kulu kɑ ɑ̀ tɛ.\n",
            "2020-02-11 21:56:48,460 Validation result (greedy) at epoch  82, step     7000: bleu:  13.38, loss: 89329.6641, ppl:  18.0912, duration: 12.2634s\n",
            "2020-02-11 21:56:48,572 Epoch  82: total training loss 99.87\n",
            "2020-02-11 21:56:48,572 EPOCH 83\n",
            "2020-02-11 21:56:57,844 Epoch  83: total training loss 99.08\n",
            "2020-02-11 21:56:57,844 EPOCH 84\n",
            "2020-02-11 21:56:59,386 Epoch  84 Step:     7100 Batch Loss:     1.315156 Tokens per Sec:    22592, Lr: 0.000300\n",
            "2020-02-11 21:57:07,073 Epoch  84: total training loss 97.96\n",
            "2020-02-11 21:57:07,073 EPOCH 85\n",
            "2020-02-11 21:57:10,170 Epoch  85 Step:     7200 Batch Loss:     0.902618 Tokens per Sec:    21760, Lr: 0.000300\n",
            "2020-02-11 21:57:16,354 Epoch  85: total training loss 96.64\n",
            "2020-02-11 21:57:16,354 EPOCH 86\n",
            "2020-02-11 21:57:21,181 Epoch  86 Step:     7300 Batch Loss:     1.252862 Tokens per Sec:    22870, Lr: 0.000300\n",
            "2020-02-11 21:57:25,648 Epoch  86: total training loss 94.94\n",
            "2020-02-11 21:57:25,648 EPOCH 87\n",
            "2020-02-11 21:57:32,164 Epoch  87 Step:     7400 Batch Loss:     1.278766 Tokens per Sec:    21962, Lr: 0.000300\n",
            "2020-02-11 21:57:35,113 Epoch  87: total training loss 94.83\n",
            "2020-02-11 21:57:35,113 EPOCH 88\n",
            "2020-02-11 21:57:43,052 Epoch  88 Step:     7500 Batch Loss:     1.348532 Tokens per Sec:    22651, Lr: 0.000300\n",
            "2020-02-11 21:57:44,451 Epoch  88: total training loss 95.36\n",
            "2020-02-11 21:57:44,451 EPOCH 89\n",
            "2020-02-11 21:57:53,697 Epoch  89: total training loss 91.97\n",
            "2020-02-11 21:57:53,697 EPOCH 90\n",
            "2020-02-11 21:57:53,942 Epoch  90 Step:     7600 Batch Loss:     0.814619 Tokens per Sec:    20829, Lr: 0.000300\n",
            "2020-02-11 21:58:03,004 Epoch  90: total training loss 91.43\n",
            "2020-02-11 21:58:03,004 EPOCH 91\n",
            "2020-02-11 21:58:04,888 Epoch  91 Step:     7700 Batch Loss:     1.106051 Tokens per Sec:    22805, Lr: 0.000300\n",
            "2020-02-11 21:58:12,273 Epoch  91: total training loss 89.62\n",
            "2020-02-11 21:58:12,273 EPOCH 92\n",
            "2020-02-11 21:58:15,859 Epoch  92 Step:     7800 Batch Loss:     1.152600 Tokens per Sec:    22754, Lr: 0.000300\n",
            "2020-02-11 21:58:21,622 Epoch  92: total training loss 88.73\n",
            "2020-02-11 21:58:21,623 EPOCH 93\n",
            "2020-02-11 21:58:26,707 Epoch  93 Step:     7900 Batch Loss:     1.016823 Tokens per Sec:    22367, Lr: 0.000300\n",
            "2020-02-11 21:58:30,981 Epoch  93: total training loss 88.75\n",
            "2020-02-11 21:58:30,981 EPOCH 94\n",
            "2020-02-11 21:58:37,655 Epoch  94 Step:     8000 Batch Loss:     1.090580 Tokens per Sec:    22697, Lr: 0.000300\n",
            "2020-02-11 21:58:51,361 Example #0\n",
            "2020-02-11 21:58:51,361 \tSource:     I thank him that enabled me, even Christ Jesus our Lord, for that he counted me faithful, appointing me to his service;\n",
            "2020-02-11 21:58:51,361 \tReference:  A go sɑɑbu i Kpe Yesu Mɛsiyɑ sɛ, ngɑ kɑ ɑ̀ nɑ ɑ no gɑɑbi. A gɑ kɑ ɑ̀ sɑɑbu domi ɑ̀ nɑ ɑ lɑsɑbu nɑɑnekpɛ kɑ ɑ dɑm ngɑ gbei kunɑ.\n",
            "2020-02-11 21:58:51,361 \tHypothesis: A go kɑ ɑ sɛdɑ ɑ̀ ciini kɑ ɑ̀ jɛ i Kpe Yesu Mɛsiyɑ sɑbu sɛ. À nɑ ɑ cɛbɛ hinneeriyom kɑ simbɑ ndɑ ngɑ gbei kɑ ɑ nɑ ɑ cɛbɛ.\n",
            "2020-02-11 21:58:51,361 Example #1\n",
            "2020-02-11 21:58:51,361 \tSource:     Now lettest thou thy servant depart, Lord, According to thy word, in peace;\n",
            "2020-02-11 21:58:51,361 \tReference:  Mɑɑsɑnkulu Kpe, n mɑ tu n bɑnyɑ mɑ kpei ndɑ bɑɑni zɑngɑ n Sendɑ cii.\n",
            "2020-02-11 21:58:51,361 \tHypothesis: E Kpe, n mɑ tirɑ di nɑm kɑ n nyɑize mɑɑ zɑɑ. N mɑ lɑɑkɑli ndɑ n mee sendi hinnɑ.\n",
            "2020-02-11 21:58:51,361 Example #2\n",
            "2020-02-11 21:58:51,361 \tSource:     if so be that God is one, and he shall justify the circumcision by faith, and the uncircumcision through faith.\n",
            "2020-02-11 21:58:51,361 \tReference:  Ikpɛ fɔlɔnku yɑ gɑ bɑnguize yom cɛɑndi susu nɑɑne gɑɑ. À go zɑm kɑ dɑmbɑnguize yom mo cɛɑndi susu nɑɑne gɑɑ.\n",
            "2020-02-11 21:58:51,362 \tHypothesis: À go kɑ Ikpɛ tɛ zɑngɑ bɔrɔ kɑ ɑ̀ si kɑ Ikpɛ cɛɑndi susu nɑɑne gɑɑ. À gɑ ɑ̀ cɛɑndi susu. À gɑ ɑ̀ tɛ zɑm kɑ nɑɑne nɑɑne gɑɑ nɑɑne kunɑ.\n",
            "2020-02-11 21:58:51,362 Example #3\n",
            "2020-02-11 21:58:51,362 \tSource:     And from that city many of the Samaritans believed on him because of the word of the woman, who testified, He told me all things that ever I did.\n",
            "2020-02-11 21:58:51,362 \tReference:  Wɑngɑrɑ ngɑ di, Sɑmɑriɑncɛ boobo nɑɑne Yesu gɑɑ weibɔrɔ di sendɑ sɑbu sɛ. Weibɔrɔ di tɛ sɛdɑ kɑ cii: Hɛ kulu kɑ ɑ jinɑ kɑ tɛ, ɑ̀ nɑ ɑ̀ cii ɑ sɛ.\n",
            "2020-02-11 21:58:51,362 \tHypothesis: Zɑ wɑngɑrɑ bɔrɔ boobo kɑ ǹ nɑɑne ɑ̀ gɑɑ, ǹ go nɑɑne ɑ̀ gɑɑ. Ǹ go tɛ sɛdɑ kɑ cii ɑ̀ sɛ: À nɑ ɑ no hɛ kulu kɑ ɑ̀ cii kɑ ɑ sɛ.\n",
            "2020-02-11 21:58:51,362 Validation result (greedy) at epoch  94, step     8000: bleu:  14.04, loss: 91286.4453, ppl:  19.2758, duration: 13.7070s\n",
            "2020-02-11 21:58:53,974 Epoch  94: total training loss 87.15\n",
            "2020-02-11 21:58:53,974 EPOCH 95\n",
            "2020-02-11 21:59:02,333 Epoch  95 Step:     8100 Batch Loss:     1.252790 Tokens per Sec:    22471, Lr: 0.000300\n",
            "2020-02-11 21:59:03,308 Epoch  95: total training loss 86.44\n",
            "2020-02-11 21:59:03,308 EPOCH 96\n",
            "2020-02-11 21:59:12,620 Epoch  96: total training loss 86.01\n",
            "2020-02-11 21:59:12,620 EPOCH 97\n",
            "2020-02-11 21:59:13,203 Epoch  97 Step:     8200 Batch Loss:     1.033871 Tokens per Sec:    21899, Lr: 0.000300\n",
            "2020-02-11 21:59:21,972 Epoch  97: total training loss 84.14\n",
            "2020-02-11 21:59:21,972 EPOCH 98\n",
            "2020-02-11 21:59:24,158 Epoch  98 Step:     8300 Batch Loss:     1.007341 Tokens per Sec:    22056, Lr: 0.000300\n",
            "2020-02-11 21:59:31,280 Epoch  98: total training loss 82.63\n",
            "2020-02-11 21:59:31,281 EPOCH 99\n",
            "2020-02-11 21:59:35,157 Epoch  99 Step:     8400 Batch Loss:     1.005271 Tokens per Sec:    22367, Lr: 0.000300\n",
            "2020-02-11 21:59:40,572 Epoch  99: total training loss 82.09\n",
            "2020-02-11 21:59:40,572 EPOCH 100\n",
            "2020-02-11 21:59:46,019 Epoch 100 Step:     8500 Batch Loss:     0.942513 Tokens per Sec:    22545, Lr: 0.000300\n",
            "2020-02-11 21:59:49,904 Epoch 100: total training loss 82.12\n",
            "2020-02-11 21:59:49,905 Training ended after 100 epochs.\n",
            "2020-02-11 21:59:49,905 Best validation result (greedy) at step     4000:  15.99 ppl.\n",
            "2020-02-11 22:00:09,967  dev bleu:  13.32 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-02-11 22:00:09,968 Translations saved to: models/enddn_transformer/00004000.hyps.dev\n",
            "2020-02-11 22:00:13,473 test bleu:  21.19 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-02-11 22:00:13,473 Translations saved to: models/enddn_transformer/00004000.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJHT-J8tU56N",
        "colab_type": "code",
        "outputId": "34a6c2b8-52b6-4cbf-bfdf-e764a6eccccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!ls joeynmt/models/${src}${tgt}_transformer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00004000.hyps.dev   2000.hyps  4000.hyps  8000.hyps\t tensorboard\n",
            "00004000.hyps.test  3000.ckpt  5000.hyps  best.ckpt\t train.log\n",
            "1000.hyps\t    3000.hyps  6000.hyps  config.yaml\t trg_vocab.txt\n",
            "2000.ckpt\t    4000.ckpt  7000.hyps  src_vocab.txt  validations.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBoDS09JM807",
        "outputId": "ee96c2f4-cae3-4a6e-8c3e-9364b7a28fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot create symbolic link '/content/drive/My Drive/masakhane/en-ddn-baseline/models/enddn_transformer/best.ckpt': Operation not supported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNBlLPMFVKc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp joeynmt/models/${src}${tgt}_transformer/best.ckpt \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n94wlrCjVc17",
        "outputId": "62fcfd2d-1cf7-43d2-d15e-81f914b52764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 1000\tLoss: 108471.50781\tPPL: 33.64506\tbleu: 2.13345\tLR: 0.00030000\t*\n",
            "Steps: 2000\tLoss: 93253.19531\tPPL: 20.54460\tbleu: 6.44712\tLR: 0.00030000\t*\n",
            "Steps: 3000\tLoss: 86804.96875\tPPL: 16.66970\tbleu: 9.49384\tLR: 0.00030000\t*\n",
            "Steps: 4000\tLoss: 85529.50781\tPPL: 15.99461\tbleu: 11.08313\tLR: 0.00030000\t*\n",
            "Steps: 5000\tLoss: 85867.19531\tPPL: 16.17064\tbleu: 11.61374\tLR: 0.00030000\t\n",
            "Steps: 6000\tLoss: 87552.13281\tPPL: 17.07833\tbleu: 12.36364\tLR: 0.00030000\t\n",
            "Steps: 7000\tLoss: 89329.66406\tPPL: 18.09119\tbleu: 13.38334\tLR: 0.00030000\t\n",
            "Steps: 8000\tLoss: 91286.44531\tPPL: 19.27579\tbleu: 14.04216\tLR: 0.00030000\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66WhRE9lIhoD",
        "outputId": "a0c4b3e3-81aa-4559-d2bb-37ac34a1280a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-02-11 22:15:09,857 Hello! This is Joey-NMT.\n",
            "2020-02-11 22:15:32,580  dev bleu:  13.32 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "Hello\n",
            "2020-02-11 22:15:36,088 test bleu:  21.19 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}