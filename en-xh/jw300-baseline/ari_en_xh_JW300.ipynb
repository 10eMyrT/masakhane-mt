{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ari_en_xh_JW300.ipynb","provenance":[],"collapsed_sections":["smUYbE8bGNE0","xyAKRE7TJ-yt","TmPMur_UAVM3"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BdwKlib49HcY","colab_type":"text"},"source":["# <center>Masakhane - Machine Translation for African Languages (Using JoeyNMT)</center>\n","## <leftalign> Author : Ari Ramkilowan</leftalign>\n","## <leftalign> Language Pair : English - isiXhosa</leftalign>\n","## <leftalign> Corpus : JW300 </leftalign>"]},{"cell_type":"markdown","metadata":{"id":"GuU-cZx6JyWf","colab_type":"text"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"smUYbE8bGNE0","colab_type":"text"},"source":["## Install JoeyNMT"]},{"cell_type":"code","metadata":{"id":"O8NG5kJ-9SLW","colab_type":"code","outputId":"38a3d4f0-64d0-4c11-86b8-7e671863fced","executionInfo":{"status":"ok","timestamp":1573624868744,"user_tz":-120,"elapsed":52789,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["! git clone https://github.com/joeynmt/joeynmt.git\n","! cd joeynmt; pip3 install ."],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'joeynmt'...\n","remote: Enumerating objects: 2184, done.\u001b[K\n","Receiving objects:   0% (1/2184)   \rReceiving objects:   1% (22/2184)   \rReceiving objects:   2% (44/2184)   \rReceiving objects:   3% (66/2184)   \rReceiving objects:   4% (88/2184)   \rReceiving objects:   5% (110/2184)   \rReceiving objects:   6% (132/2184)   \rReceiving objects:   7% (153/2184)   \rReceiving objects:   8% (175/2184)   \rReceiving objects:   9% (197/2184)   \rReceiving objects:  10% (219/2184)   \rReceiving objects:  11% (241/2184)   \rReceiving objects:  12% (263/2184)   \rReceiving objects:  13% (284/2184)   \rReceiving objects:  14% (306/2184)   \rReceiving objects:  15% (328/2184)   \rReceiving objects:  16% (350/2184)   \rReceiving objects:  17% (372/2184)   \rReceiving objects:  18% (394/2184)   \rReceiving objects:  19% (415/2184)   \rReceiving objects:  20% (437/2184)   \rReceiving objects:  21% (459/2184)   \rReceiving objects:  22% (481/2184)   \rReceiving objects:  23% (503/2184)   \rReceiving objects:  24% (525/2184)   \rReceiving objects:  25% (546/2184)   \rReceiving objects:  26% (568/2184)   \rReceiving objects:  27% (590/2184)   \rReceiving objects:  28% (612/2184)   \rReceiving objects:  29% (634/2184)   \rReceiving objects:  30% (656/2184)   \rReceiving objects:  31% (678/2184)   \rReceiving objects:  32% (699/2184)   \rReceiving objects:  33% (721/2184)   \rReceiving objects:  34% (743/2184)   \rReceiving objects:  35% (765/2184)   \rReceiving objects:  36% (787/2184)   \rReceiving objects:  37% (809/2184)   \rReceiving objects:  38% (830/2184)   \rReceiving objects:  39% (852/2184)   \rReceiving objects:  40% (874/2184)   \rReceiving objects:  41% (896/2184)   \rReceiving objects:  42% (918/2184)   \rReceiving objects:  43% (940/2184)   \rReceiving objects:  44% (961/2184)   \rReceiving objects:  45% (983/2184)   \rReceiving objects:  46% (1005/2184)   \rReceiving objects:  47% (1027/2184)   \rReceiving objects:  48% (1049/2184)   \rReceiving objects:  49% (1071/2184)   \rReceiving objects:  50% (1092/2184)   \rReceiving objects:  51% (1114/2184)   \rReceiving objects:  52% (1136/2184)   \rReceiving objects:  53% (1158/2184)   \rReceiving objects:  54% (1180/2184)   \rReceiving objects:  55% (1202/2184)   \rReceiving objects:  56% (1224/2184)   \rReceiving objects:  57% (1245/2184)   \rReceiving objects:  58% (1267/2184)   \rReceiving objects:  59% (1289/2184)   \rReceiving objects:  60% (1311/2184)   \rReceiving objects:  61% (1333/2184)   \rReceiving objects:  62% (1355/2184)   \rReceiving objects:  63% (1376/2184)   \rReceiving objects:  64% (1398/2184)   \rReceiving objects:  65% (1420/2184)   \rReceiving objects:  66% (1442/2184)   \rReceiving objects:  67% (1464/2184)   \rReceiving objects:  68% (1486/2184)   \rReceiving objects:  69% (1507/2184)   \rReceiving objects:  70% (1529/2184)   \rReceiving objects:  71% (1551/2184)   \rReceiving objects:  72% (1573/2184)   \rReceiving objects:  73% (1595/2184)   \rReceiving objects:  74% (1617/2184)   \rReceiving objects:  75% (1638/2184)   \rReceiving objects:  76% (1660/2184)   \rReceiving objects:  77% (1682/2184)   \rremote: Total 2184 (delta 0), reused 0 (delta 0), pack-reused 2184\u001b[K\n","Receiving objects:  78% (1704/2184)   \rReceiving objects:  79% (1726/2184)   \rReceiving objects:  80% (1748/2184)   \rReceiving objects:  81% (1770/2184)   \rReceiving objects:  82% (1791/2184)   \rReceiving objects:  83% (1813/2184)   \rReceiving objects:  84% (1835/2184)   \rReceiving objects:  85% (1857/2184)   \rReceiving objects:  86% (1879/2184)   \rReceiving objects:  87% (1901/2184)   \rReceiving objects:  88% (1922/2184)   \rReceiving objects:  89% (1944/2184)   \rReceiving objects:  90% (1966/2184)   \rReceiving objects:  91% (1988/2184)   \rReceiving objects:  92% (2010/2184)   \rReceiving objects:  93% (2032/2184)   \rReceiving objects:  94% (2053/2184)   \rReceiving objects:  95% (2075/2184)   \rReceiving objects:  96% (2097/2184)   \rReceiving objects:  97% (2119/2184)   \rReceiving objects:  98% (2141/2184)   \rReceiving objects:  99% (2163/2184)   \rReceiving objects: 100% (2184/2184)   \rReceiving objects: 100% (2184/2184), 2.58 MiB | 14.62 MiB/s, done.\n","Resolving deltas:   0% (0/1521)   \rResolving deltas:   1% (21/1521)   \rResolving deltas:  10% (163/1521)   \rResolving deltas:  11% (172/1521)   \rResolving deltas:  16% (254/1521)   \rResolving deltas:  18% (278/1521)   \rResolving deltas:  20% (318/1521)   \rResolving deltas:  23% (354/1521)   \rResolving deltas:  24% (372/1521)   \rResolving deltas:  25% (381/1521)   \rResolving deltas:  27% (422/1521)   \rResolving deltas:  28% (427/1521)   \rResolving deltas:  29% (454/1521)   \rResolving deltas:  30% (457/1521)   \rResolving deltas:  31% (482/1521)   \rResolving deltas:  33% (512/1521)   \rResolving deltas:  34% (522/1521)   \rResolving deltas:  35% (537/1521)   \rResolving deltas:  36% (550/1521)   \rResolving deltas:  37% (564/1521)   \rResolving deltas:  38% (584/1521)   \rResolving deltas:  41% (624/1521)   \rResolving deltas:  42% (646/1521)   \rResolving deltas:  43% (656/1521)   \rResolving deltas:  44% (671/1521)   \rResolving deltas:  45% (686/1521)   \rResolving deltas:  46% (704/1521)   \rResolving deltas:  47% (722/1521)   \rResolving deltas:  48% (745/1521)   \rResolving deltas:  49% (757/1521)   \rResolving deltas:  50% (764/1521)   \rResolving deltas:  54% (825/1521)   \rResolving deltas:  55% (846/1521)   \rResolving deltas:  57% (870/1521)   \rResolving deltas:  58% (890/1521)   \rResolving deltas:  59% (912/1521)   \rResolving deltas:  61% (930/1521)   \rResolving deltas:  62% (945/1521)   \rResolving deltas:  65% (991/1521)   \rResolving deltas:  67% (1021/1521)   \rResolving deltas:  70% (1072/1521)   \rResolving deltas:  72% (1110/1521)   \rResolving deltas:  75% (1144/1521)   \rResolving deltas:  77% (1186/1521)   \rResolving deltas:  78% (1194/1521)   \rResolving deltas:  79% (1210/1521)   \rResolving deltas:  81% (1235/1521)   \rResolving deltas:  82% (1251/1521)   \rResolving deltas:  87% (1333/1521)   \rResolving deltas:  89% (1363/1521)   \rResolving deltas:  91% (1393/1521)   \rResolving deltas:  93% (1426/1521)   \rResolving deltas:  94% (1430/1521)   \rResolving deltas:  95% (1445/1521)   \rResolving deltas:  96% (1471/1521)   \rResolving deltas:  97% (1476/1521)   \rResolving deltas:  98% (1504/1521)   \rResolving deltas:  99% (1506/1521)   \rResolving deltas: 100% (1521/1521)   \rResolving deltas: 100% (1521/1521), done.\n","Processing /content/joeynmt\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (4.3.0)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.17.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (41.4.0)\n","Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.3.1+cu100)\n","Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.15.0)\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n","Collecting sacrebleu>=1.3.6\n","  Downloading https://files.pythonhosted.org/packages/0e/e5/93d252182f7cbd4b59bb3ec5797e2ce33cfd6f5aadaf327db170cf4b7887/sacrebleu-1.4.2-py3-none-any.whl\n","Collecting subword-nmt\n","  Downloading https://files.pythonhosted.org/packages/26/08/58267cb3ac00f5f895457777ed9e0d106dbb5e6388fa7923d8663b04b849/subword_nmt-0.3.6-py2.py3-none-any.whl\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.1.1)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.9.0)\n","Collecting pyyaml>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n","\u001b[K     |████████████████████████████████| 266kB 9.8MB/s \n","\u001b[?25hCollecting pylint\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/f1/758de486e46ea2b8717992704b0fdd968b7cbc2bc790b976fae4a35a212c/pylint-2.4.3-py3-none-any.whl (302kB)\n","\u001b[K     |████████████████████████████████| 307kB 41.6MB/s \n","\u001b[?25hRequirement already satisfied: six==1.12 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.12.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->joeynmt==0.0.1) (0.46)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.0.8)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.1.8)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.33.6)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.1)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.11.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.10.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.28.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.21.0)\n","Collecting portalocker\n","  Downloading https://files.pythonhosted.org/packages/91/db/7bc703c0760df726839e0699b7f78a4d8217fdc9c7fcb1b51b39c5a22a4e/portalocker-1.5.2-py2.py3-none-any.whl\n","Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.3.6->joeynmt==0.0.1) (3.6.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.6.1)\n","Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (0.25.3)\n","Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.3.1)\n","Collecting mccabe<0.7,>=0.6\n","  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n","Collecting astroid<2.4,>=2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/ae/86734823047962e7b8c8529186a1ac4a7ca19aaf1aa0c7713c022ef593fd/astroid-2.3.3-py3-none-any.whl (205kB)\n","\u001b[K     |████████████████████████████████| 215kB 42.9MB/s \n","\u001b[?25hCollecting isort<5,>=4.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.14->joeynmt==0.0.1) (2.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (0.16.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2019.9.11)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.8)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn->joeynmt==0.0.1) (2018.9)\n","Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/d3/9d1802c161626d0278bafb1ffb32f76b9d01e123881bbf9d91e8ccf28e18/typed_ast-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (736kB)\n","\u001b[K     |████████████████████████████████| 737kB 22.1MB/s \n","\u001b[?25hCollecting lazy-object-proxy==1.4.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: joeynmt, pyyaml\n","  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=71902 sha256=6636cb21c8d34f62fe9eb0ad8eb8a3da798ea32468ef44cc18532d16b5b8b5f2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-bqbj68_4/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n","  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44104 sha256=50024750d60f4359a8d3b1058838db0e87d4fc8eb816f89cc519e784376eda4c\n","  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n","Successfully built joeynmt pyyaml\n","Installing collected packages: portalocker, sacrebleu, subword-nmt, pyyaml, mccabe, typed-ast, lazy-object-proxy, astroid, isort, pylint, joeynmt\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed astroid-2.3.3 isort-4.3.21 joeynmt-0.0.1 lazy-object-proxy-1.4.3 mccabe-0.6.1 portalocker-1.5.2 pylint-2.4.3 pyyaml-5.1.2 sacrebleu-1.4.2 subword-nmt-0.3.6 typed-ast-1.4.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xyAKRE7TJ-yt","colab_type":"text"},"source":["## Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"44Sz_9aV9aDX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"2b825ed9-a9c9-47d1-b516-1b20577cf61c","executionInfo":{"status":"ok","timestamp":1573624852824,"user_tz":-120,"elapsed":39556,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["# If running on Google Colab - mount google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4xIVqSqWm7HT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"34b144c2-3a59-4b86-f93e-509ee79bf112","executionInfo":{"status":"ok","timestamp":1573624809450,"user_tz":-120,"elapsed":3787,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["import torch\n","\n","device_num = torch.cuda.current_device()\n","torch.cuda.get_device_name(device_num)\n","# torch.cuda.is_available()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla K80'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"YkeQhZCh_6Jn","colab_type":"text"},"source":["## Set your source and target languages"]},{"cell_type":"code","metadata":{"id":"tx8BNHwVK1jt","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","\n","source_language = \"en\"\n","target_language = \"xh\" \n","lc = True  # If True, lowercase the data.\n","seed = 42  # Random seed for shuffling.\n","tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n","vocab_size=4000\n","corpus = \"JW300\"\n","\n","os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n","os.environ[\"tgt\"] = target_language\n","os.environ[\"tag\"] = tag\n","os.environ[\"vocab_size\"] = str(vocab_size)\n","os.environ[\"corpus\"] = corpus"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lq_NRbE8LJTI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2fd3ab48-45c6-4bbc-c178-3a7dededdfc1","executionInfo":{"status":"ok","timestamp":1573624894237,"user_tz":-120,"elapsed":4377,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["# This will save it to a folder in our gdrive instead!\n","# !mkdir -p \"/content/drive/My Drive/masakhane/$src-$trg-$tag\"\n","gdrive_path = f\"/content/drive/My Drive/masakhane/{source_language}-{target_language}-{tag}/\"\n","os.environ[\"gdrive_path\"] = gdrive_path\n","! echo $gdrive_path"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/masakhane/en-xh-baseline/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T4RrqHaeLNQi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"350dffd4-b694-47f9-e920-74d7e45f381e","executionInfo":{"status":"ok","timestamp":1573624925262,"user_tz":-120,"elapsed":4059,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["# create path to joeynmt executables scripts, configs etc\n","\n","joey_path = f\"/content/joeynmt\"\n","os.environ[\"joey_path\"] = joey_path\n","! ls $joey_path/configs"],"execution_count":7,"outputs":[{"output_type":"stream","text":["iwslt14_deen_bpe.yaml\t\t   transformer_reverse.yaml\n","iwslt_deen_bahdanau.yaml\t   transformer_small.yaml\n","iwslt_envi_luong.yaml\t\t   transformer_wmt17_ende.yaml\n","iwslt_envi_xnmt.yaml\t\t   transformer_wmt17_lven.yaml\n","reverse.yaml\t\t\t   wmt_ende_best.yaml\n","small.yaml\t\t\t   wmt_ende_default.yaml\n","transformer_copy.yaml\t\t   wmt_lven_best.yaml\n","transformer_iwslt14_deen_bpe.yaml  wmt_lven_default.yaml\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TmPMur_UAVM3","colab_type":"text"},"source":["## Download the global test set.\n","\n","***(This changes from time to time, do this just to make sure you have the most recent version)***"]},{"cell_type":"code","metadata":{"id":"kN-HUp87LVGB","colab_type":"code","colab":{}},"source":["! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n","  \n","! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$tgt.en\n","! mv test.en-$tgt.en test.en\n","\n","! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$tgt.$tgt \n","! mv test.en-$tgt.$tgt test.$tgt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1VpW7P7U_tc0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ve8qqVviAVPu","colab_type":"code","outputId":"a708ac0d-3ae9-4127-a3a7-fc6b96169792","executionInfo":{"status":"ok","timestamp":1573454297173,"user_tz":-120,"elapsed":1334,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Read the test data to filter from train and dev splits.\n","# Store english portion in set for quick filtering checks.\n","en_test_sents = set()\n","filter_test_sents = \"test.en-any.en\"\n","j = 0\n","blanks = [] # sometimes blank lines creep innto test set - store which lines these are\n","with open(filter_test_sents) as f:\n","  for line in f:\n","    en_test_sents.add(line.strip())\n","    if len(line)<=1:\n","      blanks.append(j)\n","    j += 1\n","print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))\n","print(f'There are {len(blanks)} blank lines in the test set')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Loaded 3571 global test sentences to filter from the training/dev data.\n","There are 0 blank lines in the test set\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RHh4MOzGAVSJ","colab_type":"code","colab":{}},"source":["# filter test set\n","\n","source_file = f\"test.{source_language}\"\n","target_file = f\"test.{target_language}\"\n","\n","source = []\n","target = []\n","\n","with open(source_file) as f:\n","  source = f.readlines()\n","            \n","with open(target_file) as f:\n","  target = f.readlines()\n","\n","df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n","\n","# remove trailing newline chars\n","df['source_sentence'] = df['source_sentence'].str.rstrip('\" \\n')\n","df['target_sentence'] = df['target_sentence'].str.rstrip('\" \\n')\n","\n","# remove leading newline chars\n","df['source_sentence'] = df['source_sentence'].str.lstrip('\"')\n","df['target_sentence'] = df['target_sentence'].str.lstrip('\"')\n","\n","# remove rows with really short sentences\n","df = df[~(df['source_sentence'].str.len() <8)] # remove rows wher esource text len <8 characters\n","df = df[~(df['target_sentence'].str.len() <8)] # remove rows wher esource text len <8 characters\n","\n","# save the filtered test set\n","df['source_sentence'].to_csv(f'{source_file}', index=False, header=False, doublequote=False)\n","df['target_sentence'].to_csv(f'{target_file}', index=False, header=False, doublequote=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lRclKYsmAVUe","colab_type":"code","colab":{}},"source":["# copy test sets to gdrive\n","! cp test.$src \"$gdrive_path\"\n","! cp test.$tgt \"$gdrive_path\"\n","! cp test.$src-any.$src \"$gdrive_path\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJAfPZfKMGz-","colab_type":"text"},"source":["## Import prepared dataset"]},{"cell_type":"code","metadata":{"id":"Iykgv6nTAVXB","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AXmXE_yGMTTb","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"WgPhV9EkMSgf","colab_type":"code","colab":{}},"source":["# This csv has extra columns added but no preprocessing done. all preprocessing should be captured in the NMT modelling notebook\n","\n","input_file = f\"{gdrive_path}/{source_language}-{target_language}-{corpus}-new.csv\"\n","df = pd.read_csv(input_file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a-rKljkHMSpT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"931e1e86-2727-4175-c549-0cf0d85f0cad","executionInfo":{"status":"ok","timestamp":1573454807204,"user_tz":-120,"elapsed":1310,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["df.head()"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source_sentence</th>\n","      <th>target_sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>How One Marriage Was Saved</td>\n","      <td>Indlela Owasindiswa Ngayo Lo Mtshato</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>“ The application of the counsel in the book M...</td>\n","      <td>Omnye umfundi onoxabiso nowaseMzantsi Afrika w...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>“ Chapter 5 , ‘ A Wife Who Is Dearly Loved , ’...</td>\n","      <td>“ Isahluko 5 , esithi ‘ Umfazi Othandwa Kunene...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I never imagined in my wildest dreams that I c...</td>\n","      <td>Andizange ndicinge nasephupheni ukuba ndandise...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Thank you very , very much .</td>\n","      <td>Enkosi kakhulu .</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                     source_sentence                                    target_sentence\n","0                         How One Marriage Was Saved               Indlela Owasindiswa Ngayo Lo Mtshato\n","1  “ The application of the counsel in the book M...  Omnye umfundi onoxabiso nowaseMzantsi Afrika w...\n","2  “ Chapter 5 , ‘ A Wife Who Is Dearly Loved , ’...  “ Isahluko 5 , esithi ‘ Umfazi Othandwa Kunene...\n","3  I never imagined in my wildest dreams that I c...  Andizange ndicinge nasephupheni ukuba ndandise...\n","4                       Thank you very , very much .                                   Enkosi kakhulu ."]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"Xw9yvWqRMSs_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"6a152178-3d15-4720-c78f-df32e13e7ff2","executionInfo":{"status":"ok","timestamp":1573454812263,"user_tz":-120,"elapsed":1709,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["# How many samples\n","size = len(df)\n","print(f\"\\n {size} samples in original text\")\n","  "],"execution_count":17,"outputs":[{"output_type":"stream","text":["\n"," 876189 samples in original text\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gMiSnuirMSy7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zxBcbWrhMSwy","colab_type":"text"},"source":["## Preprocess input data"]},{"cell_type":"code","metadata":{"id":"96yovwFjNfLm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"b19090cf-b2f6-462f-8dd1-bc75e77ca8c7","executionInfo":{"status":"ok","timestamp":1573454824771,"user_tz":-120,"elapsed":2795,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["## Preprocessing - Step 1 : Drop NaNs\n","\n","df_pp = df.dropna()\n","df_pp.info(memory_usage='deep')\n","new_size = len(df_pp)\n","print(f\"\\n {size-new_size}({100*(size-new_size)/size :.2f} %) samples removed by dropping all NaNs\")\n","size = new_size"],"execution_count":18,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 867276 entries, 0 to 876188\n","Data columns (total 2 columns):\n","source_sentence    867276 non-null object\n","target_sentence    867276 non-null object\n","dtypes: object(2)\n","memory usage: 332.4 MB\n","\n"," 8913(1.02 %) samples removed by dropping all NaNs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cMlFDy5dNk1k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"47e765e4-e9a1-4c61-e80f-e6d42ffece92","executionInfo":{"status":"ok","timestamp":1573454836647,"user_tz":-120,"elapsed":3641,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["## Preprocessing - Step 2a : Drop all duplicates in Source (en) text\n","\n","df_pp = df_pp.drop_duplicates(subset='source_sentence')\n","df_pp.info(memory_usage='deep')\n","new_size = len(df_pp)\n","print(f\"\\n {size-new_size}({100*(size-new_size)/size :.2f} %) samples removed by dropping Source sentence duplicates\")\n","size = new_size"],"execution_count":19,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 800546 entries, 0 to 876188\n","Data columns (total 2 columns):\n","source_sentence    800546 non-null object\n","target_sentence    800546 non-null object\n","dtypes: object(2)\n","memory usage: 356.2 MB\n","\n"," 66730(7.69 %) samples removed by dropping Source sentence duplicates\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2UqpPbb-Nk6n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"d25e2407-cb65-4655-b151-022b4f4bffa4","executionInfo":{"status":"ok","timestamp":1573454974284,"user_tz":-120,"elapsed":3595,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["## Preprocessing - Step 2b : Drop all duplicates in Target (zu) text\n","\n","df_pp = df_pp.drop_duplicates(subset='target_sentence')\n","df_pp.info(memory_usage='deep')\n","new_size = len(df_pp)\n","print(f\"\\n {size-new_size}({100*(size-new_size)/size :.2f} %) samples removed by dropping Target sentence duplicates\")\n","size = new_size"],"execution_count":20,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 795646 entries, 0 to 876188\n","Data columns (total 2 columns):\n","source_sentence    795646 non-null object\n","target_sentence    795646 non-null object\n","dtypes: object(2)\n","memory usage: 385.9 MB\n","\n"," 4900(0.61 %) samples removed by dropping Target sentence duplicates\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HuQT7HJoNlHW","colab_type":"code","colab":{}},"source":["##  Preprocessing - Step 3 : Remove all numeric entries\n","\n","pattern = r\"([0-9]*\\.?[0-9]*)\"  # catch integers and decimals\n","import re\n","r = re.compile(pattern)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mkn3wJkKNlLM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"outputId":"ee2cf8e8-0997-4ac0-d84f-c19f17a48481","executionInfo":{"status":"ok","timestamp":1573455016507,"user_tz":-120,"elapsed":15641,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["%%time\n","##  Preprocessing - Step 3a : Remove all numeric entries - Source text\n","\n","df_pp['source_sentence'] = df_pp['source_sentence'].str.replace(pattern,\"\")\n","df_pp['source_sentence'] = df_pp['source_sentence'].replace(\"\",np.nan)\n","\n","df_pp = df_pp.dropna()\n","df_pp.info(memory_usage='deep')\n","new_size = len(df_pp)\n","\n","print(f\"\\n {size-new_size}({100*(size-new_size)/size :.2f} %) samples removed by dropping nummeric entries from source text\")\n","size = new_size"],"execution_count":22,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 795478 entries, 0 to 876188\n","Data columns (total 2 columns):\n","source_sentence    795478 non-null object\n","target_sentence    795478 non-null object\n","dtypes: object(2)\n","memory usage: 347.2 MB\n","\n"," 168(0.02 %) samples removed by dropping nummeric entries from source text\n","CPU times: user 14.2 s, sys: 89.4 ms, total: 14.3 s\n","Wall time: 14.3 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8M5qLc8sNlD5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"outputId":"ad04f320-1224-46af-b8ab-bf48338c3ba7","executionInfo":{"status":"ok","timestamp":1573455037112,"user_tz":-120,"elapsed":16768,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["%%time\n","##  Preprocessing - Step 3b : Remove all numeric entries - Target text\n","\n","df_pp['target_sentence'] = df_pp['target_sentence'].str.replace(r,\"\")\n","df_pp['target_sentence'] = df_pp['target_sentence'].replace(\"\",np.nan)\n","\n","df_pp = df_pp.dropna()\n","df_pp.info(memory_usage='deep')\n","new_size = len(df_pp)\n","\n","print(f\"\\n {size-new_size}({100*(size-new_size)/size :.2f} %) samples removed by dropping nummeric entries from target text\")\n","size = new_size"],"execution_count":23,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 795471 entries, 0 to 876188\n","Data columns (total 2 columns):\n","source_sentence    795471 non-null object\n","target_sentence    795471 non-null object\n","dtypes: object(2)\n","memory usage: 314.4 MB\n","\n"," 7(0.00 %) samples removed by dropping nummeric entries from target text\n","CPU times: user 14.7 s, sys: 95.4 ms, total: 14.8 s\n","Wall time: 14.8 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cboxEtSJNlBq","colab_type":"text"},"source":["#### Preprocessing - Step 4 :Get length of sentences and then drop really short sentences"]},{"cell_type":"code","metadata":{"id":"1ninZmS4Nk_D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":243},"outputId":"3006ed0f-9712-4b91-e865-59c484f8c1c3","executionInfo":{"status":"ok","timestamp":1573455059262,"user_tz":-120,"elapsed":5383,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["%%time\n","# add length columns\n","\n","\n","df_pp['source_ch_len'] = df_pp['source_sentence'].str.len()\n","df_pp['source_w_len'] = [len(text.split()) for text in df_pp['source_sentence']] \n","df_pp['target_ch_len'] = df_pp['target_sentence'].str.len()\n","df_pp['target_w_len'] = [len(text.split()) for text in df_pp['target_sentence']] \n","df_pp.info(memory_usage='deep')"],"execution_count":24,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 795471 entries, 0 to 876188\n","Data columns (total 6 columns):\n","source_sentence    795471 non-null object\n","target_sentence    795471 non-null object\n","source_ch_len      795471 non-null int64\n","source_w_len       795471 non-null int64\n","target_ch_len      795471 non-null int64\n","target_w_len       795471 non-null int64\n","dtypes: int64(4), object(2)\n","memory usage: 338.7 MB\n","CPU times: user 3.91 s, sys: 31.2 ms, total: 3.94 s\n","Wall time: 3.96 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qnBEuswUOv-r","colab_type":"code","colab":{}},"source":["# # character len distrn - source text - \n","# df_pp['source_ch_len'].value_counts().sort_index()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YPtXT_8AOwHt","colab_type":"code","colab":{}},"source":["# # character len distrn - target text\n","# df_pp['target_ch_len'].value_counts().sort_index()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pgC7fDk2OwSg","colab_type":"code","colab":{}},"source":["## how many rows with source text <=2chars and what do they look like ?"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RrdlJbiDOwQV","colab_type":"code","colab":{}},"source":["# # how many single character sentences from source ?\n","# f\"{df_pp['source_ch_len'].value_counts()[1]} single character source sentences\"\n","\n","# df_pp[df_pp['source_ch_len']<=1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMbqX15kOwOK","colab_type":"code","colab":{}},"source":["# # how many 2-character sentences from source ?\n","# f\"{df_pp['source_ch_len'].value_counts()[2]} 2-character source sentences\"\n","\n","# df_pp[df_pp['source_ch_len']==2]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"abJQERXGOwL7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":243},"outputId":"e3107382-18a7-40d1-fc59-8acb29d524f9","executionInfo":{"status":"ok","timestamp":1573455117462,"user_tz":-120,"elapsed":2740,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["##  Preprocessing - Step 4a :  drop everything where the ch_len <=2 in source text\n","\n","df_pp = df_pp[~(df_pp['source_ch_len'] <=2) ]\n","\n","df_pp.info(memory_usage='deep')\n","new_size = len(df_pp)\n","print(f\"\\n {size-new_size}({100*(size-new_size)/size :.2f} %) samples removed by dropping rows with source sentences <= 2 characters\")\n","size = new_size"],"execution_count":25,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 795184 entries, 0 to 876188\n","Data columns (total 6 columns):\n","source_sentence    795184 non-null object\n","target_sentence    795184 non-null object\n","source_ch_len      795184 non-null int64\n","source_w_len       795184 non-null int64\n","target_ch_len      795184 non-null int64\n","target_w_len       795184 non-null int64\n","dtypes: int64(4), object(2)\n","memory usage: 338.7 MB\n","\n"," 287(0.04 %) samples removed by dropping rows with source sentences <= 2 characters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ziOVG-qhO9aN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":243},"outputId":"c34d076a-3194-442b-810b-56f63dea8614","executionInfo":{"status":"ok","timestamp":1573455141351,"user_tz":-120,"elapsed":3214,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["##  Preprocessing - Step 4b :  drop everything where the ch_len <=2 in target text\n","\n","df_pp = df_pp[~(df_pp['target_ch_len'] <=2) ]\n","\n","df_pp.info(memory_usage='deep')\n","new_size = len(df_pp)\n","print(f\"\\n {size-new_size}({100*(size-new_size)/size :.2f} %) samples removed by dropping rows with target sentences <= 2 characters\")\n","size = new_size"],"execution_count":26,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 795182 entries, 0 to 876188\n","Data columns (total 6 columns):\n","source_sentence    795182 non-null object\n","target_sentence    795182 non-null object\n","source_ch_len      795182 non-null int64\n","source_w_len       795182 non-null int64\n","target_ch_len      795182 non-null int64\n","target_w_len       795182 non-null int64\n","dtypes: int64(4), object(2)\n","memory usage: 338.7 MB\n","\n"," 2(0.00 %) samples removed by dropping rows with target sentences <= 2 characters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6uAVeYx6O9kb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":278},"outputId":"bee9c55e-2249-4cec-aa8a-f9e5d4d501a7","executionInfo":{"status":"ok","timestamp":1573455160802,"user_tz":-120,"elapsed":4218,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["%%time\n","##  Preprocessing - Step 5 :  remove text from test set\n","\n","with open(f\"{gdrive_path}/test.en-any.en\") as f:\n","    rows = f.readlines()\n","test_set_en = [row.strip() for row in rows]\n","\n","\n","df_pp = df_pp[~df_pp['source_sentence'].str.strip().isin(test_set_en)]\n","\n","df_pp.info(memory_usage='deep')\n","new_size = len(df_pp)\n","print(f\"\\n {size-new_size}({100*(size-new_size)/size :.2f} %) samples removed by dropping rows from test set\")\n","size = new_size"],"execution_count":27,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 794113 entries, 0 to 876188\n","Data columns (total 6 columns):\n","source_sentence    794113 non-null object\n","target_sentence    794113 non-null object\n","source_ch_len      794113 non-null int64\n","source_w_len       794113 non-null int64\n","target_ch_len      794113 non-null int64\n","target_w_len       794113 non-null int64\n","dtypes: int64(4), object(2)\n","memory usage: 338.3 MB\n","\n"," 1069(0.13 %) samples removed by dropping rows from test set\n","CPU times: user 2 s, sys: 49.9 ms, total: 2.05 s\n","Wall time: 2.07 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qM6SeYWBO9iU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":278},"outputId":"feb02f73-0fb6-4679-8bb0-f6c715e989a0","executionInfo":{"status":"ok","timestamp":1573455175794,"user_tz":-120,"elapsed":3825,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["%%time\n","##  Preprocessing - Step 6 :  remove the extra \"\n","df_pp['source_sentence'] = df_pp['source_sentence'].map(lambda x: x.lstrip('\"').rstrip('\"'))\n","df_pp['target_sentence'] = df_pp['target_sentence'].map(lambda x: x.lstrip('\"').rstrip('\"'))\n","\n","\n","df_pp.info(memory_usage='deep')\n","new_size = len(df_pp)\n","print(f\"\\n {size-new_size}({100*(size-new_size)/size :.2f} %) samples removed by dropping rows with extra quotes\")\n","size = new_size"],"execution_count":28,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 794113 entries, 0 to 876188\n","Data columns (total 6 columns):\n","source_sentence    794113 non-null object\n","target_sentence    794113 non-null object\n","source_ch_len      794113 non-null int64\n","source_w_len       794113 non-null int64\n","target_ch_len      794113 non-null int64\n","target_w_len       794113 non-null int64\n","dtypes: int64(4), object(2)\n","memory usage: 338.3 MB\n","\n"," 0(0.00 %) samples removed by dropping rows with extra quotes\n","CPU times: user 2.09 s, sys: 5.8 ms, total: 2.1 s\n","Wall time: 2.11 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h-qKPxS1O9gA","colab_type":"text"},"source":["## create dev df "]},{"cell_type":"code","metadata":{"id":"7jJTnBETPQ0_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"efea87c6-0591-422a-a1f9-21c83beda232","executionInfo":{"status":"ok","timestamp":1573455220115,"user_tz":-120,"elapsed":1758,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["df_dev = df_pp[['source_sentence', 'target_sentence']]\n","# Shuffle the data to remove bias in dev set selection.\n","seed=42\n","df_dev = df_dev.sample(frac=1, random_state=seed).reset_index(drop=True)\n","df_dev.info()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 794113 entries, 0 to 794112\n","Data columns (total 2 columns):\n","source_sentence    794113 non-null object\n","target_sentence    794113 non-null object\n","dtypes: object(2)\n","memory usage: 12.1+ MB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"61oWGnN4PSxZ","colab_type":"text"},"source":["## Create train test dev sets"]},{"cell_type":"code","metadata":{"id":"W2yVgwDZPS5b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"92ca54d4-9f37-4479-926a-20321775adfa","executionInfo":{"status":"ok","timestamp":1573455387637,"user_tz":-120,"elapsed":82192,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["%%time\n","# This section does the split between train/dev for the parallel corpora then saves them as separate files\n","# We use 1000 dev test and the given test set.\n","\n","# Do the split between dev/train and create parallel corpora\n","num_dev_patterns = 1000\n","\n","# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n","if lc:  # Julia: making lowercasing optional\n","    df_dev[\"source_sentence\"] = df_dev[\"source_sentence\"].str.lower()\n","    df_dev[\"target_sentence\"] = df_dev[\"target_sentence\"].str.lower()\n","\n","# Julia: test sets are already generated\n","dev = df_dev.tail(num_dev_patterns) # Herman: Error in original\n","stripped = df_dev.drop(df_dev.tail(num_dev_patterns).index)\n","\n","with open(f\"{gdrive_path}/train.\"+source_language, \"w\") as src_file, open(f\"{gdrive_path}/train.\"+target_language, \"w\") as tgt_file:\n","  for index, row in stripped.iterrows():\n","    src_file.write(row[\"source_sentence\"]+\"\\n\")\n","    tgt_file.write(row[\"target_sentence\"]+\"\\n\")\n","    \n","with open(f\"{gdrive_path}/dev.\"+source_language, \"w\") as src_file, open(f\"{gdrive_path}/dev.\"+target_language, \"w\") as tgt_file:\n","  for index, row in dev.iterrows():\n","    src_file.write(row[\"source_sentence\"]+\"\\n\")\n","    tgt_file.write(row[\"target_sentence\"]+\"\\n\")"],"execution_count":30,"outputs":[{"output_type":"stream","text":["CPU times: user 1min 19s, sys: 238 ms, total: 1min 19s\n","Wall time: 1min 20s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rkh4P5tpPS8y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":593},"outputId":"0aebba41-35b6-4b5c-bbb3-1bf31f3f983b","executionInfo":{"status":"ok","timestamp":1573455835556,"user_tz":-120,"elapsed":21007,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n","! head \"$gdrive_path/train.$src\"\n","! echo \"=================================\"\n","! head \"$gdrive_path/dev.$src\"\n","! echo \"=================================\"\n","! head \"$gdrive_path/test.$src\""],"execution_count":31,"outputs":[{"output_type":"stream","text":["negative feelings may likewise be aroused when the media spotlight racial conflicts , police brutality , and protest rallies or when they portray ethnic groups in a negative light \n","after catching your breath , you move another lever back and forth , slowly inching your way across the other half \n","the fear - inspiring day of jehovah\n"," “ my dream has come true ”\n","satan uses the spirit of the world to control people , but we can break free of its influence\n","he is recorded as saying : “ i am jehovah , that is my name  ”\n","some have taken training courses that have opened up job opportunities enabling them to engage in or resume full - time service \n","what , though , does god’s word say about exercising parental authority ?\n","because god is a person , he also has a personality with likes and dislikes ​ — even feelings \n","all in the congregation cooperated , including the children \n","=================================\n","a prophecy in the bible book of revelation provides the answer \n","• for what conditions and trials is satan responsible ?\n","to provide some privacy , the christian sisters in our group stitched sheets together , and we used them to divide the barracks into two sections ​ — one for the men and the other for the women \n","some hebrew christians were evidently troubled by the attacks \n","millions are victims of crime , war , indiscriminate violence , natural disasters , or injustice at the hands of people in authority \n","accepting jehovah’s authority involves responding to counsel based on his word \n","we would bike to a town about  miles [  km ] out of berlin , where we stayed and preached for two weeks \n","when confronted with what he had done , he repented , and jehovah forgave him ​ — although he was severely disciplined with regard to problems in his household \n","herod’s subjects had to finance the huge costs of his luxurious living , his ambitious building projects , his elaborate administration , and his various grants to friends and cities \n","he reports : “ i did not hold back my heart from any sort of rejoicing  ”\n","=================================\n","Jesus said : “ You must love your neighbor as yourself . ”\n","For day and night your hand was heavy upon me . ”\n","Some of the names in this article have been changed .\n","Some names in this article have been changed .\n","This is the greatest and first commandment . ”\n","It does not belong to man who is walking even to direct his step . ”\n","“ The whole world is lying in the power of the wicked one . ”\n","Published by Jehovah’s Witnesses but now out of print .\n","\"Moreover , do not call anyone your father on earth , for one is your Father , the heavenly One .\"\n","\"I am going to make a helper for him , as a complement of him . ”\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5LmHit7TPTAx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":593},"outputId":"81e1bfe6-0945-4daa-8d5b-dd352d9e62dd","executionInfo":{"status":"ok","timestamp":1573455927531,"user_tz":-120,"elapsed":21151,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["! head \"$gdrive_path/train.$tgt\"\n","! echo \"=================================\"\n","! head \"$gdrive_path/dev.$tgt\"\n","! echo \"=================================\"\n","! head \"$gdrive_path/test.$tgt\""],"execution_count":32,"outputs":[{"output_type":"stream","text":["iimvakalelo ezingafanelekanga ngokufanayo zisenokubakho xa amajelo eendaba ebalaselisa ungquzulwano lobuhlanga , inkohlakalo yamapolisa , nokuqhankqalaza kwamaqela okanye xa kugxekwa isizwe esithile \n","emva kokunqumama kancinane uthoba izibilini , ushukumisa enye intonga ende uyisa ngemva nangaphambili , uhamba kancinane ukuwela esinye isiqingatha \n","imini eyoyikekayo kayehova\n"," “ umnqweno wam uzalisekile ”\n","usathana usebenzisa umoya wehlabathi ukulawula abantu , kodwa sinokuyiphepha impembelelo yawo\n","wathi : “ ndinguyehova  lilo elo igama lam  ”\n","bambi baye bafumana uqeqesho kwizifundo ezibavulele amathuba emisebenzi ebavumelayo ukuba bangenele okanye babuyele kwinkonzo yexesha elizeleyo \n","noko ke , ithini ibhayibhile ngokusebenzisa igunya lobuzali ?\n","ngenxa yokuba uthixo engumntu , ukwanabo nobuntu , kwaye unezinto azithandayo nangazithandiyo — nkqu neemvakalelo unazo \n","bonke ebandleni bafak ’ isandla kuquka nabantwana \n","=================================\n","isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo \n","• ziziphi iimeko neemvavanyo ezibangelwa ngusathana ?\n","ukuze bakwazi ukuzonwaya , oodade badibanisa amashiti , baza basahlula phakathi isisele sangamagumbi amabini — elinye yalelamadoda elinye ilelamabhinqa \n","kucacile ukuba ezi ntlaselo zaziwakhathaza amanye aloo makristu angamahebhere \n","izigidi zithwaxwa lulwaphulo - mthetho , imfazwe , lugonyamelo , ziintlekele zokwemvelo , okanye ukuxhatshazwa ngabantu abasemagunyeni \n","ukwamkela igunya likayehova kuquka ukwenza ngokuvisisana nesiluleko esisekelwe kwilizwi lakhe \n","sasidla ngokusebenzisa iibhayisikile ukuya kwidolophu emalunga neekhilomitha ezili -  ngaphandle kweberlin , apho sasihlala size sishumayele kangangeeveki ezimbini \n","xa wabuzwa ngoko wayekwenzile , waguquka , yaye uyehova wamxolela — nangona wohlwaywa ngokuqatha ngeengxaki ezabakho endlwini yakhe \n","abantu babethwele uxanduva lokuhlawulela ubunewunewu bukaherode , amaphulo akhe okwakha izakhiwo ezingayiwayo , urhulumente wakhe omkhulu kunye nezinwe awayenazo kubahlobo bakhe nakwizixeko ezithile \n","uthi : “ andayivimba intliziyo yam naluphi na uhlobo lwemihlali  ”\n","=================================\n","UYesu wathi : “ Uze umthande ummelwane wakho njengoko uzithanda ngako . ”\n","\"Ngokuba imini nobusuku , besinzima phezu kwam isandla sakho . ”\"\n","Wambi amagama akweli nqaku aguquliwe .\n","Wambi amagama kweli nqaku atshintshiwe .\n","Lo ngowona myalelo mkhulu nowokuqala . ”\n","Asikokomntu ohambayo ukwalathisa inyathelo lakhe . ”\n","“ Ihlabathi liphela lisemandleni ongendawo . ”\n","Ipapashwe ngamaNgqina kaYehova kodwa ngoku ayisashicilelwa .\n","\"Ngaphezu koko , ningabizi nabani na ngokuthi nguyihlo emhlabeni , kuba mnye uYihlo , Lowo wasezulwini .\"\n","Ndiza kumenzela umncedi oza kuba ngumphelelisi wakhe . ”\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VoNcSQDQPTKa","colab_type":"text"},"source":["## Preprocessing the Data into Subword BPE Tokens\n","\n","- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n","\n","- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n","\n","- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "]},{"cell_type":"code","metadata":{"id":"vG7wL8RNPTQ5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"a12528c7-2e35-4775-e5dd-1c653a2bbe8f","executionInfo":{"status":"ok","timestamp":1573456322756,"user_tz":-120,"elapsed":299774,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["%%time\n","! subword-nmt learn-joint-bpe-and-vocab --input  \"$gdrive_path\"train.$src  \"$gdrive_path\"train.$tgt -s $vocab_size -o  \"$gdrive_path\"bpe.codes.$vocab_size --write-vocabulary  \"$gdrive_path\"vocab.$src  \"$gdrive_path\"vocab.$tgt\n","\n","# Apply BPE splits to the train, development and test data.\n","! subword-nmt apply-bpe -c \"$gdrive_path\"bpe.codes.$vocab_size --vocabulary \"$gdrive_path\"vocab.$src < \"$gdrive_path\"train.$src > \"$gdrive_path\"train.bpe.$src\n","! subword-nmt apply-bpe -c \"$gdrive_path\"bpe.codes.$vocab_size --vocabulary \"$gdrive_path\"vocab.$tgt < \"$gdrive_path\"train.$tgt > \"$gdrive_path\"train.bpe.$tgt\n","\n","! subword-nmt apply-bpe -c \"$gdrive_path\"bpe.codes.$vocab_size --vocabulary \"$gdrive_path\"vocab.$src < \"$gdrive_path\"dev.$src > \"$gdrive_path\"dev.bpe.$src\n","! subword-nmt apply-bpe -c \"$gdrive_path\"bpe.codes.$vocab_size --vocabulary \"$gdrive_path\"vocab.$tgt < \"$gdrive_path\"dev.$tgt > \"$gdrive_path\"dev.bpe.$tgt\n","\n","! subword-nmt apply-bpe -c \"$gdrive_path\"bpe.codes.$vocab_size --vocabulary \"$gdrive_path\"vocab.$src < \"$gdrive_path\"test.$src > \"$gdrive_path\"test.bpe.$src\n","! subword-nmt apply-bpe -c \"$gdrive_path\"bpe.codes.$vocab_size --vocabulary \"$gdrive_path\"vocab.$tgt < \"$gdrive_path\"test.$tgt > \"$gdrive_path\"test.bpe.$tgt\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":["CPU times: user 1.12 s, sys: 186 ms, total: 1.3 s\n","Wall time: 4min 57s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CGR5FZukPTYa","colab_type":"code","colab":{}},"source":["# Create that vocab using build_vocab\n","! sudo chmod 777 joeynmt/scripts/build_vocab.py\n","! joeynmt/scripts/build_vocab.py \"$gdrive_path\"train.bpe.\"$src\" \"$gdrive_path\"train.bpe.\"$tgt\" --output_path \"$gdrive_path\"vocab.txt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G8VNWCsPPTcX","colab_type":"code","colab":{}},"source":["# Some output\n","! echo \"BPE Xhosa Sentences\"\n","! tail -n 5 \"$gdrive_path\"test.bpe.$tgt\n","\n","! echo \"Combined BPE Vocab\"\n","! tail -n 10 \"$gdrive_path\"vocab.txt  # Herman\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6o0iIaLPThr","colab_type":"text"},"source":["## Creating the JoeyNMT Config\n","\n","JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n","\n","- We used Transformer architecture \n","- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n","\n","Things worth playing with:\n","- The batch size (also recommended to change for low-resourced languages)\n","- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n","- The decoder options (beam_size, alpha)\n","- Evaluation metrics (BLEU versus Crhf4)"]},{"cell_type":"code","metadata":{"id":"ibXa29wCPToR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bc265853-a1a5-4162-8f60-578c0c54927b","executionInfo":{"status":"ok","timestamp":1573624932039,"user_tz":-120,"elapsed":1155,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["name = '%s%s%s%s' % (source_language, target_language, str(vocab_size),tag)\n","name"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'enxh4000baseline'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"zjDjZGtiPTl8","colab_type":"code","colab":{}},"source":["# Create this dir before we run for the first time so we store check points\n","# !mkdir -p \"$gdrive_path/pretrained/$src$trg$vocab_size$tag/\" # Herman"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B69oIj2LPTfv","colab_type":"code","colab":{}},"source":["# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n","# (You can of course play with all the parameters if you'd like!)\n","\n","name = '%s%s%s%s' % (source_language, target_language, str(vocab_size),tag)\n","gdrive_path = os.environ[\"gdrive_path\"]\n","\n","# Create the config\n","config = \"\"\"\n","name: \"{name}_transformer\"\n","\n","data:\n","    src: \"{source_language}\"\n","    trg: \"{target_language}\"\n","    train: \"{gdrive_path}train.bpe\"\n","    dev:   \"{gdrive_path}dev.bpe\"\n","    test:  \"{gdrive_path}test.bpe\"\n","    level: \"bpe\"\n","    lowercase: False\n","    max_sent_length: 100\n","    src_vocab: \"{gdrive_path}vocab.txt\"\n","    trg_vocab: \"{gdrive_path}vocab.txt\"\n","\n","testing:\n","    beam_size: 5\n","    alpha: 1.0\n","\n","training:\n","    load_model: \"{gdrive_path}pretrained/{name}/50000.ckpt\"\n","    random_seed: 42\n","    optimizer: \"adam\"\n","    normalization: \"tokens\"\n","    adam_betas: [0.9, 0.999] \n","    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n","    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n","    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n","    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n","    decrease_factor: 0.7\n","    loss: \"crossentropy\"\n","    learning_rate: 0.0003\n","    learning_rate_min: 0.00000001\n","    weight_decay: 0.0\n","    label_smoothing: 0.1\n","    batch_size: 4096\n","    batch_type: \"token\"\n","    eval_batch_size: 3600\n","    eval_batch_type: \"token\"\n","    batch_multiplier: 1\n","    early_stopping_metric: \"ppl\"\n","    epochs: 20                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n","    validation_freq: 1000          # TODO: Set to at least once per epoch.\n","    logging_freq: 100\n","    eval_metric: \"bleu\"\n","    model_dir: \"models/{name}_transformer\"\n","    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n","    shuffle: True\n","    use_cuda: True\n","    max_output_length: 100\n","    print_valid_sents: [0, 1, 2, 3]\n","    keep_last_ckpts: 3\n","\n","model:\n","    initializer: \"xavier\"\n","    bias_initializer: \"zeros\"\n","    init_gain: 1.0\n","    embed_initializer: \"xavier\"\n","    embed_init_gain: 1.0\n","    tied_embeddings: True\n","    tied_softmax: True\n","    encoder:\n","        type: \"transformer\"\n","        num_layers: 6\n","        num_heads: 4             # TODO: Increase to 8 for larger data.\n","        embeddings:\n","            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n","            scale: True\n","            dropout: 0.3\n","        # typically ff_size = 4 x hidden_size\n","        hidden_size: 256         # TODO: Increase to 512 for larger data.\n","        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n","        dropout: 0.4\n","    decoder:\n","        type: \"transformer\"\n","        num_layers: 6\n","        num_heads: 8              # TODO: Increase to 8 for larger data.\n","        embeddings:\n","            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n","            scale: True\n","            dropout: 0.3\n","        # typically ff_size = 4 x hidden_size\n","        hidden_size: 256         # TODO: Increase to 512 for larger data.\n","        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n","        dropout: 0.4\n","\"\"\".format(name=name,\n","           gdrive_path=os.environ[\"gdrive_path\"],\n","           source_language=source_language,\n","           target_language=target_language\n","          )\n","\n","with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n","    f.write(config)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6D3d3xbNPTVK","colab_type":"text"},"source":["## Train the Model"]},{"cell_type":"code","metadata":{"id":"hVrBHa_bPTOD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"dbe69d4f-15a8-40f6-e2e1-bec4bd36554b","executionInfo":{"status":"ok","timestamp":1573624984342,"user_tz":-120,"elapsed":15721,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["! cd joeynmt/configs; ls\n","# copy config to gdrive\n","! cp joeynmt/configs/transformer_$src$tgt$vocab_size$tag.yaml \"$gdrive_path/pretrained/$src$tgt$vocab_size$tag/\""],"execution_count":10,"outputs":[{"output_type":"stream","text":["iwslt14_deen_bpe.yaml\t\t   transformer_reverse.yaml\n","iwslt_deen_bahdanau.yaml\t   transformer_small.yaml\n","iwslt_envi_luong.yaml\t\t   transformer_wmt17_ende.yaml\n","iwslt_envi_xnmt.yaml\t\t   transformer_wmt17_lven.yaml\n","reverse.yaml\t\t\t   wmt_ende_best.yaml\n","small.yaml\t\t\t   wmt_ende_default.yaml\n","transformer_copy.yaml\t\t   wmt_lven_best.yaml\n","transformer_enxh4000baseline.yaml  wmt_lven_default.yaml\n","transformer_iwslt14_deen_bpe.yaml\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YJU6gERiPTG9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7c71d858-52e0-494a-81c8-67a15b5b2b22","executionInfo":{"status":"ok","timestamp":1573628942519,"user_tz":-120,"elapsed":896,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["%%time\n","# Train the model\n","# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n","! cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt$vocab_size$tag.yaml"],"execution_count":11,"outputs":[{"output_type":"stream","text":["2019-11-13 06:03:38,559 Hello! This is Joey-NMT.\n","2019-11-13 06:03:40,110 Total params: 12169728\n","2019-11-13 06:03:40,112 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n","2019-11-13 06:03:46,479 Loading model from /content/drive/My Drive/masakhane/en-xh-baseline/pretrained/enxh4000baseline/50000.ckpt\n","2019-11-13 06:03:48,880 cfg.name                           : enxh4000baseline_transformer\n","2019-11-13 06:03:48,880 cfg.data.src                       : en\n","2019-11-13 06:03:48,880 cfg.data.trg                       : xh\n","2019-11-13 06:03:48,880 cfg.data.train                     : /content/drive/My Drive/masakhane/en-xh-baseline/train.bpe\n","2019-11-13 06:03:48,880 cfg.data.dev                       : /content/drive/My Drive/masakhane/en-xh-baseline/dev.bpe\n","2019-11-13 06:03:48,880 cfg.data.test                      : /content/drive/My Drive/masakhane/en-xh-baseline/test.bpe\n","2019-11-13 06:03:48,881 cfg.data.level                     : bpe\n","2019-11-13 06:03:48,881 cfg.data.lowercase                 : False\n","2019-11-13 06:03:48,881 cfg.data.max_sent_length           : 100\n","2019-11-13 06:03:48,881 cfg.data.src_vocab                 : /content/drive/My Drive/masakhane/en-xh-baseline/vocab.txt\n","2019-11-13 06:03:48,881 cfg.data.trg_vocab                 : /content/drive/My Drive/masakhane/en-xh-baseline/vocab.txt\n","2019-11-13 06:03:48,881 cfg.testing.beam_size              : 5\n","2019-11-13 06:03:48,881 cfg.testing.alpha                  : 1.0\n","2019-11-13 06:03:48,881 cfg.training.load_model            : /content/drive/My Drive/masakhane/en-xh-baseline/pretrained/enxh4000baseline/50000.ckpt\n","2019-11-13 06:03:48,882 cfg.training.random_seed           : 42\n","2019-11-13 06:03:48,882 cfg.training.optimizer             : adam\n","2019-11-13 06:03:48,882 cfg.training.normalization         : tokens\n","2019-11-13 06:03:48,882 cfg.training.adam_betas            : [0.9, 0.999]\n","2019-11-13 06:03:48,882 cfg.training.scheduling            : plateau\n","2019-11-13 06:03:48,882 cfg.training.patience              : 5\n","2019-11-13 06:03:48,882 cfg.training.learning_rate_factor  : 0.5\n","2019-11-13 06:03:48,882 cfg.training.learning_rate_warmup  : 1000\n","2019-11-13 06:03:48,883 cfg.training.decrease_factor       : 0.7\n","2019-11-13 06:03:48,883 cfg.training.loss                  : crossentropy\n","2019-11-13 06:03:48,883 cfg.training.learning_rate         : 0.0003\n","2019-11-13 06:03:48,883 cfg.training.learning_rate_min     : 1e-08\n","2019-11-13 06:03:48,883 cfg.training.weight_decay          : 0.0\n","2019-11-13 06:03:48,883 cfg.training.label_smoothing       : 0.1\n","2019-11-13 06:03:48,883 cfg.training.batch_size            : 4096\n","2019-11-13 06:03:48,883 cfg.training.batch_type            : token\n","2019-11-13 06:03:48,883 cfg.training.eval_batch_size       : 3600\n","2019-11-13 06:03:48,884 cfg.training.eval_batch_type       : token\n","2019-11-13 06:03:48,884 cfg.training.batch_multiplier      : 1\n","2019-11-13 06:03:48,884 cfg.training.early_stopping_metric : ppl\n","2019-11-13 06:03:48,884 cfg.training.epochs                : 20\n","2019-11-13 06:03:48,884 cfg.training.validation_freq       : 1000\n","2019-11-13 06:03:48,884 cfg.training.logging_freq          : 100\n","2019-11-13 06:03:48,884 cfg.training.eval_metric           : bleu\n","2019-11-13 06:03:48,884 cfg.training.model_dir             : models/enxh4000baseline_transformer\n","2019-11-13 06:03:48,885 cfg.training.overwrite             : True\n","2019-11-13 06:03:48,885 cfg.training.shuffle               : True\n","2019-11-13 06:03:48,885 cfg.training.use_cuda              : True\n","2019-11-13 06:03:48,885 cfg.training.max_output_length     : 100\n","2019-11-13 06:03:48,885 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n","2019-11-13 06:03:48,885 cfg.training.keep_last_ckpts       : 3\n","2019-11-13 06:03:48,885 cfg.model.initializer              : xavier\n","2019-11-13 06:03:48,885 cfg.model.bias_initializer         : zeros\n","2019-11-13 06:03:48,885 cfg.model.init_gain                : 1.0\n","2019-11-13 06:03:48,886 cfg.model.embed_initializer        : xavier\n","2019-11-13 06:03:48,886 cfg.model.embed_init_gain          : 1.0\n","2019-11-13 06:03:48,886 cfg.model.tied_embeddings          : True\n","2019-11-13 06:03:48,886 cfg.model.tied_softmax             : True\n","2019-11-13 06:03:48,886 cfg.model.encoder.type             : transformer\n","2019-11-13 06:03:48,886 cfg.model.encoder.num_layers       : 6\n","2019-11-13 06:03:48,886 cfg.model.encoder.num_heads        : 4\n","2019-11-13 06:03:48,886 cfg.model.encoder.embeddings.embedding_dim : 256\n","2019-11-13 06:03:48,887 cfg.model.encoder.embeddings.scale : True\n","2019-11-13 06:03:48,887 cfg.model.encoder.embeddings.dropout : 0.3\n","2019-11-13 06:03:48,887 cfg.model.encoder.hidden_size      : 256\n","2019-11-13 06:03:48,887 cfg.model.encoder.ff_size          : 1024\n","2019-11-13 06:03:48,887 cfg.model.encoder.dropout          : 0.4\n","2019-11-13 06:03:48,888 cfg.model.decoder.type             : transformer\n","2019-11-13 06:03:48,888 cfg.model.decoder.num_layers       : 6\n","2019-11-13 06:03:48,888 cfg.model.decoder.num_heads        : 8\n","2019-11-13 06:03:48,888 cfg.model.decoder.embeddings.embedding_dim : 256\n","2019-11-13 06:03:48,888 cfg.model.decoder.embeddings.scale : True\n","2019-11-13 06:03:48,888 cfg.model.decoder.embeddings.dropout : 0.3\n","2019-11-13 06:03:48,888 cfg.model.decoder.hidden_size      : 256\n","2019-11-13 06:03:48,888 cfg.model.decoder.ff_size          : 1024\n","2019-11-13 06:03:48,889 cfg.model.decoder.dropout          : 0.4\n","2019-11-13 06:03:48,889 Data set sizes: \n","\ttrain 791387,\n","\tvalid 1000,\n","\ttest 2711\n","2019-11-13 06:03:48,889 First training example:\n","\t[SRC] neg@@ ative feelings may lik@@ e@@ wise be ar@@ ous@@ ed when the med@@ ia sp@@ ot@@ light r@@ ac@@ ial con@@ fl@@ ic@@ ts , pol@@ ice br@@ ut@@ ality , and prot@@ est r@@ al@@ li@@ es or when they por@@ tr@@ ay eth@@ n@@ ic grou@@ ps in a neg@@ ative light\n","\t[TRG] iim@@ vakalelo ezing@@ af@@ anel@@ ekanga ngokufanayo zis@@ enoku@@ bakho xa am@@ aj@@ elo e@@ end@@ aba eb@@ alas@@ elisa ung@@ qu@@ z@@ ulw@@ ano lobu@@ hl@@ anga , ink@@ ohl@@ akalo yam@@ ap@@ ol@@ isa , noku@@ q@@ h@@ ank@@ q@@ al@@ aza kwam@@ aq@@ ela okanye xa ku@@ g@@ x@@ ekwa is@@ izwe es@@ ithile\n","2019-11-13 06:03:48,889 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) the (6) to (7) of (8) a (9) and\n","2019-11-13 06:03:48,889 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) the (6) to (7) of (8) a (9) and\n","2019-11-13 06:03:48,891 Number of Src words (types): 4334\n","2019-11-13 06:03:48,891 Number of Trg words (types): 4334\n","2019-11-13 06:03:48,891 Model(\n","\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n","\tdecoder=TransformerDecoder(num_layers=6, num_heads=8),\n","\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4334),\n","\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4334))\n","2019-11-13 06:03:48,897 EPOCH 1\n","2019-11-13 06:04:26,023 Epoch   1 Step:    50100 Batch Loss:     2.006075 Tokens per Sec:     6687, Lr: 0.000300\n","2019-11-13 06:05:01,379 Epoch   1 Step:    50200 Batch Loss:     1.880419 Tokens per Sec:     7052, Lr: 0.000300\n","2019-11-13 06:05:36,623 Epoch   1 Step:    50300 Batch Loss:     1.853437 Tokens per Sec:     6992, Lr: 0.000300\n","2019-11-13 06:06:11,793 Epoch   1 Step:    50400 Batch Loss:     1.922919 Tokens per Sec:     7046, Lr: 0.000300\n","2019-11-13 06:06:46,832 Epoch   1 Step:    50500 Batch Loss:     1.845251 Tokens per Sec:     7146, Lr: 0.000300\n","2019-11-13 06:07:21,688 Epoch   1 Step:    50600 Batch Loss:     1.718637 Tokens per Sec:     7087, Lr: 0.000300\n","2019-11-13 06:07:56,583 Epoch   1 Step:    50700 Batch Loss:     1.875595 Tokens per Sec:     7036, Lr: 0.000300\n","2019-11-13 06:08:31,746 Epoch   1 Step:    50800 Batch Loss:     1.845588 Tokens per Sec:     7137, Lr: 0.000300\n","2019-11-13 06:09:07,039 Epoch   1 Step:    50900 Batch Loss:     2.105852 Tokens per Sec:     7182, Lr: 0.000300\n","2019-11-13 06:09:41,897 Epoch   1 Step:    51000 Batch Loss:     2.063391 Tokens per Sec:     6869, Lr: 0.000300\n","2019-11-13 06:11:32,405 Hooray! New best validation result [ppl]!\n","2019-11-13 06:11:32,405 Saving new checkpoint.\n","2019-11-13 06:11:32,670 Example #0\n","2019-11-13 06:11:32,672 \tSource:     a prophecy in the bible book of revelation provides the answer\n","2019-11-13 06:11:32,672 \tReference:  isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:11:32,673 \tHypothesis: isiprofeto sebhayibhile kwincwadi yesityhilelo sichaza impendulo\n","2019-11-13 06:11:32,673 Example #1\n","2019-11-13 06:11:32,674 \tSource:     • for what conditions and trials is satan responsible ?\n","2019-11-13 06:11:32,674 \tReference:  • ziziphi iimeko neemvavanyo ezibangelwa ngusathana ?\n","2019-11-13 06:11:32,674 \tHypothesis: • ziziphi iimeko nezilingo usathana azinakuphendula ngazo ?\n","2019-11-13 06:11:32,674 Example #2\n","2019-11-13 06:11:32,674 \tSource:     to provide some privacy , the christian sisters in our group stitched sheets together , and we used them to divide the barracks into two sections ​ — one for the men and the other for the women\n","2019-11-13 06:11:32,675 \tReference:  ukuze bakwazi ukuzonwaya , oodade badibanisa amashiti , baza basahlula phakathi isisele sangamagumbi amabini — elinye yalelamadoda elinye ilelamabhinqa\n","2019-11-13 06:11:32,675 \tHypothesis: ukuze sinyamekele abanye oodade abangamakristu , oodade abangamakristu babethengisa izithulu , yaye sasidla ngokubeka izihlangu ezimbini — enye yezo ziqhushumbisi — omnye wabafazi nabanye abafazi\n","2019-11-13 06:11:32,675 Example #3\n","2019-11-13 06:11:32,675 \tSource:     some hebrew christians were evidently troubled by the attacks\n","2019-11-13 06:11:32,675 \tReference:  kucacile ukuba ezi ntlaselo zaziwakhathaza amanye aloo makristu angamahebhere\n","2019-11-13 06:11:32,675 \tHypothesis: kuyabonakala ukuba amanye amakristu angamahebhere ayesoyika ukuhlaselwa ziintlaselo\n","2019-11-13 06:11:32,676 Validation result (greedy) at epoch   1, step    51000: bleu:  19.03, loss: 44208.5703, ppl:   5.3112, duration: 110.7779s\n","2019-11-13 06:12:07,871 Epoch   1 Step:    51100 Batch Loss:     2.112274 Tokens per Sec:     7040, Lr: 0.000300\n","2019-11-13 06:12:42,816 Epoch   1 Step:    51200 Batch Loss:     1.989592 Tokens per Sec:     7138, Lr: 0.000300\n","2019-11-13 06:13:17,923 Epoch   1 Step:    51300 Batch Loss:     2.232303 Tokens per Sec:     7049, Lr: 0.000300\n","2019-11-13 06:13:53,069 Epoch   1 Step:    51400 Batch Loss:     1.949181 Tokens per Sec:     7067, Lr: 0.000300\n","2019-11-13 06:14:28,061 Epoch   1 Step:    51500 Batch Loss:     1.793106 Tokens per Sec:     7014, Lr: 0.000300\n","2019-11-13 06:15:02,808 Epoch   1 Step:    51600 Batch Loss:     2.515713 Tokens per Sec:     7073, Lr: 0.000300\n","2019-11-13 06:15:37,952 Epoch   1 Step:    51700 Batch Loss:     1.906359 Tokens per Sec:     7118, Lr: 0.000300\n","2019-11-13 06:16:12,642 Epoch   1 Step:    51800 Batch Loss:     2.227512 Tokens per Sec:     7037, Lr: 0.000300\n","2019-11-13 06:16:47,742 Epoch   1 Step:    51900 Batch Loss:     2.129593 Tokens per Sec:     7126, Lr: 0.000300\n","2019-11-13 06:17:22,718 Epoch   1 Step:    52000 Batch Loss:     1.787629 Tokens per Sec:     7023, Lr: 0.000300\n","2019-11-13 06:19:12,707 Hooray! New best validation result [ppl]!\n","2019-11-13 06:19:12,707 Saving new checkpoint.\n","2019-11-13 06:19:12,987 Example #0\n","2019-11-13 06:19:12,987 \tSource:     a prophecy in the bible book of revelation provides the answer\n","2019-11-13 06:19:12,988 \tReference:  isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:19:12,988 \tHypothesis: isiprofeto kwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:19:12,988 Example #1\n","2019-11-13 06:19:12,988 \tSource:     • for what conditions and trials is satan responsible ?\n","2019-11-13 06:19:12,988 \tReference:  • ziziphi iimeko neemvavanyo ezibangelwa ngusathana ?\n","2019-11-13 06:19:12,989 \tHypothesis: • ziziphi iimeko neemvavanyo usathana azimisele ukuzibophelela ?\n","2019-11-13 06:19:12,989 Example #2\n","2019-11-13 06:19:12,989 \tSource:     to provide some privacy , the christian sisters in our group stitched sheets together , and we used them to divide the barracks into two sections ​ — one for the men and the other for the women\n","2019-11-13 06:19:12,989 \tReference:  ukuze bakwazi ukuzonwaya , oodade badibanisa amashiti , baza basahlula phakathi isisele sangamagumbi amabini — elinye yalelamadoda elinye ilelamabhinqa\n","2019-11-13 06:19:12,989 \tHypothesis: ukuze silungise abanye oodade abangamakristu kwiqela lethu lahlanganisana , yaye sasisebenzisa ukuze bahlule izikhundla ezibini — enye yezo zamadoda namanye amabhinqa\n","2019-11-13 06:19:12,990 Example #3\n","2019-11-13 06:19:12,990 \tSource:     some hebrew christians were evidently troubled by the attacks\n","2019-11-13 06:19:12,990 \tReference:  kucacile ukuba ezi ntlaselo zaziwakhathaza amanye aloo makristu angamahebhere\n","2019-11-13 06:19:12,990 \tHypothesis: kuyabonakala ukuba amanye amakristu angamahebhere ayevuswa ziintlaselo\n","2019-11-13 06:19:12,990 Validation result (greedy) at epoch   1, step    52000: bleu:  19.11, loss: 44154.8672, ppl:   5.3005, duration: 110.2717s\n","2019-11-13 06:19:47,830 Epoch   1 Step:    52100 Batch Loss:     1.902059 Tokens per Sec:     7128, Lr: 0.000300\n","2019-11-13 06:20:22,819 Epoch   1 Step:    52200 Batch Loss:     1.795813 Tokens per Sec:     7115, Lr: 0.000300\n","2019-11-13 06:20:57,627 Epoch   1 Step:    52300 Batch Loss:     1.830140 Tokens per Sec:     6976, Lr: 0.000300\n","2019-11-13 06:21:32,640 Epoch   1 Step:    52400 Batch Loss:     2.007540 Tokens per Sec:     7085, Lr: 0.000300\n","2019-11-13 06:22:07,994 Epoch   1 Step:    52500 Batch Loss:     1.785106 Tokens per Sec:     7238, Lr: 0.000300\n","2019-11-13 06:22:42,665 Epoch   1 Step:    52600 Batch Loss:     2.050060 Tokens per Sec:     7114, Lr: 0.000300\n","2019-11-13 06:23:17,348 Epoch   1 Step:    52700 Batch Loss:     1.747816 Tokens per Sec:     7157, Lr: 0.000300\n","2019-11-13 06:23:52,293 Epoch   1 Step:    52800 Batch Loss:     2.092386 Tokens per Sec:     7025, Lr: 0.000300\n","2019-11-13 06:24:27,059 Epoch   1 Step:    52900 Batch Loss:     1.644768 Tokens per Sec:     6941, Lr: 0.000300\n","2019-11-13 06:25:02,245 Epoch   1 Step:    53000 Batch Loss:     1.808438 Tokens per Sec:     7140, Lr: 0.000300\n","2019-11-13 06:26:52,487 Hooray! New best validation result [ppl]!\n","2019-11-13 06:26:52,488 Saving new checkpoint.\n","2019-11-13 06:26:52,745 Example #0\n","2019-11-13 06:26:52,746 \tSource:     a prophecy in the bible book of revelation provides the answer\n","2019-11-13 06:26:52,746 \tReference:  isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:26:52,746 \tHypothesis: isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:26:52,746 Example #1\n","2019-11-13 06:26:52,747 \tSource:     • for what conditions and trials is satan responsible ?\n","2019-11-13 06:26:52,747 \tReference:  • ziziphi iimeko neemvavanyo ezibangelwa ngusathana ?\n","2019-11-13 06:26:52,747 \tHypothesis: • ziziphi iimeko nezilingo usathana azimisele ukuzibophelela ?\n","2019-11-13 06:26:52,747 Example #2\n","2019-11-13 06:26:52,747 \tSource:     to provide some privacy , the christian sisters in our group stitched sheets together , and we used them to divide the barracks into two sections ​ — one for the men and the other for the women\n","2019-11-13 06:26:52,748 \tReference:  ukuze bakwazi ukuzonwaya , oodade badibanisa amashiti , baza basahlula phakathi isisele sangamagumbi amabini — elinye yalelamadoda elinye ilelamabhinqa\n","2019-11-13 06:26:52,748 \tHypothesis: ukuze silungise abanye oodade abangamakristu kwiqela lethu lamakristu , saza sazisebenzisa ukuze bahlule izikhundla ezibini — omnye umntu ngamnye kwabathile namanye amabhinqa\n","2019-11-13 06:26:52,748 Example #3\n","2019-11-13 06:26:52,749 \tSource:     some hebrew christians were evidently troubled by the attacks\n","2019-11-13 06:26:52,749 \tReference:  kucacile ukuba ezi ntlaselo zaziwakhathaza amanye aloo makristu angamahebhere\n","2019-11-13 06:26:52,749 \tHypothesis: kuyabonakala ukuba amanye amakristu angamahebhere ayesoyika intlaselo\n","2019-11-13 06:26:52,749 Validation result (greedy) at epoch   1, step    53000: bleu:  19.25, loss: 43961.0430, ppl:   5.2618, duration: 110.5035s\n","2019-11-13 06:27:27,228 Epoch   1 Step:    53100 Batch Loss:     1.817497 Tokens per Sec:     6822, Lr: 0.000300\n","2019-11-13 06:28:02,229 Epoch   1 Step:    53200 Batch Loss:     2.218625 Tokens per Sec:     6999, Lr: 0.000300\n","2019-11-13 06:28:37,298 Epoch   1 Step:    53300 Batch Loss:     1.962717 Tokens per Sec:     7001, Lr: 0.000300\n","2019-11-13 06:29:12,130 Epoch   1 Step:    53400 Batch Loss:     1.696723 Tokens per Sec:     7049, Lr: 0.000300\n","2019-11-13 06:29:47,389 Epoch   1 Step:    53500 Batch Loss:     2.412253 Tokens per Sec:     6993, Lr: 0.000300\n","2019-11-13 06:30:22,637 Epoch   1 Step:    53600 Batch Loss:     1.949203 Tokens per Sec:     7148, Lr: 0.000300\n","2019-11-13 06:30:57,839 Epoch   1 Step:    53700 Batch Loss:     2.102186 Tokens per Sec:     7100, Lr: 0.000300\n","2019-11-13 06:31:32,757 Epoch   1 Step:    53800 Batch Loss:     1.851004 Tokens per Sec:     7052, Lr: 0.000300\n","2019-11-13 06:32:07,925 Epoch   1 Step:    53900 Batch Loss:     1.541280 Tokens per Sec:     7087, Lr: 0.000300\n","2019-11-13 06:32:43,236 Epoch   1 Step:    54000 Batch Loss:     1.915078 Tokens per Sec:     7158, Lr: 0.000300\n","2019-11-13 06:34:33,249 Hooray! New best validation result [ppl]!\n","2019-11-13 06:34:33,249 Saving new checkpoint.\n","2019-11-13 06:34:33,541 Example #0\n","2019-11-13 06:34:33,542 \tSource:     a prophecy in the bible book of revelation provides the answer\n","2019-11-13 06:34:33,542 \tReference:  isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:34:33,542 \tHypothesis: isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:34:33,542 Example #1\n","2019-11-13 06:34:33,542 \tSource:     • for what conditions and trials is satan responsible ?\n","2019-11-13 06:34:33,542 \tReference:  • ziziphi iimeko neemvavanyo ezibangelwa ngusathana ?\n","2019-11-13 06:34:33,543 \tHypothesis: • ziziphi iimeko neemvavanyo usathana azimisele ziphi iimeko ?\n","2019-11-13 06:34:33,543 Example #2\n","2019-11-13 06:34:33,543 \tSource:     to provide some privacy , the christian sisters in our group stitched sheets together , and we used them to divide the barracks into two sections ​ — one for the men and the other for the women\n","2019-11-13 06:34:33,543 \tReference:  ukuze bakwazi ukuzonwaya , oodade badibanisa amashiti , baza basahlula phakathi isisele sangamagumbi amabini — elinye yalelamadoda elinye ilelamabhinqa\n","2019-11-13 06:34:33,543 \tHypothesis: ukuze silungiselele abanye oodade abangamakristu kwiqela lethu lakhuphisana , yaye sasisebenzisa ukuba bahlule izihlwele ezitshatileyo — enye yezo zihlandlo ezibini — omnye umntu namanye amabhinqa\n","2019-11-13 06:34:33,543 Example #3\n","2019-11-13 06:34:33,543 \tSource:     some hebrew christians were evidently troubled by the attacks\n","2019-11-13 06:34:33,543 \tReference:  kucacile ukuba ezi ntlaselo zaziwakhathaza amanye aloo makristu angamahebhere\n","2019-11-13 06:34:33,543 \tHypothesis: kuyabonakala ukuba amanye amakristu angamahebhere ayesoloko esoyika\n","2019-11-13 06:34:33,544 Validation result (greedy) at epoch   1, step    54000: bleu:  19.42, loss: 43777.0781, ppl:   5.2254, duration: 110.3070s\n","2019-11-13 06:35:08,423 Epoch   1 Step:    54100 Batch Loss:     1.552066 Tokens per Sec:     6997, Lr: 0.000300\n","2019-11-13 06:35:43,107 Epoch   1 Step:    54200 Batch Loss:     1.709921 Tokens per Sec:     7023, Lr: 0.000300\n","2019-11-13 06:36:18,337 Epoch   1 Step:    54300 Batch Loss:     2.201611 Tokens per Sec:     7140, Lr: 0.000300\n","2019-11-13 06:36:53,309 Epoch   1 Step:    54400 Batch Loss:     2.115207 Tokens per Sec:     7074, Lr: 0.000300\n","2019-11-13 06:37:28,347 Epoch   1 Step:    54500 Batch Loss:     2.126402 Tokens per Sec:     7078, Lr: 0.000300\n","2019-11-13 06:38:03,227 Epoch   1 Step:    54600 Batch Loss:     2.059912 Tokens per Sec:     6997, Lr: 0.000300\n","2019-11-13 06:38:38,320 Epoch   1 Step:    54700 Batch Loss:     2.039664 Tokens per Sec:     7156, Lr: 0.000300\n","2019-11-13 06:39:13,281 Epoch   1 Step:    54800 Batch Loss:     1.798143 Tokens per Sec:     7117, Lr: 0.000300\n","2019-11-13 06:39:47,834 Epoch   1 Step:    54900 Batch Loss:     2.002563 Tokens per Sec:     7039, Lr: 0.000300\n","2019-11-13 06:40:23,118 Epoch   1 Step:    55000 Batch Loss:     1.797622 Tokens per Sec:     7008, Lr: 0.000300\n","2019-11-13 06:42:13,258 Hooray! New best validation result [ppl]!\n","2019-11-13 06:42:13,258 Saving new checkpoint.\n","2019-11-13 06:42:13,549 Example #0\n","2019-11-13 06:42:13,550 \tSource:     a prophecy in the bible book of revelation provides the answer\n","2019-11-13 06:42:13,550 \tReference:  isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:42:13,550 \tHypothesis: isiprofeto kwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:42:13,550 Example #1\n","2019-11-13 06:42:13,551 \tSource:     • for what conditions and trials is satan responsible ?\n","2019-11-13 06:42:13,551 \tReference:  • ziziphi iimeko neemvavanyo ezibangelwa ngusathana ?\n","2019-11-13 06:42:13,551 \tHypothesis: • ziziphi iimeko neemvavanyo usathana azimisele ukuzenza ?\n","2019-11-13 06:42:13,551 Example #2\n","2019-11-13 06:42:13,551 \tSource:     to provide some privacy , the christian sisters in our group stitched sheets together , and we used them to divide the barracks into two sections ​ — one for the men and the other for the women\n","2019-11-13 06:42:13,552 \tReference:  ukuze bakwazi ukuzonwaya , oodade badibanisa amashiti , baza basahlula phakathi isisele sangamagumbi amabini — elinye yalelamadoda elinye ilelamabhinqa\n","2019-11-13 06:42:13,552 \tHypothesis: ukuze silungiselele abanye oodade abangamakristu kwiqela lethu lahlanganisana , yaye sasidla ngokubenza bahlule izikhululo ezibini — enye yezo zinto — amadoda namanye amabhinqa\n","2019-11-13 06:42:13,552 Example #3\n","2019-11-13 06:42:13,552 \tSource:     some hebrew christians were evidently troubled by the attacks\n","2019-11-13 06:42:13,552 \tReference:  kucacile ukuba ezi ntlaselo zaziwakhathaza amanye aloo makristu angamahebhere\n","2019-11-13 06:42:13,553 \tHypothesis: ngokucacileyo amanye amakristu angamahebhere ayewaphathwa ziintlaselo\n","2019-11-13 06:42:13,553 Validation result (greedy) at epoch   1, step    55000: bleu:  19.15, loss: 43506.0703, ppl:   5.1722, duration: 110.4343s\n","2019-11-13 06:42:48,778 Epoch   1 Step:    55100 Batch Loss:     2.002864 Tokens per Sec:     7057, Lr: 0.000300\n","2019-11-13 06:43:23,544 Epoch   1 Step:    55200 Batch Loss:     1.690183 Tokens per Sec:     7051, Lr: 0.000300\n","2019-11-13 06:43:58,476 Epoch   1 Step:    55300 Batch Loss:     1.839829 Tokens per Sec:     7035, Lr: 0.000300\n","2019-11-13 06:44:32,968 Epoch   1 Step:    55400 Batch Loss:     1.926507 Tokens per Sec:     6858, Lr: 0.000300\n","2019-11-13 06:45:07,868 Epoch   1 Step:    55500 Batch Loss:     1.930189 Tokens per Sec:     7002, Lr: 0.000300\n","2019-11-13 06:45:42,765 Epoch   1 Step:    55600 Batch Loss:     2.235298 Tokens per Sec:     7063, Lr: 0.000300\n","2019-11-13 06:46:18,005 Epoch   1 Step:    55700 Batch Loss:     2.227378 Tokens per Sec:     7039, Lr: 0.000300\n","2019-11-13 06:46:52,774 Epoch   1 Step:    55800 Batch Loss:     2.083739 Tokens per Sec:     6938, Lr: 0.000300\n","2019-11-13 06:47:27,644 Epoch   1 Step:    55900 Batch Loss:     1.746540 Tokens per Sec:     7027, Lr: 0.000300\n","2019-11-13 06:48:02,683 Epoch   1 Step:    56000 Batch Loss:     2.130831 Tokens per Sec:     7096, Lr: 0.000300\n","2019-11-13 06:49:52,614 Hooray! New best validation result [ppl]!\n","2019-11-13 06:49:52,614 Saving new checkpoint.\n","2019-11-13 06:49:52,894 Example #0\n","2019-11-13 06:49:52,894 \tSource:     a prophecy in the bible book of revelation provides the answer\n","2019-11-13 06:49:52,894 \tReference:  isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:49:52,894 \tHypothesis: isiprofeto sebhayibhile sencwadi yesityhilelo sinikela impendulo\n","2019-11-13 06:49:52,894 Example #1\n","2019-11-13 06:49:52,895 \tSource:     • for what conditions and trials is satan responsible ?\n","2019-11-13 06:49:52,895 \tReference:  • ziziphi iimeko neemvavanyo ezibangelwa ngusathana ?\n","2019-11-13 06:49:52,895 \tHypothesis: • ziziphi iimeko nezilingo usathana azimisele ziphi iimeko ?\n","2019-11-13 06:49:52,895 Example #2\n","2019-11-13 06:49:52,895 \tSource:     to provide some privacy , the christian sisters in our group stitched sheets together , and we used them to divide the barracks into two sections ​ — one for the men and the other for the women\n","2019-11-13 06:49:52,895 \tReference:  ukuze bakwazi ukuzonwaya , oodade badibanisa amashiti , baza basahlula phakathi isisele sangamagumbi amabini — elinye yalelamadoda elinye ilelamabhinqa\n","2019-11-13 06:49:52,896 \tHypothesis: ukuze anikele abanye oodade abangamakristu kwiqela lethu lamakristu lahlanganisana , yaye sasebenzisa amaqhinga okuhlula izihlwele ezimbini — omnye umntu omnye kwabafazi\n","2019-11-13 06:49:52,896 Example #3\n","2019-11-13 06:49:52,896 \tSource:     some hebrew christians were evidently troubled by the attacks\n","2019-11-13 06:49:52,896 \tReference:  kucacile ukuba ezi ntlaselo zaziwakhathaza amanye aloo makristu angamahebhere\n","2019-11-13 06:49:52,896 \tHypothesis: kuyabonakala ukuba amanye amakristu angamahebhere ayesoloko esoyika\n","2019-11-13 06:49:52,897 Validation result (greedy) at epoch   1, step    56000: bleu:  19.37, loss: 43403.7227, ppl:   5.1522, duration: 110.2134s\n","2019-11-13 06:50:28,226 Epoch   1 Step:    56100 Batch Loss:     2.310874 Tokens per Sec:     7123, Lr: 0.000300\n","2019-11-13 06:51:03,217 Epoch   1 Step:    56200 Batch Loss:     1.809236 Tokens per Sec:     7051, Lr: 0.000300\n","2019-11-13 06:51:38,204 Epoch   1 Step:    56300 Batch Loss:     2.027521 Tokens per Sec:     7161, Lr: 0.000300\n","2019-11-13 06:52:13,007 Epoch   1 Step:    56400 Batch Loss:     1.924958 Tokens per Sec:     7028, Lr: 0.000300\n","2019-11-13 06:52:48,196 Epoch   1 Step:    56500 Batch Loss:     1.720386 Tokens per Sec:     7058, Lr: 0.000300\n","2019-11-13 06:53:23,090 Epoch   1 Step:    56600 Batch Loss:     1.644543 Tokens per Sec:     6999, Lr: 0.000300\n","2019-11-13 06:53:58,007 Epoch   1 Step:    56700 Batch Loss:     2.011129 Tokens per Sec:     7121, Lr: 0.000300\n","2019-11-13 06:54:32,913 Epoch   1 Step:    56800 Batch Loss:     1.902578 Tokens per Sec:     7133, Lr: 0.000300\n","2019-11-13 06:55:08,014 Epoch   1 Step:    56900 Batch Loss:     1.877176 Tokens per Sec:     7096, Lr: 0.000300\n","2019-11-13 06:55:43,285 Epoch   1 Step:    57000 Batch Loss:     2.014625 Tokens per Sec:     7134, Lr: 0.000300\n","2019-11-13 06:57:33,787 Hooray! New best validation result [ppl]!\n","2019-11-13 06:57:33,787 Saving new checkpoint.\n","2019-11-13 06:57:34,070 Example #0\n","2019-11-13 06:57:34,070 \tSource:     a prophecy in the bible book of revelation provides the answer\n","2019-11-13 06:57:34,071 \tReference:  isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:57:34,071 \tHypothesis: isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 06:57:34,071 Example #1\n","2019-11-13 06:57:34,071 \tSource:     • for what conditions and trials is satan responsible ?\n","2019-11-13 06:57:34,071 \tReference:  • ziziphi iimeko neemvavanyo ezibangelwa ngusathana ?\n","2019-11-13 06:57:34,071 \tHypothesis: • ziziphi iimeko neemvavanyo usathana azimiselayo ?\n","2019-11-13 06:57:34,071 Example #2\n","2019-11-13 06:57:34,071 \tSource:     to provide some privacy , the christian sisters in our group stitched sheets together , and we used them to divide the barracks into two sections ​ — one for the men and the other for the women\n","2019-11-13 06:57:34,072 \tReference:  ukuze bakwazi ukuzonwaya , oodade badibanisa amashiti , baza basahlula phakathi isisele sangamagumbi amabini — elinye yalelamadoda elinye ilelamabhinqa\n","2019-11-13 06:57:34,072 \tHypothesis: ukuze banikele abanye oodade abangamakristu kwiqela lethu lahlanganisana , yaye sasisebenzisa ukuba bahlule izibuko ezimbini — enye yazo amadoda namanye amabhinqa\n","2019-11-13 06:57:34,072 Example #3\n","2019-11-13 06:57:34,072 \tSource:     some hebrew christians were evidently troubled by the attacks\n","2019-11-13 06:57:34,072 \tReference:  kucacile ukuba ezi ntlaselo zaziwakhathaza amanye aloo makristu angamahebhere\n","2019-11-13 06:57:34,072 \tHypothesis: kuyabonakala ukuba amanye amakristu angamahebhere ayesoyika ukuhlaselwa\n","2019-11-13 06:57:34,072 Validation result (greedy) at epoch   1, step    57000: bleu:  19.74, loss: 43166.0820, ppl:   5.1062, duration: 110.7870s\n","2019-11-13 06:58:09,102 Epoch   1 Step:    57100 Batch Loss:     2.146807 Tokens per Sec:     7143, Lr: 0.000300\n","2019-11-13 06:58:44,163 Epoch   1 Step:    57200 Batch Loss:     1.849630 Tokens per Sec:     6994, Lr: 0.000300\n","2019-11-13 06:59:19,318 Epoch   1 Step:    57300 Batch Loss:     1.972921 Tokens per Sec:     7110, Lr: 0.000300\n","2019-11-13 06:59:54,193 Epoch   1 Step:    57400 Batch Loss:     1.940165 Tokens per Sec:     7090, Lr: 0.000300\n","2019-11-13 07:00:29,012 Epoch   1 Step:    57500 Batch Loss:     2.099602 Tokens per Sec:     6947, Lr: 0.000300\n","2019-11-13 07:01:03,769 Epoch   1 Step:    57600 Batch Loss:     1.670428 Tokens per Sec:     6976, Lr: 0.000300\n","2019-11-13 07:01:38,385 Epoch   1 Step:    57700 Batch Loss:     1.985309 Tokens per Sec:     7071, Lr: 0.000300\n","2019-11-13 07:02:13,284 Epoch   1 Step:    57800 Batch Loss:     1.643944 Tokens per Sec:     7070, Lr: 0.000300\n","2019-11-13 07:02:48,300 Epoch   1 Step:    57900 Batch Loss:     2.038939 Tokens per Sec:     7136, Lr: 0.000300\n","2019-11-13 07:03:23,622 Epoch   1 Step:    58000 Batch Loss:     1.815755 Tokens per Sec:     7012, Lr: 0.000300\n","2019-11-13 07:05:14,070 Example #0\n","2019-11-13 07:05:14,070 \tSource:     a prophecy in the bible book of revelation provides the answer\n","2019-11-13 07:05:14,070 \tReference:  isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 07:05:14,070 \tHypothesis: isiprofeto esikwincwadi yebhayibhile yesityhilelo sinikela impendulo\n","2019-11-13 07:05:14,070 Example #1\n","2019-11-13 07:05:14,071 \tSource:     • for what conditions and trials is satan responsible ?\n","2019-11-13 07:05:14,071 \tReference:  • ziziphi iimeko neemvavanyo ezibangelwa ngusathana ?\n","2019-11-13 07:05:14,071 \tHypothesis: • ziziphi iimeko neemvavanyo usathana azimisele ukuzenza ?\n","2019-11-13 07:05:14,071 Example #2\n","2019-11-13 07:05:14,071 \tSource:     to provide some privacy , the christian sisters in our group stitched sheets together , and we used them to divide the barracks into two sections ​ — one for the men and the other for the women\n","2019-11-13 07:05:14,071 \tReference:  ukuze bakwazi ukuzonwaya , oodade badibanisa amashiti , baza basahlula phakathi isisele sangamagumbi amabini — elinye yalelamadoda elinye ilelamabhinqa\n","2019-11-13 07:05:14,072 \tHypothesis: ukuze silungiselele abanye oodade abangamakristu kwiqela lethu lidlelane , yaye sasisebenzisa ukuba bahlule izihlwele ezahlukeneyo — enye yazo amadoda namanye amabhinqa\n","2019-11-13 07:05:14,072 Example #3\n","2019-11-13 07:05:14,072 \tSource:     some hebrew christians were evidently troubled by the attacks\n","2019-11-13 07:05:14,072 \tReference:  kucacile ukuba ezi ntlaselo zaziwakhathaza amanye aloo makristu angamahebhere\n","2019-11-13 07:05:14,072 \tHypothesis: kuyabonakala ukuba amanye amakristu angamahebhere ayesoloko esoyika\n","2019-11-13 07:05:14,072 Validation result (greedy) at epoch   1, step    58000: bleu:  19.38, loss: 43249.8359, ppl:   5.1223, duration: 110.4495s\n","2019-11-13 07:05:49,246 Epoch   1 Step:    58100 Batch Loss:     2.040620 Tokens per Sec:     7154, Lr: 0.000300\n","2019-11-13 07:06:23,866 Epoch   1 Step:    58200 Batch Loss:     2.021636 Tokens per Sec:     7061, Lr: 0.000300\n","2019-11-13 07:06:58,265 Epoch   1 Step:    58300 Batch Loss:     2.120660 Tokens per Sec:     7020, Lr: 0.000300\n","2019-11-13 07:07:25,446 Epoch   1: total training loss 16265.04\n","2019-11-13 07:07:25,447 EPOCH 2\n","2019-11-13 07:07:34,473 Epoch   2 Step:    58400 Batch Loss:     2.090833 Tokens per Sec:     6087, Lr: 0.000300\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/content/joeynmt/joeynmt/__main__.py\", line 41, in <module>\n","    main()\n","  File \"/content/joeynmt/joeynmt/__main__.py\", line 29, in main\n","    train(cfg_file=args.config_path)\n","  File \"/content/joeynmt/joeynmt/training.py\", line 569, in train\n","    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n","  File \"/content/joeynmt/joeynmt/training.py\", line 269, in train_and_validate\n","    batch_loss = self._train_batch(batch, update=update)\n","  File \"/content/joeynmt/joeynmt/training.py\", line 424, in _train_batch\n","    norm_batch_multiply.backward()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 166, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 99, in backward\n","    allow_unreachable=True)  # allow_unreachable flag\n","KeyboardInterrupt\n","CPU times: user 17.6 s, sys: 2.22 s, total: 19.8 s\n","Wall time: 1h 5min 4s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8ThMN3ATPTEt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"289c3455-d65e-4e7a-8608-a8bac21576c0","executionInfo":{"status":"ok","timestamp":1573628942525,"user_tz":-120,"elapsed":82,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["# Copy the created models from the notebook storage to google drive for persistant storage \n","!cp  -r joeynmt/models/${src}${tgt}${vocab_size}${tag}_transformer/* \"$gdrive_path\"\"pretrained/$src$tgt$vocab_size$tag/\"\n","!cp  joeynmt/models/${src}${tgt}${vocab_size}${tag}_transformer/best.ckpt \"$gdrive_path\"\"pretrained/$src$tgt$vocab_size$tag\""],"execution_count":12,"outputs":[{"output_type":"stream","text":["cp: cannot create symbolic link '/content/drive/My Drive/masakhane/en-xh-baseline/pretrained/enxh4000baseline/best.ckpt': Function not implemented\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aSJUF7weWPv5","colab_type":"code","colab":{}},"source":["# copy across the config file\n","!cp  joeynmt/configs/transformer_${src}${tgt}${vocab_size}${tag}.yaml \"$gdrive_path\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HsEWaXbDWP2I","colab_type":"code","colab":{}},"source":["# Test our model\n","# ! cd joeynmt; python3 -m joeynmt test \"$gdrive_path\"\"pretrained/$src$tgt$vocab_size$tag/config.yaml\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlREWwrxWP0G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"f5930fab-6d22-4d91-efd3-3bf61274dbe5","executionInfo":{"status":"ok","timestamp":1573629187109,"user_tz":-120,"elapsed":244607,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["# OR\n","! cd joeynmt; python3 -m joeynmt test \"$gdrive_path\"\"transformer_${src}${tgt}${vocab_size}${tag}.yaml\""],"execution_count":14,"outputs":[{"output_type":"stream","text":["2019-11-13 07:09:00,315 Hello! This is Joey-NMT.\n","2019-11-13 07:10:25,741  dev bleu:  19.92 [Beam search decoding with beam size = 5 and alpha = 1.0]\n","2019-11-13 07:12:59,403 test bleu:   6.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LO_hXIdAWPtU","colab_type":"code","colab":{}},"source":["# Translate mode is mopre interactive but almsot the same as running in test mode\n","! cd joeynmt; python3 -m joeynmt translate \"$gdrive_path\"\"transformer_${src}${tgt}${vocab_size}${tag}.yaml\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6HTKX2UWPrB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"2cb0cf3a-8d21-427c-c663-dd70803347c6","executionInfo":{"status":"ok","timestamp":1573629202356,"user_tz":-120,"elapsed":15265,"user":{"displayName":"Ari Ramkilowan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOdxjV-rMNcCCRFbbimFCdX4yGotTEbejYVcEc=s64","userId":"04960745627161950732"}}},"source":["# Output our validation accuracy\n","! cat \"$gdrive_path/pretrained/${src}${tgt}${vocab_size}${tag}/validations.txt\""],"execution_count":15,"outputs":[{"output_type":"stream","text":["Steps: 51000\tLoss: 44208.57031\tPPL: 5.31123\tbleu: 19.02948\tLR: 0.00030000\t*\n","Steps: 52000\tLoss: 44154.86719\tPPL: 5.30047\tbleu: 19.11237\tLR: 0.00030000\t*\n","Steps: 53000\tLoss: 43961.04297\tPPL: 5.26180\tbleu: 19.24662\tLR: 0.00030000\t*\n","Steps: 54000\tLoss: 43777.07812\tPPL: 5.22537\tbleu: 19.41868\tLR: 0.00030000\t*\n","Steps: 55000\tLoss: 43506.07031\tPPL: 5.17215\tbleu: 19.15417\tLR: 0.00030000\t*\n","Steps: 56000\tLoss: 43403.72266\tPPL: 5.15219\tbleu: 19.36647\tLR: 0.00030000\t*\n","Steps: 57000\tLoss: 43166.08203\tPPL: 5.10616\tbleu: 19.73976\tLR: 0.00030000\t*\n","Steps: 58000\tLoss: 43249.83594\tPPL: 5.12233\tbleu: 19.38474\tLR: 0.00030000\t\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jjwfXFAvWPpI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uXHLz9JWPma","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"43pEw3cKWPh3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}