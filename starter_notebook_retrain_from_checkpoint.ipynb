{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igc5itf-xMGj"
   },
   "source": [
    "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4fXCKCf36IK"
   },
   "source": [
    "## Note before beginning:\n",
    "### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
    "\n",
    "### - The tl;dr: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
    "\n",
    "### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
    "\n",
    "### - With 100 epochs, it should take around 7 hours to run in Google Colab\n",
    "\n",
    "### - Once you've gotten a result for your language, please attach and email your notebook that generated it to masakhanetranslation@gmail.com\n",
    "\n",
    "### - If you care enough and get a chance, doing a brief background on your language would be amazing. See examples in  [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l929HimrxS0a"
   },
   "source": [
    "## Continue training from a checkpoint\n",
    "\n",
    "This notebook assumed that you have already started training your model and you saving your resources on google drive for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "oGRmDELn7Az0",
    "outputId": "a4af4467-07d6-4244-d3f4-638eee94c241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cn3tgQLzUxwn"
   },
   "outputs": [],
   "source": [
    "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
    "# These will also become the suffix's of all vocab and corpus files used throughout\n",
    "import os\n",
    "source_language = \"en\" \n",
    "target_language = \"yo\" \n",
    "lc = False  # If True, lowercase the data.\n",
    "seed = 42  # Random seed for shuffling.\n",
    "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
    "\n",
    "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
    "os.environ[\"tgt\"] = target_language\n",
    "os.environ[\"tag\"] = tag\n",
    "\n",
    "# This will save it to a folder in our gdrive instead!\n",
    "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
    "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBSgJHEw7Nvx"
   },
   "outputs": [],
   "source": [
    "!echo $gdrive_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epeCydmCyS8X"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Installation of JoeyNMT\n",
    "\n",
    "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iBRMm4kMxZ8L",
    "outputId": "2362c63e-5ccf-4f27-caac-5eb033269cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'joeynmt'...\n",
      "remote: Enumerating objects: 58, done.\u001b[K\n",
      "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
      "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
      "remote: Total 2242 (delta 33), reused 37 (delta 21), pack-reused 2184\u001b[K\n",
      "Receiving objects: 100% (2242/2242), 2.62 MiB | 7.78 MiB/s, done.\n",
      "Resolving deltas: 100% (1554/1554), done.\n",
      "Processing /content/joeynmt\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (6.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.17.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (42.0.2)\n",
      "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.15.0)\n",
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n",
      "Collecting sacrebleu>=1.3.6\n",
      "  Downloading https://files.pythonhosted.org/packages/45/31/1a135b964c169984b27fb2f7a50280fa7f8e6d9d404d8a9e596180487fd1/sacrebleu-1.4.3-py3-none-any.whl\n",
      "Collecting subword-nmt\n",
      "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.1.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.9.0)\n",
      "Collecting pyyaml>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d9/ea9816aea31beeadccd03f1f8b625ecf8f645bd66744484d162d84803ce5/PyYAML-5.3.tar.gz (268kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 11.5MB/s \n",
      "\u001b[?25hCollecting pylint\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/59/43fc36c5ee316bb9aeb7cf5329cdbdca89e5749c34d5602753827c0aa2dc/pylint-2.4.4-py3-none-any.whl (302kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 43.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: six==1.12 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.0.8)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.9.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.33.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.1.8)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.28.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.21.0)\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/91/db/7bc703c0760df726839e0699b7f78a4d8217fdc9c7fcb1b51b39c5a22a4e/portalocker-1.5.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.3.6->joeynmt==0.0.1) (3.6.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.6.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (0.25.3)\n",
      "Collecting isort<5,>=4.2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 9.9MB/s \n",
      "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
      "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
      "Collecting astroid<2.4,>=2.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/ae/86734823047962e7b8c8529186a1ac4a7ca19aaf1aa0c7713c022ef593fd/astroid-2.3.3-py3-none-any.whl (205kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 40.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.14->joeynmt==0.0.1) (2.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (0.16.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2019.11.28)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn->joeynmt==0.0.1) (2018.9)\n",
      "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
      "\u001b[K     |████████████████████████████████| 747kB 41.8MB/s \n",
      "\u001b[?25hCollecting lazy-object-proxy==1.4.*\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 10.9MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: joeynmt, pyyaml\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=72136 sha256=403d0c80f80d2756fe9802733466000beb044859d287c8247189f892db6d2a5a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7jhcrnwj/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyyaml: filename=PyYAML-5.3-cp36-cp36m-linux_x86_64.whl size=44229 sha256=3d0e72a8200b2464edac60d0f227daed2ad38165546f483d3ae9ebc92e8214fb\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/76/4d/a95b8dd7b452b69e8ed4f68b69e1b55e12c9c9624dd962b191\n",
      "Successfully built joeynmt pyyaml\n",
      "Installing collected packages: portalocker, sacrebleu, subword-nmt, pyyaml, isort, mccabe, typed-ast, lazy-object-proxy, astroid, pylint, joeynmt\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed astroid-2.3.3 isort-4.3.21 joeynmt-0.0.1 lazy-object-proxy-1.4.3 mccabe-0.6.1 portalocker-1.5.2 pylint-2.4.4 pyyaml-5.3 sacrebleu-1.4.3 subword-nmt-0.3.7 typed-ast-1.4.1\n"
     ]
    }
   ],
   "source": [
    "# Install JoeyNMT\n",
    "! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ixmzi60WsUZ8"
   },
   "source": [
    "# Creating the JoeyNMT Config\n",
    "\n",
    "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
    "\n",
    "- We used Transformer architecture \n",
    "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
    "\n",
    "Things worth playing with:\n",
    "- The batch size (also recommended to change for low-resourced languages)\n",
    "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
    "- The decoder options (beam_size, alpha)\n",
    "- Evaluation metrics (BLEU versus Crhf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnlfd_JSwIBg"
   },
   "outputs": [],
   "source": [
    "model_temp_dir = \"/content/drive/My Drive/masakhane/%s-%s-%s/model-temp\" % (source_language, target_language, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_Vhps0yzyrA"
   },
   "outputs": [],
   "source": [
    "# Copy the created models from the temporary storage to main storage on google drive for persistant storage \n",
    "# the content of te folder will be overwrite when you start trainin\n",
    "!cp -r ${model_temp_dir}/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIs1lY2hxMsl"
   },
   "outputs": [],
   "source": [
    "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
    "# (You can of course play with all the parameters if you'd like!)\n",
    "\n",
    "name = '%s%s' % (source_language, target_language)\n",
    "gdrive_path = os.environ[\"gdrive_path\"]\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{name}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language}\"\n",
    "    train: \"{gdrive_path}/train.bpe\"\n",
    "    dev:   \"{gdrive_path}/dev.bpe\"\n",
    "    test:  \"{gdrive_path}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"{gdrive_path}/vocab.txt\"\n",
    "    trg_vocab: \"{gdrive_path}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    load_model: \"{gdrive_path}/models/{name}_transformer/best.ckpt\" # TODO: change 'best.ckpt' to your choice to load a pre-trained model from the checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 5                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"{model_temp_dir}\"\n",
    "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language, model_temp_dir=model_temp_dir)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIifxE3Qzuvs"
   },
   "source": [
    "# Train the Model\n",
    "\n",
    "This single line of joeynmt runs the training using the config we made above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6ZBPFwT94WpI",
    "outputId": "07c036ef-9fb7-4da9-c58e-24a49908acd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-25 17:38:35,829 Hello! This is Joey-NMT.\n",
      "2020-01-25 17:38:37,938 Total params: 12188160\n",
      "2020-01-25 17:38:37,939 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
      "2020-01-25 17:38:42,135 cfg.name                           : enyo_transformer\n",
      "2020-01-25 17:38:42,136 cfg.data.src                       : en\n",
      "2020-01-25 17:38:42,136 cfg.data.trg                       : yo\n",
      "2020-01-25 17:38:42,136 cfg.data.train                     : /content/drive/My Drive/masakhane/en-yo-baseline/train.bpe\n",
      "2020-01-25 17:38:42,136 cfg.data.dev                       : /content/drive/My Drive/masakhane/en-yo-baseline/dev.bpe\n",
      "2020-01-25 17:38:42,136 cfg.data.test                      : /content/drive/My Drive/masakhane/en-yo-baseline/test.bpe\n",
      "2020-01-25 17:38:42,137 cfg.data.level                     : bpe\n",
      "2020-01-25 17:38:42,137 cfg.data.lowercase                 : False\n",
      "2020-01-25 17:38:42,137 cfg.data.max_sent_length           : 100\n",
      "2020-01-25 17:38:42,137 cfg.data.src_vocab                 : /content/drive/My Drive/masakhane/en-yo-baseline/vocab.txt\n",
      "2020-01-25 17:38:42,137 cfg.data.trg_vocab                 : /content/drive/My Drive/masakhane/en-yo-baseline/vocab.txt\n",
      "2020-01-25 17:38:42,137 cfg.testing.beam_size              : 5\n",
      "2020-01-25 17:38:42,138 cfg.testing.alpha                  : 1.0\n",
      "2020-01-25 17:38:42,138 cfg.training.random_seed           : 42\n",
      "2020-01-25 17:38:42,138 cfg.training.optimizer             : adam\n",
      "2020-01-25 17:38:42,138 cfg.training.normalization         : tokens\n",
      "2020-01-25 17:38:42,138 cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2020-01-25 17:38:42,138 cfg.training.scheduling            : plateau\n",
      "2020-01-25 17:38:42,138 cfg.training.patience              : 5\n",
      "2020-01-25 17:38:42,139 cfg.training.learning_rate_factor  : 0.5\n",
      "2020-01-25 17:38:42,139 cfg.training.learning_rate_warmup  : 1000\n",
      "2020-01-25 17:38:42,139 cfg.training.decrease_factor       : 0.7\n",
      "2020-01-25 17:38:42,139 cfg.training.loss                  : crossentropy\n",
      "2020-01-25 17:38:42,139 cfg.training.learning_rate         : 0.0003\n",
      "2020-01-25 17:38:42,139 cfg.training.learning_rate_min     : 1e-08\n",
      "2020-01-25 17:38:42,140 cfg.training.weight_decay          : 0.0\n",
      "2020-01-25 17:38:42,140 cfg.training.label_smoothing       : 0.1\n",
      "2020-01-25 17:38:42,140 cfg.training.batch_size            : 4096\n",
      "2020-01-25 17:38:42,140 cfg.training.batch_type            : token\n",
      "2020-01-25 17:38:42,140 cfg.training.eval_batch_size       : 3600\n",
      "2020-01-25 17:38:42,140 cfg.training.eval_batch_type       : token\n",
      "2020-01-25 17:38:42,140 cfg.training.batch_multiplier      : 1\n",
      "2020-01-25 17:38:42,141 cfg.training.early_stopping_metric : ppl\n",
      "2020-01-25 17:38:42,141 cfg.training.epochs                : 5\n",
      "2020-01-25 17:38:42,141 cfg.training.validation_freq       : 1000\n",
      "2020-01-25 17:38:42,141 cfg.training.logging_freq          : 100\n",
      "2020-01-25 17:38:42,141 cfg.training.eval_metric           : bleu\n",
      "2020-01-25 17:38:42,141 cfg.training.model_dir             : /content/drive/My Drive/masakhane/en-yo-baseline/model-temp\n",
      "2020-01-25 17:38:42,141 cfg.training.overwrite             : True\n",
      "2020-01-25 17:38:42,142 cfg.training.shuffle               : True\n",
      "2020-01-25 17:38:42,142 cfg.training.use_cuda              : True\n",
      "2020-01-25 17:38:42,142 cfg.training.max_output_length     : 100\n",
      "2020-01-25 17:38:42,142 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2020-01-25 17:38:42,142 cfg.training.keep_last_ckpts       : 3\n",
      "2020-01-25 17:38:42,142 cfg.model.initializer              : xavier\n",
      "2020-01-25 17:38:42,142 cfg.model.bias_initializer         : zeros\n",
      "2020-01-25 17:38:42,143 cfg.model.init_gain                : 1.0\n",
      "2020-01-25 17:38:42,143 cfg.model.embed_initializer        : xavier\n",
      "2020-01-25 17:38:42,143 cfg.model.embed_init_gain          : 1.0\n",
      "2020-01-25 17:38:42,143 cfg.model.tied_embeddings          : True\n",
      "2020-01-25 17:38:42,143 cfg.model.tied_softmax             : True\n",
      "2020-01-25 17:38:42,143 cfg.model.encoder.type             : transformer\n",
      "2020-01-25 17:38:42,143 cfg.model.encoder.num_layers       : 6\n",
      "2020-01-25 17:38:42,144 cfg.model.encoder.num_heads        : 4\n",
      "2020-01-25 17:38:42,144 cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2020-01-25 17:38:42,144 cfg.model.encoder.embeddings.scale : True\n",
      "2020-01-25 17:38:42,144 cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2020-01-25 17:38:42,144 cfg.model.encoder.hidden_size      : 256\n",
      "2020-01-25 17:38:42,144 cfg.model.encoder.ff_size          : 1024\n",
      "2020-01-25 17:38:42,144 cfg.model.encoder.dropout          : 0.3\n",
      "2020-01-25 17:38:42,145 cfg.model.decoder.type             : transformer\n",
      "2020-01-25 17:38:42,145 cfg.model.decoder.num_layers       : 6\n",
      "2020-01-25 17:38:42,145 cfg.model.decoder.num_heads        : 4\n",
      "2020-01-25 17:38:42,145 cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2020-01-25 17:38:42,145 cfg.model.decoder.embeddings.scale : True\n",
      "2020-01-25 17:38:42,145 cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2020-01-25 17:38:42,146 cfg.model.decoder.hidden_size      : 256\n",
      "2020-01-25 17:38:42,146 cfg.model.decoder.ff_size          : 1024\n",
      "2020-01-25 17:38:42,146 cfg.model.decoder.dropout          : 0.3\n",
      "2020-01-25 17:38:42,146 Data set sizes: \n",
      "\ttrain 415100,\n",
      "\tvalid 1000,\n",
      "\ttest 0\n",
      "2020-01-25 17:38:42,146 First training example:\n",
      "\t[SRC] T@@ R@@ A@@ IN Y@@ O@@ U@@ R C@@ H@@ I@@ L@@ D@@ R@@ E@@ N : “ I teach my children to ch@@ ec@@ k the exp@@ ir@@ ation d@@ ate of any p@@ ack@@ aged food it@@ em@@ s , such as s@@ n@@ ac@@ ks , before they bu@@ y them . ” ​ — Ru@@ th , N@@ ig@@ er@@ ia\n",
      "\t[TRG] K@@ Ọ́ ÀWỌN Ọ@@ M@@ Ọ R@@ Ẹ : “ Mo kọ́ àwọn ọmọ mi pé kí wọ́n tó ra oúnjẹ bí ìp@@ á@@ p@@ án@@ u , tó wà nínú ag@@ ol@@ o , i@@ ke , bé@@ bà , tàbí ọ̀r@@ á , kí wọ́n máa yẹ ara oúnjẹ náà wò kí wọ́n lè mọ dé@@ è@@ tì tó máa bà jẹ́ . ” — Ru@@ th , N@@ àì@@ jí@@ ríà\n",
      "2020-01-25 17:38:42,146 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) tó (8) a (9) to\n",
      "2020-01-25 17:38:42,147 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) tó (8) a (9) to\n",
      "2020-01-25 17:38:42,147 Number of Src words (types): 4406\n",
      "2020-01-25 17:38:42,148 Number of Trg words (types): 4406\n",
      "2020-01-25 17:38:42,148 Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4406),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4406))\n",
      "2020-01-25 17:38:42,158 EPOCH 1\n",
      "2020-01-25 17:38:57,499 Epoch   1 Step:      100 Batch Loss:     5.651584 Tokens per Sec:    15096, Lr: 0.000300\n",
      "2020-01-25 17:39:12,142 Epoch   1 Step:      200 Batch Loss:     5.154332 Tokens per Sec:    15550, Lr: 0.000300\n",
      "2020-01-25 17:39:26,902 Epoch   1 Step:      300 Batch Loss:     5.083930 Tokens per Sec:    15659, Lr: 0.000300\n",
      "2020-01-25 17:39:41,908 Epoch   1 Step:      400 Batch Loss:     4.944828 Tokens per Sec:    15655, Lr: 0.000300\n",
      "2020-01-25 17:39:56,915 Epoch   1 Step:      500 Batch Loss:     4.723352 Tokens per Sec:    15503, Lr: 0.000300\n",
      "2020-01-25 17:40:12,050 Epoch   1 Step:      600 Batch Loss:     4.390188 Tokens per Sec:    15343, Lr: 0.000300\n",
      "2020-01-25 17:40:27,209 Epoch   1 Step:      700 Batch Loss:     4.194923 Tokens per Sec:    15289, Lr: 0.000300\n",
      "2020-01-25 17:40:42,707 Epoch   1 Step:      800 Batch Loss:     4.009085 Tokens per Sec:    14961, Lr: 0.000300\n",
      "2020-01-25 17:40:58,161 Epoch   1 Step:      900 Batch Loss:     3.899387 Tokens per Sec:    14786, Lr: 0.000300\n",
      "2020-01-25 17:41:13,609 Epoch   1 Step:     1000 Batch Loss:     3.835323 Tokens per Sec:    14973, Lr: 0.000300\n",
      "2020-01-25 17:42:01,255 Hooray! New best validation result [ppl]!\n",
      "2020-01-25 17:42:01,255 Saving new checkpoint.\n",
      "2020-01-25 17:42:02,341 Example #0\n",
      "2020-01-25 17:42:02,341 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
      "2020-01-25 17:42:02,342 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
      "2020-01-25 17:42:02,342 \tHypothesis: Àwọn tó ń jẹ́ pé àwọn èèyàn ni wọ́n ń ṣe àwọn èèyàn tó ń ṣe àwọn èèyàn .\n",
      "2020-01-25 17:42:02,342 Example #1\n",
      "2020-01-25 17:42:02,342 \tSource:     Now I had to find a legitimate line of work .\n",
      "2020-01-25 17:42:02,342 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
      "2020-01-25 17:42:02,343 \tHypothesis: Àmọ́ , ó sì jẹ́ pé àwọn èèyàn ni wọ́n ń ṣe .\n",
      "2020-01-25 17:42:02,343 Example #2\n",
      "2020-01-25 17:42:02,343 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
      "2020-01-25 17:42:02,343 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
      "2020-01-25 17:42:02,343 \tHypothesis: Kí ni Jésù ṣe ń ṣe láti ṣe láti ṣe ohun tó wà nínú àwọn tó ń ṣe ?\n",
      "2020-01-25 17:42:02,343 Example #3\n",
      "2020-01-25 17:42:02,343 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
      "2020-01-25 17:42:02,343 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
      "2020-01-25 17:42:02,344 \tHypothesis: Àmọ́ , ó sì máa ń ṣe láti ṣe ohun tó ń ṣe láti ṣe ohun tó ń ṣe .\n",
      "2020-01-25 17:42:02,344 Validation result (greedy) at epoch   1, step     1000: bleu:   1.93, loss: 117912.7969, ppl:  52.3456, duration: 48.7340s\n",
      "2020-01-25 17:42:18,014 Epoch   1 Step:     1100 Batch Loss:     4.161552 Tokens per Sec:    15021, Lr: 0.000300\n",
      "2020-01-25 17:42:33,271 Epoch   1 Step:     1200 Batch Loss:     3.991871 Tokens per Sec:    14784, Lr: 0.000300\n",
      "2020-01-25 17:42:48,777 Epoch   1 Step:     1300 Batch Loss:     3.774519 Tokens per Sec:    15151, Lr: 0.000300\n",
      "2020-01-25 17:43:04,091 Epoch   1 Step:     1400 Batch Loss:     3.709967 Tokens per Sec:    15052, Lr: 0.000300\n",
      "2020-01-25 17:43:19,612 Epoch   1 Step:     1500 Batch Loss:     3.834824 Tokens per Sec:    15167, Lr: 0.000300\n",
      "2020-01-25 17:43:34,833 Epoch   1 Step:     1600 Batch Loss:     3.752159 Tokens per Sec:    14683, Lr: 0.000300\n",
      "2020-01-25 17:43:50,299 Epoch   1 Step:     1700 Batch Loss:     3.602469 Tokens per Sec:    15436, Lr: 0.000300\n",
      "2020-01-25 17:44:05,536 Epoch   1 Step:     1800 Batch Loss:     3.654793 Tokens per Sec:    15029, Lr: 0.000300\n",
      "2020-01-25 17:44:20,896 Epoch   1 Step:     1900 Batch Loss:     3.701706 Tokens per Sec:    15044, Lr: 0.000300\n",
      "2020-01-25 17:44:36,181 Epoch   1 Step:     2000 Batch Loss:     3.573286 Tokens per Sec:    14885, Lr: 0.000300\n",
      "2020-01-25 17:45:23,634 Hooray! New best validation result [ppl]!\n",
      "2020-01-25 17:45:23,635 Saving new checkpoint.\n",
      "2020-01-25 17:45:24,718 Example #0\n",
      "2020-01-25 17:45:24,719 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
      "2020-01-25 17:45:24,719 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
      "2020-01-25 17:45:24,719 \tHypothesis: Ó sọ pé : “ Ẹ jẹ́ kí àwọn ọmọ ẹ̀yìn rẹ̀ máa fi hàn pé Ọlọ́run ń ṣe àwọn èèyàn .\n",
      "2020-01-25 17:45:24,719 Example #1\n",
      "2020-01-25 17:45:24,720 \tSource:     Now I had to find a legitimate line of work .\n",
      "2020-01-25 17:45:24,720 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
      "2020-01-25 17:45:24,720 \tHypothesis: Mo tún máa ń bá àwọn ọmọ Ísírẹ́lì .\n",
      "2020-01-25 17:45:24,720 Example #2\n",
      "2020-01-25 17:45:24,721 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
      "2020-01-25 17:45:24,721 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
      "2020-01-25 17:45:24,721 \tHypothesis: Ṣé mo máa ń ṣe àwọn ọmọ mi láti máa ṣe iṣẹ́ ìwàásù Jèhófà , kí wọ́n sì máa ṣe iṣẹ́ ìwàásù wa ?\n",
      "2020-01-25 17:45:24,721 Example #3\n",
      "2020-01-25 17:45:24,722 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
      "2020-01-25 17:45:24,722 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
      "2020-01-25 17:45:24,722 \tHypothesis: Ó dájú pé ó jẹ́ pé ó máa ń ṣe , ó sì máa ń bá a nìṣó , ó sì máa ń bá a nìṣó .\n",
      "2020-01-25 17:45:24,722 Validation result (greedy) at epoch   1, step     2000: bleu:   3.00, loss: 101306.4844, ppl:  29.9779, duration: 48.5407s\n",
      "2020-01-25 17:45:40,859 Epoch   1 Step:     2100 Batch Loss:     3.625047 Tokens per Sec:    14420, Lr: 0.000300\n",
      "2020-01-25 17:45:56,074 Epoch   1 Step:     2200 Batch Loss:     3.254508 Tokens per Sec:    14930, Lr: 0.000300\n",
      "2020-01-25 17:46:11,615 Epoch   1 Step:     2300 Batch Loss:     3.497646 Tokens per Sec:    15114, Lr: 0.000300\n",
      "2020-01-25 17:46:26,778 Epoch   1 Step:     2400 Batch Loss:     3.432929 Tokens per Sec:    15264, Lr: 0.000300\n",
      "2020-01-25 17:46:42,035 Epoch   1 Step:     2500 Batch Loss:     3.469344 Tokens per Sec:    15151, Lr: 0.000300\n",
      "2020-01-25 17:46:57,286 Epoch   1 Step:     2600 Batch Loss:     3.209185 Tokens per Sec:    14987, Lr: 0.000300\n",
      "2020-01-25 17:47:12,553 Epoch   1 Step:     2700 Batch Loss:     3.516438 Tokens per Sec:    14877, Lr: 0.000300\n",
      "2020-01-25 17:47:27,931 Epoch   1 Step:     2800 Batch Loss:     2.984364 Tokens per Sec:    15266, Lr: 0.000300\n",
      "2020-01-25 17:47:43,180 Epoch   1 Step:     2900 Batch Loss:     3.131640 Tokens per Sec:    14823, Lr: 0.000300\n",
      "2020-01-25 17:47:58,417 Epoch   1 Step:     3000 Batch Loss:     3.081353 Tokens per Sec:    15058, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBoDS09JM807"
   },
   "outputs": [],
   "source": [
    "# Copy the created models from the temporary storage to main storage on google drive for persistant storage \n",
    "!cp -r ${model_temp_dir}/* \"$gdrive_path/models/${src}${tgt}_transformer/\"\n",
    "\n",
    "# Output our validation accuracy\n",
    "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNTiVj9NtGAc"
   },
   "outputs": [],
   "source": [
    "# Test our model\n",
    "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4u0fQ-O0vvW1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "starter_notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
